{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/icaros-usc/pyribs/blob/master/examples/tutorials/lunar_lander.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander Tutorial\n",
    "\n",
    "In this tutorial, we'll walk through how to use MAP-Elites to solve OpenAI Gym's Lunar Lander problem. Specifically, the environment we'll be using is `LunarLander-v2`.\n",
    "\n",
    "## Overview\n",
    "OpenAI Gym is common toolkit used to test and evaluate reinforcement learning algorithms. It provides various environments/problems for algorithms to solve. In our case, we'll be trying to get a lunar lander to land successfully on the moon within a certain target area. To find out more, visit [OpenAI's page](https://gym.openai.com/envs/LunarLander-v2/) on this environment. Using MAP-Elites, we'll discover and visualize a diverse range of solutions to this problem.\n",
    "\n",
    "If you're unfamiliar with the MAP-Elites algorithm, take a look at [this paper](https://arxiv.org/abs/1504.04909) which introduces the algorithm that we use in this notebook. It should be noted that while the algorithm in the paper minimizes performance, the `ribs` implementation maximizes performance instead. Additionally, instead of generating random solutions in the initial stage of MAP-Elites, our implementation samples from a Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Here we'll get all of our dependencies. Most of these should be familiar to you, but there's a few you may not have seen before. `dask` is a library that allows parallelization. `ribs` is the libary that this tutorial is for! We'll be using `ribs` to solve `LunarLander-v2` with MAP-Elites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import fire\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from ribs.archives import GridArchive\n",
    "from ribs.optimizers import Optimizer\n",
    "from ribs.emitters import GaussianEmitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lunar Lander Simulation\n",
    "\n",
    "First, let's write a `simulate()` function to run a prospective solution in the `LunarLander-v2` environment. We'll get to how we generate these prospective solutions later.\n",
    "\n",
    "`simulate()` takes in a prospective solution (i.e. policy) and uses this policy to make the Lunar Lander take actions in the environment. After the simulation is completed, `simulate()` returns several things. It returns the sum of the rewards of all actions taken in the environment (i.e. `total_reward`), the final state of the environment (i.e. `obs`), and the number of timesteps it took for the simulation to run to completion (i.e. `timesteps`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(\n",
    "    env_name: str,\n",
    "    model,\n",
    "    seed: int = None,\n",
    "    render: bool = False,\n",
    "    delay: int = 10,\n",
    "):\n",
    "    \"\"\"Runs the model in the env and returns the cumulative reward.\n",
    "    Add the `seed` argument to initialize the environment from the given seed\n",
    "    (this makes the environment the same between runs).\n",
    "    The model is just a linear model from input to output with softmax, so it\n",
    "    is represented by a single (action_dim, obs_dim) matrix.\n",
    "    Add an integer delay to wait `delay` ms between timesteps.\n",
    "    \"\"\"\n",
    "    total_reward = 0.0\n",
    "    env = gym.make(env_name)\n",
    "\n",
    "    # Seeding the environment before each reset ensures that our simulations are\n",
    "    # deterministic. We cannot vary the environment between the runs because\n",
    "    # that would confuse CMA-ES. See\n",
    "    # https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py#L115\n",
    "    # for the implementation of seed() for LunarLander.\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    obs = env.reset()\n",
    "\n",
    "    timesteps = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        # If render is set to True, then a video will appear showing the Lunar Lander\n",
    "        # taking actions in the environment.\n",
    "        if render:\n",
    "            env.render()\n",
    "        if delay is not None:\n",
    "            time.sleep(delay / 1000)\n",
    "\n",
    "        # Deterministic. Here is the action. Multiply observation by policy. Model is the policy and obs is state\n",
    "        action = np.argmax(model @ obs)  \n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        timesteps += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    return total_reward, obs, timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP-Elites with `ribs`\n",
    "\n",
    "`pyribs` makes it easy to run the MAP-Elites algorithm to solve reinforcement learning problems. Let's run through  some basics before we apply `ribs` to solve the Lunar Lander problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridArchive\n",
    "\n",
    "`GridArchive` is a container class used to house the solutions generated by MAP-Elites. It is our map of elites. When constructing a `GridArchive`, you can specify its dimensions, the range of valid values in our behaviour space, and certain configuration settings. These configuration settings include a seed for getting random solutions in the archive to mutate, which is essential to MAP-Elites, and a batch size. This batch size is not important for `GridArchive` but it is important for `Optimizer`, which we'll discuss soon.\n",
    "\n",
    "In `train_model()`, you see we create an `archive = GridArchive((16, 16), [(0, 1000), (-1., 1.)], config=config)`. Let's break this down.\n",
    "\n",
    "- `(16, 16)` specifies that we are creating a 2D 16x16 container for solutions. 16x16 was chosen arbitrarily.\n",
    "\n",
    "- `[(0, 1000), (-1., 1.)]` specifies upper and lower bounds for each dimension of the behavior space. In the case of Lunar Lander, we want to consider timesteps and x-position of the Lunar Lander in the environment. According to OpenAI Gym documention, each simulation can take at least 0 timesteps and at most 1000 timesteps, so we specify `(0, 1000)`. Looking at `LunarLander-v2`'s source code, we find that the minimum x-position value for the lander is -1.0 and the maximum value is 1.0, so we specify `(-1., 1.)`.\n",
    "- `config` is a dictionary that specifies certain configuration settings. As stated previously, the only value that `GridArchive` uses is the seed. `config` will also later be passed into `Optimizer`, which we'll discuss soon.\n",
    "\n",
    "`GridArchive` has a method `as_pandas()` that returns the `GridArchive` as a `pandas` data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GaussianEmitter\n",
    "`GaussianEmitter` is the class that generates solutions to either store in `GridArchive` or discard. As the name implies, it uses a Gaussian distribution to generate/mutate solutions. \n",
    "\n",
    "In `train_model()`, we create `emitter = GaussianEmitter(np.zeros(action_dim * obs_dim), sigma, archive)`. Let's break this down by looking at `GaussianEmitter`'s constructor: `GaussianEmitter(x0, sigma0, archive, config=None)`\n",
    "\n",
    "- `x0` is the center of the Gaussian distribution to generate solutions from when the archive specified by `archive` is empty of solutions. \n",
    "- `sigma0` is the standard deviation of the Gaussian distribution. Here, we simply pass in the sigma value passed into `train_model()`.\n",
    "- `archive` specifies the archive to store solutions in. In this case, we pass in the `GridArchive` we created earlier.\n",
    "- `config` allows you to pass in configuration settings, including specifications for batch sizes. Here, we don't pass anything in because `GaussianEmitter`'s default batch size is 64, which works for us.\n",
    "\n",
    "`GaussianEmitter` has two functions. `ask()` generates a batch of new solutions, either a completely new solution sampled from a Gaussian distribution or a solution generated by mutating (i.e. adding Gaussian noise) an existing solution. `tell()` takes in a batch of solutions, along with their performance values and behavior characteristics, and adds them to the archive specified by `archive` by calling `archive.add()`. `archive.add()` will decide whether or not to store each new solution by comparing each new solution's objective value with their corresponding existing solution's objective value. If for a given new solution there is no corresponding existing solution, then the new solution is automatically stored. `Optimizer`, which we discuss next, takes care of calling `ask()` and `tell()` for you, so you don't need to worry about these details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "`Optimizer` is a class that uses emitters to generate batches of solutions and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    client: Client,\n",
    "    seed: int,\n",
    "    sigma: float,\n",
    "    model_filename: str,\n",
    "    plot_filename: str,\n",
    "    iterations: int,\n",
    "    env_name: str = \"LunarLander-v2\",\n",
    "):\n",
    "    \"\"\"Trains a model with MAP-Elites and saves it.\"\"\"\n",
    "    # Environment properties.\n",
    "    env = gym.make(env_name)\n",
    "    action_dim = env.action_space.n\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "\n",
    "    config = {\n",
    "        \"seed\": seed,\n",
    "        \"batch_size\": 64,\n",
    "    }\n",
    "\n",
    "\n",
    "    archive = GridArchive((16, 16), [(0, 1000), (-1., 1.)], config=config)\n",
    "    emitter = GaussianEmitter(np.zeros(action_dim * obs_dim), sigma, archive)\n",
    "#     opt = Optimizer(np.zeros(action_dim * obs_dim), sigma, archive, config=config)\n",
    "    opt = Optimizer(archive, [emitter], config=config)\n",
    "\n",
    "    for _ in range(0, iterations - 1):\n",
    "\n",
    "        sols = opt.ask()\n",
    "\n",
    "        objs = list()\n",
    "        bcs = list()\n",
    "\n",
    "        futures = client.map(lambda sol: simulate(env_name, np.reshape(sol, (action_dim, obs_dim)), seed), sols)\n",
    "\n",
    "        results = client.gather(futures)\n",
    "\n",
    "        for reward, state, timesteps in results:\n",
    "            objs.append(reward)\n",
    "            bcs.append((timesteps, state[0]))\n",
    "\n",
    "        opt.tell(sols, objs, bcs)\n",
    "\n",
    "\n",
    "    df = archive.as_pandas()\n",
    "\n",
    "    df.to_csv(model_filename)\n",
    "\n",
    "    df = archive.as_pandas()\n",
    "    df = df.pivot('index-0', 'index-1', 'objective')\n",
    "    sns.heatmap(df)\n",
    "    plt.savefig(plot_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(model_filename, env_name, seed):\n",
    "    \"\"\"Runs a single simulation and displays the results.\"\"\"\n",
    "    model = np.load(model_filename)\n",
    "    print(\"=== Model ===\")\n",
    "    print(model)\n",
    "    cost = simulate(env_name, model, seed, True, 10)\n",
    "    print(\"Reward:\", -cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_elites(\n",
    "    seed: int = 42,\n",
    "    local_workers: int = 8,\n",
    "    sigma: float = 10.0,\n",
    "    plot_filename: str = \"lunar_lander_plot.png\",\n",
    "    model_filename: str = \"lunar_lander_model.csv\",\n",
    "    run_eval: bool = False,\n",
    "):\n",
    "    \"\"\"Uses Map-Elites to train an agent in an environment with discrete actions.\n",
    "    Args:\n",
    "        env: OpenAI Gym environment name. The environment should have a discrete\n",
    "            action space.\n",
    "        seed: Random seed for environments.\n",
    "        sigma: Initial standard deviation for CMA-ES.\n",
    "        local_workers: Number of workers to use when running locally.\n",
    "        slurm: Set to True if running on Slurm.\n",
    "        slurm_workers: Number of workers to start when running on Slurm.\n",
    "        slurm_cpus_per_worker: Number of CPUs to use on each Slurm worker.\n",
    "        plot_filename: Location to store plot image.\n",
    "        model_filename: Location for .npy model file (either for storing or\n",
    "            reading).\n",
    "        run_eval: Pass this to run an evaluation in the environment in `env`\n",
    "            with the model in `model_filename`.\n",
    "    \"\"\"\n",
    "    # Evaluations do not need Dask.\n",
    "    if run_eval:\n",
    "        run_evaluation(model_filename, \"LunarLander-v2\", seed)\n",
    "        return\n",
    "\n",
    "        # Initialize on a local machine. See the docs here:\n",
    "        # https://docs.dask.org/en/latest/setup/single-distributed.html for more\n",
    "        # info on LocalCluster. Keep in mind that for LocalCluster, the\n",
    "        # n_workers is the number of processes. Our LunarLander evaluations do\n",
    "        # not release the GIL (I think), so using threads instead of processes\n",
    "        # (which we would do by setting n_workers=1 and\n",
    "        # threads_per_worker=workers) would be very slow, as it would be\n",
    "        # single-threaded. See here for a bit more info about processes in\n",
    "        # threads in Dask:\n",
    "        # https://distributed.dask.org/en/latest/worker.html#thread-pool\n",
    "        # The link above is for multiple machines (each machine is called a\n",
    "        # worker, and each workers has processes and threads), but the idea\n",
    "        # still holds.\n",
    "    cluster = LocalCluster(n_workers=local_workers,\n",
    "                           threads_per_worker=1,\n",
    "                           processes=True)\n",
    "    client = Client(cluster)  # pylint: disable=unused-variable\n",
    "    print(\"Cluster config:\")\n",
    "    print(client.ncores())\n",
    "\n",
    "    train_model(client, seed, sigma, model_filename, plot_filename, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fire.Fire(map_elites)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
