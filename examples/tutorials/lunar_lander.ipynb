{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CMA-ME to Land the Lunar Lander Like an Airplane\n",
    "\n",
    "In the [OpenAI Gym](https://gym.openai.com) toolkit, the [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) is a task where an agent must control a spaceship to touch down gently within a goal zone near the bottom of the screen. Typically, agents in Lunar Lander take a direct approach, hovering straight down like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/itfsi8tTwWU\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f4288218250>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('itfsi8tTwWU', width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this works fine, and the astronauts on board survive the trip down. But are there less obvious (and more exciting) ways to land? Definitely ðŸ˜‰. For instance, we can land it a bit like an airplane:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/SYDbnH--R1U\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x7f42881dce10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YouTubeVideo('SYDbnH--R1U', width=560, height=315)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary difference between these two trajectories is their \"point of impact,\" that is, the x-position when the lunar lander touches the ground for the first time. In the vertical trajectory, the lunar lander first touches the ground at $x \\approx -0.1$, while in the airplane trajectory, it touches down at $x \\approx -0.6$ (coordinates range from -1 to 1 with 0 at the center of the screen).\n",
    "\n",
    "Of course, it does not stop there, and we can generate trajectories with a wide variety of impact points. In this tutorial, we will show how to do just that using the pyribs implementation of [CMA-ME](https://arxiv.org/abs/1912.02400)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we retrieve the dependencies. We freeze our versions in order to make results reproducible. If this command updates or installs any dependencies, you may need to restart the kernel to make sure they are used. We retrieve `ribs[all]` in order to use the `ribs.visualize` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ribs[all] gym~=0.17.0 Box2D~=2.3.10 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from ribs.archives import GridArchive\n",
    "from ribs.emitters import ImprovementEmitter\n",
    "from ribs.optimizers import Optimizer\n",
    "from ribs.visualize import grid_archive_heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "As seen in the introduction, the \"impact characteristics\" of a lunar lander can make a large difference in its trajectory. To be more specific, we define \"impact\" to be when either of the lunar lander's legs touches the ground for the first time. When this happens, we are interested in the following properties of the lunar lander:\n",
    "\n",
    "- $x$-position: This will lead to markedly different trajectories as seen earlier\n",
    "- $y$-velocity: If the lunar lander is moving very fast when it hits the ground, the astronauts or the lander itself may be injured.\n",
    "\n",
    "If the lunar lander never impacts the ground, we default the $x$-position to be the last $x$-position of the lander, and the $y$-velocity to be the maximum velocity of the lander (technically, the minimum, since the lander is going down).\n",
    "\n",
    "We will search for policies that produce high-performing trajectories with these characteristics. In QD terms, the reward is our objective, and the impact characteristics are our behavior characteristics (BCs). For simplicity, we will use a linear policy to control the lunar lander. As the lunar lander has discrete controls, the equation for this policy is:\n",
    "\n",
    "$$a = argmax(Ws)$$\n",
    "\n",
    "where $a$ is the action to take, $s$ is the state vector, and $W$ is our model, a matrix of weights that stays constant in each trajectory. Essentially, we transform the state to a vector with a \"signal\" for each possible action in the action space, and we choose the action with the highest signal. In searching for policies, we will really be searching for different models $W$.\n",
    "\n",
    "Finally, to ensure we have an environment where landing with different trajectories works all the time (it works best on flat terrains), we have chosen a single seed (seed 1339) to use throughout the experiment, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "seed = 1339  # Flat terrain.\n",
    "action_dim = env.action_space.n\n",
    "obs_dim = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can summarize our problem description with the following `simulate` function, which takes in the model and rolls it out in the Lunar Lander environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(env, model, seed=None):\n",
    "    \"\"\"Simulates the lunar lander model and returns total reward and impact\n",
    "\n",
    "    Args:\n",
    "        env (gym.Env): A copy of the lunar lander environment.\n",
    "        model (np.ndarray): The array of weights for the linear policy.\n",
    "        seed (int): The seed for the environment.\n",
    "    Returns:\n",
    "        total_reward (float): The reward accrued by the lander throughout its\n",
    "            trajectory.\n",
    "        impact_x_pos (float): The x position of the lander when it touches the\n",
    "            ground for the first time.\n",
    "        impact_y_vel (float): The y velocity of the lander when it touches the\n",
    "            ground for the first time.\n",
    "    \"\"\"\n",
    "    # Seeding the environment ensures simulations are deterministic.\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "\n",
    "    action_dim = env.action_space.n\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    model = model.reshape((action_dim, obs_dim))\n",
    "\n",
    "    total_reward = 0.0\n",
    "    timesteps = 0\n",
    "    obs = env.reset()\n",
    "    impact_x_pos = None\n",
    "    all_y_vels = []\n",
    "    impact_y_vel = None\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(model @ obs)  # Linear policy.\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # Refer to the definition of state here:\n",
    "        # https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py#L306\n",
    "        x_pos = obs[0]\n",
    "        y_vel = obs[3]\n",
    "        leg0_touch = bool(obs[6])\n",
    "        leg1_touch = bool(obs[7])\n",
    "        all_y_vels.append(y_vel)\n",
    "\n",
    "        # Check if the lunar lander is impacting for the first time.\n",
    "        if impact_x_pos is None and (leg0_touch or leg1_touch):\n",
    "            impact_x_pos = x_pos\n",
    "            impact_y_vel = y_vel\n",
    "\n",
    "    # If the lunar lander did not land, set the x-pos to the one from the final\n",
    "    # timestep, and set the y-vel to the maximum y-vel (we use minimum since the\n",
    "    # lander is going down).\n",
    "    if impact_x_pos is None:\n",
    "        impact_x_pos = x_pos\n",
    "        impact_y_vel = min(all_y_vels)\n",
    "\n",
    "    return total_reward, impact_x_pos, impact_y_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CMA-ME with pyribs\n",
    "\n",
    "Covariance Matrix Adaptation MAP-Elites is a Quality-Diversity (QD) algorithm that leverages [CMA-ES](http://cma.gforge.inria.fr) to enhance exploration of the search and behavior space. To use CMA-ME to search for lunar lander policies, we will need the `GridArchive`, `ImprovementEmitter`, and `Optimizer` from pyribs.\n",
    "\n",
    "First, the `GridArchive` stores solutions in a rectangular grid. Each dimension of the `GridArchive` corresponds to a dimension in behavior space that is segmented into equally sized bins. As we have two BCs for our lunar lander, we have two dimensions in the `GridArchive`. The first dimension is the impact $x$-position, which ranges from -1 to 1, and the second is the impact $y$-velocity, which ranges from -3 (hitting the ground very quickly) to 0 (gently touching down). We divide both BCs into 50 bins below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = GridArchive(\n",
    "    [50, 50],  # 50 bins in each dimension.\n",
    "    [(-1.0, 1.0), (-3.0, 0.0)],  # (-1, 1) for x-pos and (-3, 0) for y-vel.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the `ImprovementEmitter` uses CMA-ES to search for policies that add new entries to the archive or improve existing ones. Since we do not have any prior knowledge of what the model will be, we set the initial model to be the zero vector, and we set the initial step size for CMA-ES to be 1.0, so that initial solutions are sampled from the standard Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model = np.zeros((action_dim, obs_dim))\n",
    "emitters = [ImprovementEmitter(\n",
    "    archive,\n",
    "    initial_model.flatten(),\n",
    "    1.0, # Initial step size.\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we chose not to pass a ``batch_size``, i.e. the number of solutions for the emitter to generate on each call to its ``ask()`` method. Thus, the batch size was automatically calculated. We can check what it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "print(emitters[0].batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the ``Optimizer`` connects the archive and emitters together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Optimizer(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QD Search\n",
    "\n",
    "With the pyribs components defined, we can start searching with CMA-ME. This loop should take 1-2 hours to run. Since the emitter has a batch size of 14, and we are running 2500 iterations, so we will have to run 14 x 2500 = 35,000 lunar lander simulations. We also keep track of some logging info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c39c59cba5e4b15bb7ff0c3eb9cc13e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "total_itrs = 2500\n",
    "\n",
    "for itr in tqdm(range(1, total_itrs + 1)):\n",
    "    # Request models from the optimizer.\n",
    "    sols = optimizer.ask()\n",
    "\n",
    "    # Evaluate the models and record the objectives and BCs.\n",
    "    objs, bcs = [], []\n",
    "    for model in sols:\n",
    "        obj, impact_x_pos, impact_y_vel = simulate(env, model, seed)\n",
    "        objs.append(obj)\n",
    "        bcs.append([impact_x_pos, impact_y_vel])\n",
    "\n",
    "    # Send the results back to the optimizer.\n",
    "    optimizer.tell(objs, bcs)\n",
    "\n",
    "    # Logging.\n",
    "    if itr % 100 == 0:\n",
    "        df = archive.as_pandas(include_solutions=False)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"> {itr} itrs completed after {elapsed_time} s\")\n",
    "        print(f\"  - Archive Size: {len(df)}\")\n",
    "        print(f\"  - Max Score: {df['objective'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Archive\n",
    "\n",
    "We can visualize our results in a few ways. First, using `grid_archive_heatmap` from `ribs.visualize`, we can see a heatmap of the archive. The heatmap shows the BCs for which CMA-ME found a solution. At each BC, the color of the cell shows the objective value of the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "grid_archive_heatmap(archive, vmin=-300, vmax=300)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.ylabel(\"Impact y-velocity\")\n",
    "plt.xlabel(\"Impact x-position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this heatmap, we can make a few observations:\n",
    "\n",
    "- CMA-ME found solutions for almost all bins in the archive. Empty bins show up as white.\n",
    "- Most of the high-performing solutions have lower impact $y$-velocities (see the bright area at the bottom of the map). This is reasonable, as a lander that crashes into the ground probably would not stick the landing.\n",
    "- The high-performing solutions are spread across a wide range of impact $x$-positions. The highest solutions seem to be at $x \\approx 0$ (the bright spot in the middle). This makes sense since an impact $x$-position of 0 corresponds to the direct vertical approach. Nevertheless, there are many high-performing solutions that had other $x$-positions, and we will visualize them in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Individual Trajectories\n",
    "\n",
    "To view different models, we can use the OpenAI Gym Monitor to record a video, and we can use IPython to display the video in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we can filter the archive for high-performing solutions. To mimic the example at the beginning of this tutorial, we can also filter for solutions with impact positions on the left of the screen (i.e. negative positions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = archive.as_pandas()\n",
    "sols = df[(df[\"objective\"] > 200) &  # 200 is considered high-performing.\n",
    "          (df[\"behavior_0\"] < -0.4)  # behavior_0 is the impact x-position.\n",
    "         ].sort_values(\"objective\", ascending=False)\n",
    "display(sols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize some solutions, we pick a row in the archive and extract the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = sols.head(2)\n",
    "display(rows.loc[:,:\"objective\"])\n",
    "solutions = np.array(rows.loc[:,\"solution_0\":])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then construct a video environment with OpenAI gym's Monitor and use it to record videos in the `videos/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_env = gym.wrappers.Monitor(\n",
    "    gym.make(\"LunarLander-v2\"),\n",
    "    \"videos\",\n",
    "    force=True,  # Overwrite existing videos.\n",
    "    video_callable=lambda idx: True,  # Make all episodes be recorded.\n",
    ")\n",
    "\n",
    "for model in solutions:\n",
    "    simulate(video_env, model, seed)\n",
    "\n",
    "# Save video.\n",
    "video_env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we display the videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_file in glob.iglob(\"videos/*.mp4\"):\n",
    "    video = io.open(video_file, 'rb').read()\n",
    "    encoded = base64.b64encode(video).decode(\"ascii\")\n",
    "    display(HTML(f'''\n",
    "        <video width=\"360\" height=\"auto\" alt=\"test\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\" />\n",
    "        </video>\n",
    "    '''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Often, there are many interesting solutions to a problem, but the properties of these solutions cannot easily be quantified as part of the objective. In such cases, a QD algorithm can help search for solutions that share a wide variety of solutions. In this tutorial, we showed that this is the case for the lunar lander environment. Using CMA-ME, we searched for lunar lander trajectories with a wide variety of impact characteristics. Though these trajectories are vastly different, they all perform well.\n",
    "\n",
    "For extending this tutorial, we suggest:\n",
    "\n",
    "- Trying out different terrains by changing the seed. For instance, if the environment has a bunch of valleys, can the lander learn to go into this valley and glide back up?\n",
    "- Trying different gym environments. What BCs could you use in an environment like `BipedalWalker-v2`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "This tutorial is based on a [poster](https://1l7puj10vwe3zflo2jsktkit-wpengine.netdna-ssl.com/wp-content/uploads/2020/08/S20-Klapsis-Poster.pdf) created by Nikitas Klapsis as part of USC's 2020 SHINE program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = archive.as_pandas()\n",
    "sols = df[(df[\"objective\"] > 200) & (df[\"behavior_0\"] < -0.4)].sort_values(\"objective\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make into function\n",
    "i = 0\n",
    "\n",
    "\n",
    "video_env = gym.wrappers.Monitor(\n",
    "    gym.make(\"LunarLander-v2\"),\n",
    "    \"videos\",\n",
    "    force=True,  # Overwrite existing videos.\n",
    "    video_callable=lambda idx: True,  # Make all episodes be recorded.\n",
    ")\n",
    "\n",
    "# sol, obj, beh = archive.elite_with_behavior([-0.42, -0.16])\n",
    "# sol, obj, beh = archive.elite_with_behavior([-0.49, -0.26])\n",
    "# sol, obj, beh = archive.elite_with_behavior([-0.50, -0.21])\n",
    "sol = np.array(sols.iloc[i][\"solution_0\":])\n",
    "\n",
    "# sol, obj, beh = archive.elite_with_behavior([-0.58, -0.26])\n",
    "# print(obj)\n",
    "# print(beh)\n",
    "simulate(video_env, sol, seed)\n",
    "\n",
    "# Save videos.\n",
    "video_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
