{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "VIv9Brvlk-b3"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Exploring DQD Algorithms** ðŸ”¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cr-94yAuzYns",
        "outputId": "8fbdf25e-fdf4-49d0-cb28-d6ed63fe8a5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-5af02c0d-b052-934f-f2ee-7ba92f49e009)\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Check GPU type** ðŸ•µï¸\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown V100 = Excellent (*Available only for Colab Pro users*)\n",
        "\n",
        "#@markdown P100 = Very Good\n",
        "\n",
        "#@markdown T4 = Good (*preferred*)\n",
        "\n",
        "#@markdown K80 = Meh\n",
        "\n",
        "#@markdown P4 = (*Not Recommended*) \n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJZEFUAfXX2z",
        "outputId": "1c6522df-f102-4aa1-d258-df103003f81f",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 834.1 MB 1.4 MB/s eta 0:14:40tcmalloc: large alloc 1147494400 bytes == 0x39caa000 @  0x7ff543867615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ               | 1055.7 MB 1.3 MB/s eta 0:12:29tcmalloc: large alloc 1434370048 bytes == 0x7e300000 @  0x7ff543867615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 1336.2 MB 9.7 MB/s eta 0:01:13tcmalloc: large alloc 1792966656 bytes == 0x3132000 @  0x7ff543867615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1691.1 MB 1.3 MB/s eta 0:04:34tcmalloc: large alloc 2241208320 bytes == 0x6df1a000 @  0x7ff543867615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x510325 0x5b4ee6 0x58ff2e 0x50d482 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4d00fb 0x50cb8d 0x4bac0a 0x538a76 0x590ae5 0x510280 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50c4fc 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2041.3 MB 1.2 MB/s eta 0:00:01tcmalloc: large alloc 2041315328 bytes == 0xf387c000 @  0x7ff5438661e7 0x4b2590 0x4b261c 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6\n",
            "tcmalloc: large alloc 2551644160 bytes == 0x1e17f2000 @  0x7ff543867615 0x58ead6 0x4f355e 0x4d222f 0x51041f 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x5b4ee6 0x58ff2e 0x50ca37 0x58fd37 0x50ca37 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x58ff2e 0x50d482 0x5b4ee6 0x4bad99 0x4d3249\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2041.3 MB 8.8 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.10.1+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torchvision-0.10.1%2Bcu111-cp37-cp37m-linux_x86_64.whl (20.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20.6 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.1+cu111) (4.1.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.10.1+cu111) (1.21.6)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.9.1+cu111 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.1+cu111 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.1+cu111 torchvision-0.10.1+cu111\n",
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 128, done.\u001b[K\n",
            "remote: Total 128 (delta 0), reused 0 (delta 0), pack-reused 128\u001b[K\n",
            "Receiving objects: 100% (128/128), 1.12 MiB | 22.51 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n",
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 236, done.\u001b[K\n",
            "remote: Total 236 (delta 0), reused 0 (delta 0), pack-reused 236\u001b[K\n",
            "Receiving objects: 100% (236/236), 8.92 MiB | 28.90 MiB/s, done.\n",
            "Resolving deltas: 100% (122/122), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/CLIP\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53 kB 1.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.9.1+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.10.1+cu111)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Installing collected packages: ftfy, clip\n",
            "  Running setup.py develop for clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 41 kB 489 kB/s \n",
            "\u001b[?25hCollecting ninja\n",
            "  Downloading ninja-1.11.1-py2.py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (145 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 145 kB 14.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja, einops\n",
            "Successfully installed einops-0.6.0 ninja-1.11.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 548 kB 6.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 163 kB 59.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.1+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->timm) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.13.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->timm) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2022.9.24)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.10.1 timm-0.6.11\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  363M  100  363M    0     0  21.0M      0  0:00:17  0:00:17 --:--:-- 24.8M\n",
            "Cloning into 'pyribs'...\n",
            "remote: Enumerating objects: 2887, done.\u001b[K\n",
            "remote: Total 2887 (delta 0), reused 0 (delta 0), pack-reused 2887\u001b[K\n",
            "Receiving objects: 100% (2887/2887), 31.92 MiB | 37.57 MiB/s, done.\n",
            "Resolving deltas: 100% (1910/1910), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing ./pyribs\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "\u001b[33mWARNING: ribs 0.4.0 does not provide the extra 'all'\u001b[0m\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.21.6)\n",
            "Collecting numpy_groupies>=0.9.16\n",
            "  Downloading numpy_groupies-0.9.20-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (0.56.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.3.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (2.4.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from ribs==0.4.0) (3.1.0)\n",
            "Collecting semantic-version>=2.10\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs==0.4.0) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs==0.4.0) (4.13.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.51.0->ribs==0.4.0) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs==0.4.0) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.0->ribs==0.4.0) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->ribs==0.4.0) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ribs==0.4.0) (1.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.51.0->ribs==0.4.0) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.51.0->ribs==0.4.0) (4.1.1)\n",
            "Building wheels for collected packages: ribs\n",
            "  Building wheel for ribs (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ribs: filename=ribs-0.4.0-py3-none-any.whl size=78318 sha256=a2313edb22044fc99cb1dbe2faeeace8c1e66101d6e4f585bb645fe736a17053\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1pedj8nc/wheels/c9/11/4b/66717ff9f4bdcff8cba45cb0ca191db0b4e56d17b9e9461229\n",
            "Successfully built ribs\n",
            "Installing collected packages: semantic-version, numpy-groupies, ribs\n",
            "Successfully installed numpy-groupies-0.9.20 ribs-0.4.0 semantic-version-2.10.0\n"
          ]
        }
      ],
      "source": [
        "#@markdown #**Install libraries** ðŸ—ï¸\n",
        "# @markdown This cell will take around 5 minutes to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "# !git clone https://github.com/autonomousvision/stylegan_xl\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "!pip install timm\n",
        "\n",
        "# !curl -LO 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-1024x1024.pkl'\n",
        "# !curl -LO 'https://s3.eu-central-1.amazonaws.com/avg-projects/stylegan_xl/models/pokemon256.pkl'\n",
        "# !curl -LO 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/afhqwild.pkl'\n",
        "!curl -LO 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl'\n",
        "!git clone https://github.com/icaros-usc/pyribs.git\n",
        "!pip install ./pyribs[all]\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append(\"./stylegan2-ada-pytorch\")\n",
        "# sys.path.append('./stylegan_xl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vYIf5bwsYPI3"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import os, time, glob\n",
        "import pickle\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from ribs.archives import CVTArchive, GridArchive\n",
        "from ribs.emitters import GradientArborescenceEmitter\n",
        "from ribs.schedulers import Scheduler\n",
        "from ribs.visualize import grid_archive_heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "N0lkdRW5YFBk"
      },
      "outputs": [],
      "source": [
        "def tensor_to_pil_img(img):\n",
        "    img = (img.clamp(-1, 1) + 1) / 2.0\n",
        "    img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
        "    img = Image.fromarray(img.astype('uint8'))\n",
        "    return img\n",
        "\n",
        "def norm1(prompt):\n",
        "    return prompt / prompt.square().sum(dim=-1, keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def cos_sim_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().mul(2)\n",
        "\n",
        "def prompts_dist_loss(x, targets, loss):\n",
        "    distances = [loss(x, target) for target in targets]\n",
        "    return torch.stack(distances, dim=-1).sum(dim=-1)\n",
        "\n",
        "def prompts_dist_loss(x, targets, loss):\n",
        "    if len(targets) == 1: # Keeps consitent results vs previous method for single objective guidance \n",
        "      return loss(x, targets[0])\n",
        "    distances = [loss(x, target) for target in targets]\n",
        "    return torch.stack(distances, dim=-1).sum(dim=-1)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FEptLHtqYE2Z"
      },
      "outputs": [],
      "source": [
        "class MakeCutouts(torch.nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "make_cutouts = MakeCutouts(224, 32, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "29vaSPaNYEt2"
      },
      "outputs": [],
      "source": [
        "class CLIP(object):\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        clip_model_name = \"ViT-B/32\"\n",
        "        self.model, _ = clip.load(clip_model_name, device=device)\n",
        "        self.model = self.model.requires_grad_(False)\n",
        "        self.model.eval()\n",
        "        self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                              std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def embed_text(self, prompt):\n",
        "        return norm1(self.model.encode_text(clip.tokenize(prompt)\n",
        "               .to(self.device)).float())\n",
        "\n",
        "    def embed_cutout(self, image):\n",
        "        return norm1(self.model.encode_image(self.normalize(image)))\n",
        "\n",
        "    def embed_image(self, image):\n",
        "        n = image.shape[0]\n",
        "        cutouts = make_cutouts(image)\n",
        "        embeds = self.embed_cutout(cutouts)\n",
        "        embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "        return embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "96S2T01tYElA"
      },
      "outputs": [],
      "source": [
        "class Generator(object):\n",
        "\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        model_filename = 'metfaces.pkl'\n",
        "        with open(model_filename, 'rb') as fp:\n",
        "            self.model = pickle.load(fp)['G_ema'].to(device)\n",
        "            self.model.eval()\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        self.init_stats()\n",
        "        self.latent_shape = (-1, 512)\n",
        "\n",
        "    def init_stats(self):\n",
        "        zs = torch.randn([10000, self.model.mapping.z_dim], device=self.device)\n",
        "        ws = self.model.mapping(zs, None)\n",
        "        self.w_stds = ws.std(0)\n",
        "        qs = ((ws - self.model.mapping.w_avg) / self.w_stds).reshape(10000, -1)\n",
        "        self.q_norm = torch.norm(qs, dim=1).mean() * 0.15\n",
        "\n",
        "    def gen_random_ws(self, num_latents):\n",
        "        zs = torch.randn([num_latents, self.model.mapping.z_dim], device=self.device)\n",
        "        ws = self.model.mapping(zs, None)\n",
        "        return ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zGvAiVz7YEWK"
      },
      "outputs": [],
      "source": [
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, gen_model, class_model, prompt):\n",
        "        self.device = gen_model.device\n",
        "        self.gen_model = gen_model\n",
        "        self.class_model = class_model\n",
        "        self.measures = []\n",
        "        self.init_objective(f'A potrait of a {prompt}.')\n",
        "        self.add_measure(f'A potrait of a {prompt} with long hair.', \n",
        "                         f'A potrait of a {prompt} with short hair.')\n",
        "        self.add_measure(f'A potrait of a young {prompt}.', \n",
        "                         f'A potrait of an old {prompt}.')\n",
        "\n",
        "    def init_objective(self, text_prompt):\n",
        "        texts = [frase.strip() for frase in text_prompt.split(\"|\") if frase]\n",
        "        self.obj_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "\n",
        "    def add_measure(self, positive_text, negative_text):\n",
        "        texts = [frase.strip() for frase in positive_text.split(\"|\") if frase]\n",
        "        negative_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "        \n",
        "        texts = [frase.strip() for frase in negative_text.split(\"|\") if frase]\n",
        "        positive_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "        \n",
        "        self.measures.append((negative_targets, positive_targets))\n",
        "\n",
        "    def find_good_start_latent(self, batch_size=16, num_batches=32):\n",
        "        with torch.inference_mode():\n",
        "            qs = []\n",
        "            losses = []\n",
        "            G = self.gen_model.model\n",
        "            w_stds = self.gen_model.w_stds\n",
        "            for _ in range(num_batches):\n",
        "                q = (G.mapping(torch.randn([batch_size, G.mapping.z_dim], device=self.device),\n",
        "                    None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
        "                images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "                embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "                loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
        "                i = torch.argmin(loss)\n",
        "                qs.append(q[i])\n",
        "                losses.append(loss[i])\n",
        "            qs = torch.stack(qs)\n",
        "            losses = torch.stack(losses)\n",
        "\n",
        "            i = torch.argmin(losses)\n",
        "            q = qs[i].unsqueeze(0)\n",
        "\n",
        "        return q.flatten()\n",
        "\n",
        "    def generate_image(self, latent_code):\n",
        "        ws, _ = self.transform_to_w([latent_code])\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        return images\n",
        "\n",
        "    def transform_to_w(self, latent_codes):\n",
        "        qs = []\n",
        "        ws = []\n",
        "        for cur_code in latent_codes:\n",
        "            q = torch.tensor(\n",
        "                    cur_code.reshape(self.gen_model.latent_shape), \n",
        "                    device=self.device,\n",
        "                    requires_grad=True,\n",
        "                )\n",
        "            qs.append(q)\n",
        "            w = q * self.gen_model.w_stds + self.gen_model.model.mapping.w_avg\n",
        "            ws.append(w)\n",
        "\n",
        "        ws = torch.stack(ws, dim=0)\n",
        "        return ws, qs\n",
        "\n",
        "    def compute_objective_loss(self, embeds, qs, dim=None):\n",
        "        loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
        "\n",
        "        diff = torch.max(torch.norm(qs, dim=dim), self.gen_model.q_norm)\n",
        "        reg_loss = (diff - self.gen_model.q_norm).pow(2)\n",
        "        loss = loss + 0.1 * reg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def compute_objective(self, sols):\n",
        "        ws, qs = self.transform_to_w(sols)\n",
        "\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "    \n",
        "        loss = self.compute_objective_loss(embeds, qs[0])\n",
        "        loss.backward()\n",
        "\n",
        "        value = loss.cpu().detach().numpy()\n",
        "        jacobian = -qs[0].grad.cpu().detach().numpy()\n",
        "        return value, jacobian.flatten()\n",
        "\n",
        "    def compute_measure(self, index, sols):\n",
        "        ws, qs = self.transform_to_w(sols)\n",
        "\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "\n",
        "        measure_targets = self.measures[index]\n",
        "        pos_loss = prompts_dist_loss(embeds, measure_targets[0], cos_sim_loss).mean(0)\n",
        "        neg_loss = prompts_dist_loss(embeds, measure_targets[1], cos_sim_loss).mean(0)\n",
        "        loss = pos_loss - neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        value = loss.cpu().detach().numpy()\n",
        "        jacobian = qs[0].grad.cpu().detach().numpy()\n",
        "        return value, jacobian.flatten()\n",
        "\n",
        "    def compute_measures(self, sols):\n",
        "    \n",
        "        values = []\n",
        "        jacobian = []\n",
        "        for i in range(len(self.measures)):\n",
        "            value, jac = self.compute_measure(i, sols)\n",
        "            values.append(value)\n",
        "            jacobian.append(jac)\n",
        "\n",
        "        return np.stack(values, axis=0), np.stack(jacobian, axis=0)\n",
        "\n",
        "    def compute_all(self, sols):\n",
        "        with torch.inference_mode():\n",
        "\n",
        "            ws, qs = self.transform_to_w(sols)\n",
        "            qs = torch.stack(qs, dim=0)\n",
        "\n",
        "            images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "            embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "            \n",
        "            values = []\n",
        "            loss = self.compute_objective_loss(embeds, qs, dim=(1,2))\n",
        "            value = loss.cpu().detach().numpy()\n",
        "            values.append(value)\n",
        "\n",
        "            for i in range(len(self.measures)):\n",
        "                measure_targets = self.measures[i]\n",
        "                pos_loss = prompts_dist_loss(\n",
        "                        embeds, \n",
        "                        measure_targets[0], \n",
        "                        cos_sim_loss,\n",
        "                    ).mean(0)\n",
        "                neg_loss = prompts_dist_loss(\n",
        "                        embeds, \n",
        "                        measure_targets[1], \n",
        "                        cos_sim_loss\n",
        "                    ).mean(0)\n",
        "                loss = pos_loss - neg_loss\n",
        "                value = loss.cpu().detach().numpy()\n",
        "                values.append(value)\n",
        "\n",
        "        return np.stack(values, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d-RMRh5kXS8h"
      },
      "outputs": [],
      "source": [
        "def transform_obj(objs):\n",
        "    # Remap the objective from minimizing [0, 10] to maximizing [0, 100]\n",
        "    return (10.0-objs*5.0)*10.0\n",
        "\n",
        "\n",
        "def create_optimizer(algorithm, classifier, seed):\n",
        "    \"\"\"Creates an optimizer based on the algorithm name.\n",
        "\n",
        "    Args:\n",
        "        algorithm (str): Name of the algorithm passed into sphere_main.\n",
        "        classifier (Classifier): The models for the search.\n",
        "        seed (int): Main seed or the various components.\n",
        "    Returns:\n",
        "        Scheduler: A ribs Scheduler for running the algorithm.\n",
        "    \"\"\"\n",
        "    bounds = [(-0.3, 0.3), (-0.3, 0.3)]\n",
        "    initial_sol = classifier.find_good_start_latent().cpu().detach().numpy()\n",
        "    dim = len(initial_sol)\n",
        "    batch_size = 32\n",
        "    num_emitters = 1\n",
        "    resolution = 200\n",
        "    grid_dims = (resolution, resolution)\n",
        "\n",
        "    # Create archive.\n",
        "    archive = GridArchive(\n",
        "        solution_dim = dim,\n",
        "        dims=grid_dims, \n",
        "        ranges=bounds, \n",
        "        learning_rate=0.02,\n",
        "        threshold_min=0.0,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Maintain a passive elitist archive\n",
        "    passive_archive = GridArchive(\n",
        "        solution_dim = dim,\n",
        "        dims=grid_dims,\n",
        "        ranges=bounds,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Create emitters. Each emitter needs a different seed, so that they do not\n",
        "    # all do the same thing.\n",
        "    emitter_seeds = [None] * num_emitters if seed is None else list(\n",
        "        range(seed, seed + num_emitters))\n",
        "    emitters = [\n",
        "        GradientArborescenceEmitter(\n",
        "            archive=archive,\n",
        "            x0=initial_sol,\n",
        "            sigma0=0.01,\n",
        "            step_size=0.05,\n",
        "            restart_rule='basic',\n",
        "            batch_size=batch_size,\n",
        "            seed=s\n",
        "        ) for s in emitter_seeds\n",
        "    ]\n",
        "\n",
        "    return Scheduler(archive, emitters), passive_archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1w0sfOW5X8_B"
      },
      "outputs": [],
      "source": [
        "def save_heatmap(archive, heatmap_path):\n",
        "    \"\"\"Saves a heatmap of the archive to the given path.\n",
        "    Args:\n",
        "        archive (GridArchive or CVTArchive): The archive to save.\n",
        "        heatmap_path: Image path for the heatmap.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    grid_archive_heatmap(archive, vmin=0, vmax=100, cmap=\"viridis\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(heatmap_path)\n",
        "    plt.close(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Cg6QcVj0v2-n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f1f8f7f-a851-42bd-8276-9714503d25a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 338M/338M [00:06<00:00, 58.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n"
          ]
        }
      ],
      "source": [
        "# Initialization.\n",
        "#@markdown #**Run the model** ðŸš€\n",
        "#@markdown `prompt`: Enter a prompt here to guide the image generation.\n",
        "#@markdown ---\n",
        "\n",
        "prompt = \"King\"#@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "outdir='logs'\n",
        "            \n",
        "# Create a shared logging directory for the experiments for this algorithm.\n",
        "s_logdir = os.path.join(outdir, \"cma_maega\")\n",
        "logdir = Path(s_logdir)\n",
        "outdir = Path(outdir)\n",
        "if not outdir.is_dir():\n",
        "    outdir.mkdir()\n",
        "if not logdir.is_dir():\n",
        "    logdir.mkdir()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "clip_model = CLIP(device=device)\n",
        "gen_model = Generator(device=device)\n",
        "classifier = Classifier(gen_model, clip_model, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-8dVRi1TvM_w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263,
          "referenced_widgets": [
            "723ea3ceb5d04048bd8d6ea82ed0b30d",
            "c29bf7e651e6417b813bf01137b41644",
            "4e783f47fa9b43439593f0426cbbbcb3",
            "84b46c2efe1f443abd9c94e1a2502d67",
            "40500ca36f1b440688dfd918d48d2872",
            "361c89e8fdd14bc69a13c4add798d3ff",
            "b84c0b0ce2c84af48498a78c0e52001a",
            "14132ba78d41463ea5a306a86346d38b",
            "0b62d3daa53045a3acd92442096037bd",
            "c0e92357a9884c219ed894a391d78640",
            "7186c409f6d0404286136e65a5f92589"
          ]
        },
        "outputId": "ab37e677-1959-4a31-c99a-e52581fed960"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running trial 0\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "723ea3ceb5d04048bd8d6ea82ed0b30d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best 56.228695 55.89612\n",
            "best 56.374573 50.306015\n",
            "best 56.374573 55.774406\n",
            "best 56.374573 53.968643\n",
            "best 56.374573 54.174606\n",
            "best 56.374573 55.647396\n",
            "best 56.374573 -146.05255\n",
            "best 56.374573 54.96289\n",
            "best 56.374573 53.72399\n",
            "best 56.374573 55.384216\n"
          ]
        }
      ],
      "source": [
        "#StyleGAN+CLIP LSI experiments.\n",
        "trials=1\n",
        "init_pop=100\n",
        "itrs=500\n",
        "log_freq=1\n",
        "log_arch_freq=1000\n",
        "image_monitor=True\n",
        "image_monitor_freq=10\n",
        "seed=None\n",
        "\n",
        "for trial_id in range(trials):\n",
        "    print(\"Running trial\", trial_id)\n",
        "    # Create a directory for this specific trial.\n",
        "    s_logdir = os.path.join(outdir, \"cma_maega\", f\"trial_{trial_id}\")\n",
        "    logdir = Path(s_logdir)\n",
        "    if not logdir.is_dir():\n",
        "        logdir.mkdir()\n",
        "\n",
        "    # Create a directory for logging intermediate images if the monitor is on.\n",
        "    if image_monitor:\n",
        "        image_monitor_freq = max(1, image_monitor_freq)\n",
        "        gen_output_dir = os.path.join('generations')\n",
        "        logdir = Path(gen_output_dir)\n",
        "        if not logdir.is_dir():\n",
        "            logdir.mkdir()\n",
        "        gen_output_dir = os.path.join('generations', f\"trial_{trial_id}\")\n",
        "        logdir = Path(gen_output_dir)\n",
        "        if not logdir.is_dir():\n",
        "            logdir.mkdir()\n",
        "\n",
        "    # Create a new summary file\n",
        "    summary_filename = os.path.join(s_logdir, \"summary.csv\")\n",
        "    if os.path.exists(summary_filename):\n",
        "        os.remove(summary_filename)\n",
        "    with open(summary_filename, 'w') as summary_file:\n",
        "        writer = csv.writer(summary_file)\n",
        "        writer.writerow(['Iteration', 'QD-Score', 'Coverage', 'Maximum', 'Average'])\n",
        "    \n",
        "    scheduler, passive_archive = create_optimizer(algorithm=\"cma_maega\", classifier=classifier,seed = seed)\n",
        "    archive = scheduler.archive\n",
        "\n",
        "    best = -1000\n",
        "    non_logging_time = 0.0\n",
        "    for itr in tqdm(range(1, itrs + 1)):\n",
        "        itr_start = time.time()\n",
        "        \n",
        "        sols = scheduler.ask_dqd()\n",
        "        objs, jacobian_obj = classifier.compute_objective(sols)\n",
        "        objs = transform_obj(objs)\n",
        "        best = max(best, max(objs))\n",
        "\n",
        "        measures, jacobian_measure = classifier.compute_measures(sols)\n",
        "\n",
        "        jacobian_obj = np.expand_dims(jacobian_obj, axis=0)\n",
        "        jacobian = np.concatenate((jacobian_obj, jacobian_measure), axis=0)\n",
        "        jacobian = np.expand_dims(jacobian, axis=0)\n",
        "\n",
        "        measures = np.transpose(measures) \n",
        "\n",
        "        objs = objs.astype(np.float32)\n",
        "        measures = measures.astype(np.float32)\n",
        "        jacobian = jacobian.astype(np.float32)\n",
        "\n",
        "        scheduler.tell_dqd(objs, measures, jacobian)\n",
        "\n",
        "        # Update the passive elitist archive.\n",
        "        for i in range(len(sols)):\n",
        "          passive_archive.add([sols[i]], [objs[i]], [measures[i]])\n",
        "\n",
        "        sols = scheduler.ask()\n",
        "        values = classifier.compute_all(sols)\n",
        "        values = np.transpose(values)\n",
        "\n",
        "        objs = values[:,0]\n",
        "        measures = values[:,1:3]\n",
        "\n",
        "        objs = transform_obj(np.array(objs, dtype=np.float32))\n",
        "        measures = np.array(measures, dtype=np.float32)\n",
        "        \n",
        "        passive_archive.add(sols, objs, measures)\n",
        "        \n",
        "        best_gen = max(objs) \n",
        "        best = max(best, best_gen)\n",
        "\n",
        "        scheduler.tell(objs, measures)\n",
        "\n",
        "        non_logging_time += time.time() - itr_start\n",
        "\n",
        "        if itr%50 == 0:\n",
        "          print('best', best, best_gen)\n",
        "\n",
        "        if image_monitor and itr % image_monitor_freq == 0:\n",
        "            best_index = np.argmax(objs)\n",
        "            latent_code = sols[best_index]\n",
        "\n",
        "            img = classifier.generate_image(latent_code)\n",
        "            img = tensor_to_pil_img(img)\n",
        "            img.save(os.path.join(gen_output_dir, f'{itr}.png'))\n",
        "\n",
        "        # Save the archive at the given frequency.\n",
        "        # Always save on the final iteration.\n",
        "        final_itr = itr == itrs\n",
        "        if (itr > 0 and itr % log_arch_freq == 0) or final_itr:\n",
        "            # Save a full archive for analysis.\n",
        "            df = passive_archive.as_pandas(include_solutions = final_itr)\n",
        "            df.to_pickle(os.path.join(s_logdir, f\"archive_{itr:08d}.pkl\"))\n",
        "\n",
        "            # Save a heatmap image to observe how the trial is doing.\n",
        "            save_heatmap(passive_archive, os.path.join(s_logdir, f\"heatmap_{itr:08d}.png\"))\n",
        "\n",
        "        # Update the summary statistics for the archive\n",
        "        if (itr > 0 and itr % log_freq == 0) or final_itr:\n",
        "            with open(summary_filename, 'a') as summary_file:\n",
        "                writer = csv.writer(summary_file)\n",
        "                stats = passive_archive.stats\n",
        "                qd_score = stats.qd_score\n",
        "                average = stats.obj_mean\n",
        "                coverage = stats.coverage\n",
        "                data = [itr, qd_score, coverage, best, average]\n",
        "                writer.writerow(data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "archive_filename = '/content/logs/cma_maega/trial_0/archive_00000500.pkl'\n",
        "\n",
        "# min and max index for rows then columns (row major).\n",
        "# The archive is shape (200, 200) indexed from [0, 200).\n",
        "archive_dims = (200, 200)\n",
        "archive_index_range = ((64, 135), (64, 135))\n",
        "# Measure ranges\n",
        "measure_ranges = ((-0.3, 0.3), (-0.3, 0.3))\n",
        "# Controls that x rows and y columns are generated\n",
        "# Images are \"evenly\" (as possible) sampled based on this criteria\n",
        "picture_frequency = (8, 5) \n",
        "\n",
        "# Use the CPU while we are running exps.\n",
        "#device = \"cpu\"\n",
        "device = \"cuda\"\n",
        "\n",
        "# Uncomment to save all grid images separately.\n",
        "gen_output_dir = os.path.join('grid_imgs')\n",
        "logdir = Path(gen_output_dir)\n",
        "if not logdir.is_dir():\n",
        "    logdir.mkdir()\n",
        "\n",
        "model_filename = 'metfaces.pkl'\n",
        "with open(model_filename, 'rb') as fp:\n",
        "    model = pickle.load(fp)['G_ema'].to(device)\n",
        "    model.eval()\n",
        "latent_shape = (1, -1, 512)\n",
        "\n",
        "zs = torch.randn([10000, model.mapping.z_dim], device=device)\n",
        "ws = model.mapping(zs, None)\n",
        "w_stds = ws.std(0)\n",
        "qs = ((ws - model.mapping.w_avg) / w_stds).reshape(10000, -1)\n",
        "q_norm = torch.norm(qs, dim=1).mean() * 0.1\n",
        "\n",
        "# Read the archive from the log (pickle file)\n",
        "df = pd.read_pickle(archive_filename)\n",
        "\n",
        "imgs = []\n",
        "for j in reversed(range(picture_frequency[1])):\n",
        "    for i in range(picture_frequency[0]):\n",
        "        \n",
        "        delta_i = archive_index_range[0][1] - archive_index_range[0][0]\n",
        "        delta_j = archive_index_range[1][1] - archive_index_range[1][0]\n",
        "        index_i_lower = int(delta_i * i / picture_frequency[0] + archive_index_range[0][0])\n",
        "        index_i_upper = int(delta_i * (i+1) / picture_frequency[0] + archive_index_range[0][0])\n",
        "        index_j_lower = int(delta_j * j / picture_frequency[1] + archive_index_range[1][0])\n",
        "        index_j_upper = int(delta_j * (j+1) / picture_frequency[1] + archive_index_range[1][0])\n",
        "        print(i, j, index_i_lower, index_i_upper, index_j_lower, index_j_upper)\n",
        "\n",
        "        query_string = f\"{index_i_lower} <= measure_0 & measure_0 <= {index_i_upper} &\"\n",
        "        query_string += f\"{index_j_lower} <= measure_1 & measure_1 <= {index_j_upper}\" \n",
        "        print(query_string)\n",
        "        df_cell = df.query(query_string)\n",
        "        \n",
        "        if not df_cell.empty:\n",
        "\n",
        "            sol = df_cell.iloc[df_cell['objective'].argmax()]\n",
        "            print(sol)\n",
        "\n",
        "            q = torch.tensor(\n",
        "                    sol[5:].values.reshape(latent_shape),\n",
        "                    device=device,\n",
        "                    requires_grad=True,\n",
        "                )\n",
        "            w = q * w_stds + model.mapping.w_avg\n",
        "\n",
        "            img = model.synthesis(w, noise_mode='const')\n",
        "            img = (img.clamp(-1, 1) + 1) / 2.0 # Normalize from [0,1]\n",
        "\n",
        "            # Uncomment to save all grid images separately.\n",
        "            pil_img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
        "            pil_img = Image.fromarray(pil_img.astype('uint8'))\n",
        "            pil_img.save(os.path.join(gen_output_dir, f'{j}_{i}.png'))\n",
        "\n",
        "            img = img[0].detach().cpu()\n",
        "            imgs.append(img)\n",
        "        else:\n",
        "            imgs.append(torch.zeros((3,1024,1024)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "plt.figure(figsize=(16,10))\n",
        "img_grid = make_grid(imgs, nrow=picture_frequency[0], padding=0)\n",
        "img_grid = np.transpose(img_grid.cpu().numpy(), (1,2,0))\n",
        "plt.imshow(img_grid)\n",
        "\n",
        "plt.xlabel(\"Hair Length\")\n",
        "plt.ylabel(\"Age\")\n",
        "\n",
        "def create_archive_tick_labels(axis_range, measure_range, dim, num_ticks):\n",
        "    low_pos = axis_range[0] / dim\n",
        "    high_pos = axis_range[1] / dim\n",
        "\n",
        "    tick_offset = [\n",
        "        (high_pos - low_pos) * (p / num_ticks) + low_pos\n",
        "        for p in range(num_ticks+1) \n",
        "    ]\n",
        "    ticklabels = [\n",
        "        round((measure_range[1]-measure_range[0]) * p + measure_range[0], 2)\n",
        "        for p in tick_offset\n",
        "    ]\n",
        "    return ticklabels\n",
        "\n",
        "num_x_ticks = 6\n",
        "num_y_ticks = 6\n",
        "x_ticklabels = create_archive_tick_labels(archive_index_range[0], \n",
        "                    measure_ranges[0], archive_dims[0], num_x_ticks)\n",
        "y_ticklabels = create_archive_tick_labels(archive_index_range[1],\n",
        "                    measure_ranges[1], archive_dims[1], num_y_ticks)\n",
        "y_ticklabels.reverse()\n",
        "\n",
        "x_tick_range = img_grid.shape[1]\n",
        "x_ticks = np.arange(0, x_tick_range+1e-9, step=x_tick_range/num_x_ticks)\n",
        "y_tick_range = img_grid.shape[0]\n",
        "y_ticks = np.arange(0, y_tick_range+1e-9, step=y_tick_range/num_y_ticks)\n",
        "plt.xticks(x_ticks, x_ticklabels)\n",
        "plt.yticks(y_ticks, y_ticklabels)\n",
        "plt.tight_layout()\n",
        "plt.savefig('collage.pdf')"
      ],
      "metadata": {
        "id": "kgLc1FnGMYQg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "42369ae8-cd17-40ec-ca3c-1cd3d30d47c6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 4 64 72 120 135\n",
            "64 <= measure_0 & measure_0 <= 72 &120 <= measure_1 & measure_1 <= 135\n",
            "1 4 72 81 120 135\n",
            "72 <= measure_0 & measure_0 <= 81 &120 <= measure_1 & measure_1 <= 135\n",
            "2 4 81 90 120 135\n",
            "81 <= measure_0 & measure_0 <= 90 &120 <= measure_1 & measure_1 <= 135\n",
            "3 4 90 99 120 135\n",
            "90 <= measure_0 & measure_0 <= 99 &120 <= measure_1 & measure_1 <= 135\n",
            "4 4 99 108 120 135\n",
            "99 <= measure_0 & measure_0 <= 108 &120 <= measure_1 & measure_1 <= 135\n",
            "5 4 108 117 120 135\n",
            "108 <= measure_0 & measure_0 <= 117 &120 <= measure_1 & measure_1 <= 135\n",
            "6 4 117 126 120 135\n",
            "117 <= measure_0 & measure_0 <= 126 &120 <= measure_1 & measure_1 <= 135\n",
            "7 4 126 135 120 135\n",
            "126 <= measure_0 & measure_0 <= 135 &120 <= measure_1 & measure_1 <= 135\n",
            "0 3 64 72 106 120\n",
            "64 <= measure_0 & measure_0 <= 72 &106 <= measure_1 & measure_1 <= 120\n",
            "1 3 72 81 106 120\n",
            "72 <= measure_0 & measure_0 <= 81 &106 <= measure_1 & measure_1 <= 120\n",
            "2 3 81 90 106 120\n",
            "81 <= measure_0 & measure_0 <= 90 &106 <= measure_1 & measure_1 <= 120\n",
            "3 3 90 99 106 120\n",
            "90 <= measure_0 & measure_0 <= 99 &106 <= measure_1 & measure_1 <= 120\n",
            "4 3 99 108 106 120\n",
            "99 <= measure_0 & measure_0 <= 108 &106 <= measure_1 & measure_1 <= 120\n",
            "5 3 108 117 106 120\n",
            "108 <= measure_0 & measure_0 <= 117 &106 <= measure_1 & measure_1 <= 120\n",
            "6 3 117 126 106 120\n",
            "117 <= measure_0 & measure_0 <= 126 &106 <= measure_1 & measure_1 <= 120\n",
            "7 3 126 135 106 120\n",
            "126 <= measure_0 & measure_0 <= 135 &106 <= measure_1 & measure_1 <= 120\n",
            "0 2 64 72 92 106\n",
            "64 <= measure_0 & measure_0 <= 72 &92 <= measure_1 & measure_1 <= 106\n",
            "1 2 72 81 92 106\n",
            "72 <= measure_0 & measure_0 <= 81 &92 <= measure_1 & measure_1 <= 106\n",
            "2 2 81 90 92 106\n",
            "81 <= measure_0 & measure_0 <= 90 &92 <= measure_1 & measure_1 <= 106\n",
            "3 2 90 99 92 106\n",
            "90 <= measure_0 & measure_0 <= 99 &92 <= measure_1 & measure_1 <= 106\n",
            "4 2 99 108 92 106\n",
            "99 <= measure_0 & measure_0 <= 108 &92 <= measure_1 & measure_1 <= 106\n",
            "5 2 108 117 92 106\n",
            "108 <= measure_0 & measure_0 <= 117 &92 <= measure_1 & measure_1 <= 106\n",
            "6 2 117 126 92 106\n",
            "117 <= measure_0 & measure_0 <= 126 &92 <= measure_1 & measure_1 <= 106\n",
            "7 2 126 135 92 106\n",
            "126 <= measure_0 & measure_0 <= 135 &92 <= measure_1 & measure_1 <= 106\n",
            "0 1 64 72 78 92\n",
            "64 <= measure_0 & measure_0 <= 72 &78 <= measure_1 & measure_1 <= 92\n",
            "1 1 72 81 78 92\n",
            "72 <= measure_0 & measure_0 <= 81 &78 <= measure_1 & measure_1 <= 92\n",
            "2 1 81 90 78 92\n",
            "81 <= measure_0 & measure_0 <= 90 &78 <= measure_1 & measure_1 <= 92\n",
            "3 1 90 99 78 92\n",
            "90 <= measure_0 & measure_0 <= 99 &78 <= measure_1 & measure_1 <= 92\n",
            "4 1 99 108 78 92\n",
            "99 <= measure_0 & measure_0 <= 108 &78 <= measure_1 & measure_1 <= 92\n",
            "5 1 108 117 78 92\n",
            "108 <= measure_0 & measure_0 <= 117 &78 <= measure_1 & measure_1 <= 92\n",
            "6 1 117 126 78 92\n",
            "117 <= measure_0 & measure_0 <= 126 &78 <= measure_1 & measure_1 <= 92\n",
            "7 1 126 135 78 92\n",
            "126 <= measure_0 & measure_0 <= 135 &78 <= measure_1 & measure_1 <= 92\n",
            "0 0 64 72 64 78\n",
            "64 <= measure_0 & measure_0 <= 72 &64 <= measure_1 & measure_1 <= 78\n",
            "1 0 72 81 64 78\n",
            "72 <= measure_0 & measure_0 <= 81 &64 <= measure_1 & measure_1 <= 78\n",
            "2 0 81 90 64 78\n",
            "81 <= measure_0 & measure_0 <= 90 &64 <= measure_1 & measure_1 <= 78\n",
            "3 0 90 99 64 78\n",
            "90 <= measure_0 & measure_0 <= 99 &64 <= measure_1 & measure_1 <= 78\n",
            "4 0 99 108 64 78\n",
            "99 <= measure_0 & measure_0 <= 108 &64 <= measure_1 & measure_1 <= 78\n",
            "5 0 108 117 64 78\n",
            "108 <= measure_0 & measure_0 <= 117 &64 <= measure_1 & measure_1 <= 78\n",
            "6 0 117 126 64 78\n",
            "117 <= measure_0 & measure_0 <= 126 &64 <= measure_1 & measure_1 <= 78\n",
            "7 0 126 135 64 78\n",
            "126 <= measure_0 & measure_0 <= 135 &64 <= measure_1 & measure_1 <= 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r images.zip '/content/generations/trial_0' "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2EpLLp5wUW0",
        "outputId": "507de962-3112-4aa3-9a58-cf163095b6de"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/generations/trial_0/ (stored 0%)\n",
            "  adding: content/generations/trial_0/460.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/410.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/210.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/150.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/370.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/140.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/470.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/330.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/250.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/230.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/280.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/240.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/90.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/60.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/310.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/480.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/270.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/180.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/390.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/120.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/320.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/350.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/380.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/220.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/200.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/100.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/130.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/360.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/450.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/420.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/80.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/500.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/160.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/430.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/40.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/50.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/170.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/490.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/340.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/70.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/30.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/20.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/300.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/110.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/400.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/260.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/10.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/190.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/290.png (deflated 0%)\n",
            "  adding: content/generations/trial_0/440.png (deflated 0%)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "723ea3ceb5d04048bd8d6ea82ed0b30d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c29bf7e651e6417b813bf01137b41644",
              "IPY_MODEL_4e783f47fa9b43439593f0426cbbbcb3",
              "IPY_MODEL_84b46c2efe1f443abd9c94e1a2502d67"
            ],
            "layout": "IPY_MODEL_40500ca36f1b440688dfd918d48d2872"
          }
        },
        "c29bf7e651e6417b813bf01137b41644": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_361c89e8fdd14bc69a13c4add798d3ff",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b84c0b0ce2c84af48498a78c0e52001a",
            "value": "100%"
          }
        },
        "4e783f47fa9b43439593f0426cbbbcb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_14132ba78d41463ea5a306a86346d38b",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b62d3daa53045a3acd92442096037bd",
            "value": 500
          }
        },
        "84b46c2efe1f443abd9c94e1a2502d67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e92357a9884c219ed894a391d78640",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_7186c409f6d0404286136e65a5f92589",
            "value": " 500/500 [29:43&lt;00:00,  4.10s/it]"
          }
        },
        "40500ca36f1b440688dfd918d48d2872": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "361c89e8fdd14bc69a13c4add798d3ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b84c0b0ce2c84af48498a78c0e52001a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14132ba78d41463ea5a306a86346d38b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b62d3daa53045a3acd92442096037bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0e92357a9884c219ed894a391d78640": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7186c409f6d0404286136e65a5f92589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}