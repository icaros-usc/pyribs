{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzwsxS3y2fxR"
      },
      "source": [
        "## **Exploring DQD Algorithms** ðŸ”¬\n",
        "In this tutorial, we demonstrate how to solve a Differentiable Quality problem where we want to generate an archive of solutions that maximize a given objective function that are also diverse with respect to a set of specified measure functions. \n",
        "\n",
        "DQD is a special case of Quality Diversity (QD) problem, where both the objective and measure functions are first order differentiable. We present Covariance Matrix Adaptation MAP-GradientAnnealing (CMA, a DQD algorithm that leverages gradient information to efficiently explore the joint range of the objective and measure functions [Fontaine 2021](https://arxiv.org/abs/2106.03894).\n",
        "\n",
        "_This tutorial assumes that you are familiar with Covariance Matrix Adaptation MAP-Elites (CMA-ME). If you are not familiar with CMA-ME, we recommend reviewing the following tutorials:_\n",
        "- [Using CMA-ME to Land a Lunar Lander Like a Space Shuttle](https://docs.pyribs.org/en/stable/tutorials/lunar_lander.html)\n",
        "- [Illuminating the Latent Space of an MNIST GAN](https://docs.pyribs.org/en/stable/tutorials/lsi_mnist.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "cr-94yAuzYns"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Check GPU type** ðŸ•µï¸\n",
        "#@markdown ### Factory reset runtime if you don't have the desired GPU.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#@markdown V100 = Excellent (*Available only for Colab Pro users*)\n",
        "\n",
        "#@markdown P100 = Very Good\n",
        "\n",
        "#@markdown T4 = Good (*preferred*)\n",
        "\n",
        "#@markdown K80 = Meh\n",
        "\n",
        "#@markdown P4 = (*Not Recommended*) \n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "oJZEFUAfXX2z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #**Install libraries** ðŸ—ï¸\n",
        "# @markdown This cell will take around 5 minutes to download several libraries.\n",
        "\n",
        "!pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
        "# !git clone https://github.com/autonomousvision/stylegan_xl\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "!pip install timm\n",
        "\n",
        "!curl -LO 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhqu-256x256.pkl'\n",
        "!git clone https://github.com/icaros-usc/pyribs.git\n",
        "!pip install ./pyribs[all]\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"./CLIP\")\n",
        "sys.path.append(\"./stylegan2-ada-pytorch\")\n",
        "# sys.path.append('./stylegan_xl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYIf5bwsYPI3",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #**Import Python Modules** ðŸ\n",
        "\n",
        "import io\n",
        "import os, time, glob\n",
        "import pickle\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "import clip\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from ribs.archives import CVTArchive, GridArchive\n",
        "from ribs.emitters import GradientArborescenceEmitter\n",
        "from ribs.schedulers import Scheduler\n",
        "from ribs.visualize import grid_archive_heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0lkdRW5YFBk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #**Define necessary Functions** ðŸ› ï¸\n",
        "def tensor_to_pil_img(img):\n",
        "    img = (img.clamp(-1, 1) + 1) / 2.0\n",
        "    img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
        "    img = Image.fromarray(img.astype('uint8'))\n",
        "    return img\n",
        "\n",
        "def norm1(prompt):\n",
        "    return prompt / prompt.square().sum(dim=-1, keepdim=True).sqrt()\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def cos_sim_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().mul(2)\n",
        "\n",
        "def prompts_dist_loss(x, targets, loss):\n",
        "    if len(targets) == 1: # Keeps consistent results vs previous method for single objective guidance \n",
        "      return loss(x, targets[0])\n",
        "    distances = [loss(x, target) for target in targets]\n",
        "    loss = torch.stack(distances, dim=-1).sum(dim=-1)\n",
        "    return loss\n",
        "\n",
        "def save_heatmap(archive, heatmap_path):\n",
        "    \"\"\"Saves a heatmap of the archive to the given path.\n",
        "    Args:\n",
        "        archive (GridArchive or CVTArchive): The archive to save.\n",
        "        heatmap_path: Image path for the heatmap.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    grid_archive_heatmap(archive, vmin=0, vmax=100, cmap=\"viridis\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(heatmap_path)\n",
        "    plt.close(plt.gcf())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29vaSPaNYEt2",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #**Connect Text and Images using CLIP** ðŸ› ï¸\n",
        "transform = transforms.CenterCrop(224)\n",
        "\n",
        "class CLIP(object):\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        clip_model_name = \"ViT-B/32\"\n",
        "        self.model, _ = clip.load(clip_model_name, device=device)\n",
        "        self.model = self.model.requires_grad_(False)\n",
        "        self.model.eval()\n",
        "        self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                              std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def embed_text(self, prompt):\n",
        "        return norm1(self.model.encode_text(clip.tokenize(prompt)\n",
        "               .to(self.device)).float())\n",
        "\n",
        "    def embed_cutout(self, image):\n",
        "        return norm1(self.model.encode_image(self.normalize(image)))\n",
        "\n",
        "    def embed_image(self, image):\n",
        "        n = image.shape[0]\n",
        "        centered_img = transform(image)\n",
        "        embeds = self.embed_cutout(centered_img)\n",
        "        embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
        "        return embeds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96S2T01tYElA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #**Create STYLE-GAN2 Model** ðŸŽ­\n",
        "class Generator(object):\n",
        "\n",
        "    def __init__(self, device='cpu'):\n",
        "        self.device = device\n",
        "        model_filename = '/content/stylegan2-ffhqu-256x256.pkl'\n",
        "        with open(model_filename, 'rb') as fp:\n",
        "            self.model = pickle.load(fp)['G_ema'].to(device)\n",
        "            self.model.eval()\n",
        "        for p in self.model.parameters():\n",
        "            p.requires_grad_(False)\n",
        "        self.init_stats()\n",
        "        self.latent_shape = (-1, 512)\n",
        "\n",
        "    def init_stats(self):\n",
        "        zs = torch.randn([10000, self.model.mapping.z_dim], device=self.device)\n",
        "        ws = self.model.mapping(zs, None)\n",
        "        self.w_stds = ws.std(0)\n",
        "        qs = ((ws - self.model.mapping.w_avg) / self.w_stds).reshape(10000, -1)\n",
        "        self.q_norm = torch.norm(qs, dim=1).mean() * 0.35\n",
        "\n",
        "    def gen_random_ws(self, num_latents):\n",
        "        zs = torch.randn([num_latents, self.model.mapping.z_dim], device=self.device)\n",
        "        ws = self.model.mapping(zs, None)\n",
        "        return ws"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGvAiVz7YEWK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title #**Classifier with Objective and Measures** ðŸ•µï¸\n",
        "\n",
        "class Classifier(object):\n",
        "\n",
        "    def __init__(self, gen_model, class_model, prompt):\n",
        "        self.device = gen_model.device\n",
        "        self.gen_model = gen_model\n",
        "        self.class_model = class_model\n",
        "        self.measures = []\n",
        "\n",
        "        self.init_objective(f'A photo of the face of {prompt}.')\n",
        "        self.add_measure(f'A photo of {prompt} as a small child.', \n",
        "                         f'A photo of {prompt} as an elderly person.')\n",
        "        self.add_measure(f'A photo of {prompt} with long hair.', \n",
        "                         f'A photo of {prompt} with short hair.')\n",
        "\n",
        "    def init_objective(self, text_prompt):\n",
        "        texts = [frase.strip() for frase in text_prompt.split(\"|\") if frase]\n",
        "        self.obj_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "\n",
        "    def add_measure(self, positive_text, negative_text):\n",
        "        texts = [frase.strip() for frase in positive_text.split(\"|\") if frase]\n",
        "        negative_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "        \n",
        "        texts = [frase.strip() for frase in negative_text.split(\"|\") if frase]\n",
        "        positive_targets = [self.class_model.embed_text(text) for text in texts]\n",
        "        \n",
        "        self.measures.append((negative_targets, positive_targets))\n",
        "\n",
        "    def find_good_start_latent(self, batch_size=16, num_batches=32):\n",
        "        with torch.inference_mode():\n",
        "            qs = []\n",
        "            losses = []\n",
        "            G = self.gen_model.model\n",
        "            w_stds = self.gen_model.w_stds\n",
        "            for _ in range(num_batches):\n",
        "                q = (G.mapping(torch.randn([batch_size, G.mapping.z_dim], device=self.device),\n",
        "                    None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
        "                images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
        "                embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "                loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
        "                i = torch.argmin(loss)\n",
        "                qs.append(q[i])\n",
        "                losses.append(loss[i])\n",
        "            qs = torch.stack(qs)\n",
        "            losses = torch.stack(losses)\n",
        "\n",
        "            i = torch.argmin(losses)\n",
        "            q = qs[i].unsqueeze(0)\n",
        "\n",
        "        return q.flatten()\n",
        "\n",
        "    def generate_image(self, latent_code):\n",
        "        ws, _ = self.transform_to_w([latent_code])\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        return images\n",
        "\n",
        "    def transform_to_w(self, latent_codes):\n",
        "        qs = []\n",
        "        ws = []\n",
        "        for cur_code in latent_codes:\n",
        "            q = torch.tensor(\n",
        "                    cur_code.reshape(self.gen_model.latent_shape), \n",
        "                    device=self.device,\n",
        "                    requires_grad=True,\n",
        "                )\n",
        "            qs.append(q)\n",
        "            w = q * self.gen_model.w_stds + self.gen_model.model.mapping.w_avg\n",
        "            ws.append(w)\n",
        "\n",
        "        ws = torch.stack(ws, dim=0)\n",
        "        return ws, qs\n",
        "\n",
        "    def compute_objective_loss(self, embeds, qs, dim=None):\n",
        "        loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
        "\n",
        "        diff = torch.max(torch.norm(qs, dim=dim), self.gen_model.q_norm)\n",
        "        reg_loss = (diff - self.gen_model.q_norm).pow(2)\n",
        "        loss = loss + 0.2* reg_loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def compute_objective(self, sols):\n",
        "        ws, qs = self.transform_to_w(sols)\n",
        "\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "    \n",
        "        loss = self.compute_objective_loss(embeds, qs[0])\n",
        "        loss.backward()\n",
        "\n",
        "        value = loss.cpu().detach().numpy()\n",
        "        jacobian = -qs[0].grad.cpu().detach().numpy()\n",
        "        return value, jacobian.flatten()\n",
        "\n",
        "    def compute_measure(self, index, sols):\n",
        "        ws, qs = self.transform_to_w(sols)\n",
        "\n",
        "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "\n",
        "        measure_targets = self.measures[index]\n",
        "        pos_loss = prompts_dist_loss(embeds, measure_targets[0], cos_sim_loss).mean(0)\n",
        "        neg_loss = prompts_dist_loss(embeds, measure_targets[1], cos_sim_loss).mean(0)\n",
        "        loss = pos_loss - neg_loss\n",
        "        loss.backward()\n",
        "\n",
        "        value = loss.cpu().detach().numpy()\n",
        "        jacobian = qs[0].grad.cpu().detach().numpy()\n",
        "        return value, jacobian.flatten()\n",
        "\n",
        "    def compute_measures(self, sols):\n",
        "    \n",
        "        values = []\n",
        "        jacobian = []\n",
        "        for i in range(len(self.measures)):\n",
        "            value, jac = self.compute_measure(i, sols)\n",
        "            values.append(value)\n",
        "            jacobian.append(jac)\n",
        "\n",
        "        return np.stack(values, axis=0), np.stack(jacobian, axis=0)\n",
        "\n",
        "    def compute_all(self, sols):\n",
        "        with torch.inference_mode():\n",
        "\n",
        "            ws, qs = self.transform_to_w(sols)\n",
        "            qs = torch.stack(qs, dim=0)\n",
        "\n",
        "            images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
        "            embeds = self.class_model.embed_image(images.add(1).div(2))\n",
        "            \n",
        "            values = []\n",
        "            loss = self.compute_objective_loss(embeds, qs, dim=(1,2))\n",
        "            value = loss.cpu().detach().numpy()\n",
        "            values.append(value)\n",
        "\n",
        "            for i in range(len(self.measures)):\n",
        "                measure_targets = self.measures[i]\n",
        "                pos_loss = prompts_dist_loss(\n",
        "                        embeds, \n",
        "                        measure_targets[0], \n",
        "                        cos_sim_loss,\n",
        "                    ).mean(0)\n",
        "                neg_loss = prompts_dist_loss(\n",
        "                        embeds, \n",
        "                        measure_targets[1], \n",
        "                        cos_sim_loss\n",
        "                    ).mean(0)\n",
        "                loss = pos_loss - neg_loss\n",
        "                value = loss.cpu().detach().numpy()\n",
        "                values.append(value)\n",
        "\n",
        "        return np.stack(values, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-RMRh5kXS8h",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title **Optimizer with GradientArborescence** ðŸ†\n",
        "#@markdown To solve DQD, we introduce the concept of a gradient arborescence. Like gradient ascent, a gradient arborescence makes greedy ascending steps based on the objective. Unlike gradient ascent, a gradient arborescence encourages exploration by branching via the measures $m_i$\n",
        "\n",
        "#@markdown This emitter originates in  [Fontaine 2021](https://arxiv.org/abs/2106.03894). It leverages the gradient information of the objective and measure functions, generating new solutions around a \"solution point\" using gradient arborescence with coefficients drawn from a Gaussian distribution. \n",
        "\n",
        "#@markdown Based on how the solutions are ranked after being inserted into the archive, the solution point is updated with gradient ascent, and the distribution is updated with CMA-ES.\n",
        "\n",
        "\n",
        "measure_space_bounds =[(-0.2, 0.2), (-0.2, 0.2)]\n",
        "\n",
        "def transform_obj(objs):\n",
        "    # Remap the objective from minimizing [0, 10] to maximizing [0, 100]\n",
        "    return (10.0-objs*5.0)*10.0\n",
        "\n",
        "\n",
        "def create_optimizer(algorithm, classifier, seed):\n",
        "    \"\"\"Creates an optimizer based on the algorithm name.\n",
        "\n",
        "    Args:\n",
        "        algorithm (str): Name of the algorithm passed into sphere_main.\n",
        "        classifier (Classifier): The models for the search.\n",
        "        seed (int): Main seed or the various components.\n",
        "    Returns:\n",
        "        Scheduler: A ribs Scheduler for running the algorithm.\n",
        "    \"\"\"\n",
        "    bounds = measure_space_bounds\n",
        "    initial_sol = classifier.find_good_start_latent().cpu().detach().numpy()\n",
        "    dim = len(initial_sol)\n",
        "    batch_size = 32\n",
        "    num_emitters = 1\n",
        "    resolution = 200\n",
        "    grid_dims = (resolution, resolution)\n",
        "\n",
        "    # Create archive.\n",
        "    archive = GridArchive(\n",
        "        solution_dim = dim,\n",
        "        dims=grid_dims, \n",
        "        ranges=bounds, \n",
        "        learning_rate=0.02,\n",
        "        threshold_min=0.0,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Maintain a result elitist archive\n",
        "    result_archive = GridArchive(\n",
        "        solution_dim = dim,\n",
        "        dims=grid_dims,\n",
        "        ranges=bounds,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Create emitters. Each emitter needs a different seed, so that they do not\n",
        "    # all do the same thing.\n",
        "    emitter_seeds = [None] * num_emitters if seed is None else list(\n",
        "        range(seed, seed + num_emitters))\n",
        "    emitters = [\n",
        "        GradientArborescenceEmitter(\n",
        "            archive=archive,\n",
        "            x0=initial_sol,\n",
        "            sigma0=0.01,\n",
        "            lr=0.05,\n",
        "            ranker=\"imp\",\n",
        "            selection_rule=\"mu\",\n",
        "            restart_rule='basic',\n",
        "            batch_size=batch_size,\n",
        "            seed=s,\n",
        "        ) for s in emitter_seeds\n",
        "    ]\n",
        "\n",
        "    return Scheduler(archive, emitters, result_archive= result_archive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cg6QcVj0v2-n",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# Initialization.\n",
        "#@markdown #**Parameters** âœï¸\n",
        "#@markdown `prompt`: Enter a prompt of a Celebrity here to guide the image generation.\n",
        "\n",
        "prompt = \"Tom Cruise\"#@param {type:\"string\"}\n",
        "\n",
        "#@markdown ---\n",
        "outdir='logs'\n",
        "            \n",
        "# Create a shared logging directory for the experiments for this algorithm.\n",
        "s_logdir = os.path.join(outdir, \"cma_maega\")\n",
        "logdir = Path(s_logdir)\n",
        "outdir = Path(outdir)\n",
        "if not outdir.is_dir():\n",
        "    outdir.mkdir()\n",
        "if not logdir.is_dir():\n",
        "    logdir.mkdir()\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "clip_model = CLIP(device=device)\n",
        "gen_model = Generator(device=device)\n",
        "classifier = Classifier(gen_model, clip_model, prompt=prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8dVRi1TvM_w",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Run the model** ðŸš€\n",
        "#@markdown #Will take ~10 minutes per 1000 iterations\n",
        "#StyleGAN+CLIP LSI experiments.\n",
        "trials=1\n",
        "init_pop=100\n",
        "total_itrs=3000\n",
        "log_freq=1\n",
        "log_arch_freq=1000\n",
        "image_monitor=True\n",
        "image_monitor_freq=10\n",
        "seed=None\n",
        "\n",
        "for trial_id in range(trials):\n",
        "    print(\"Running trial\", trial_id)\n",
        "    # Create a directory for this specific trial.\n",
        "    s_logdir = os.path.join(outdir, \"cma_maega\", f\"trial_{trial_id}\")\n",
        "    logdir = Path(s_logdir)\n",
        "    if not logdir.is_dir():\n",
        "        logdir.mkdir()\n",
        "\n",
        "    # Create a directory for logging intermediate images if the monitor is on.\n",
        "    if image_monitor:\n",
        "        image_monitor_freq = max(1, image_monitor_freq)\n",
        "        gen_output_dir = os.path.join('generations')\n",
        "        logdir = Path(gen_output_dir)\n",
        "        if not logdir.is_dir():\n",
        "            logdir.mkdir()\n",
        "        gen_output_dir = os.path.join('generations', f\"trial_{trial_id}\")\n",
        "        logdir = Path(gen_output_dir)\n",
        "        if not logdir.is_dir():\n",
        "            logdir.mkdir()\n",
        "\n",
        "    # Create a new summary file\n",
        "    summary_filename = os.path.join(s_logdir, \"summary.csv\")\n",
        "    if os.path.exists(summary_filename):\n",
        "        os.remove(summary_filename)\n",
        "    with open(summary_filename, 'w') as summary_file:\n",
        "        writer = csv.writer(summary_file)\n",
        "        writer.writerow(['Iteration', 'QD-Score', 'Coverage', 'Maximum', 'Average'])\n",
        "    \n",
        "    scheduler = create_optimizer(algorithm=\"cma_maega\", classifier=classifier,seed = seed)\n",
        "    archive = scheduler.archive\n",
        "\n",
        "    best = -1000\n",
        "    non_logging_time = 0.0\n",
        "    for itr in tqdm(range(1, total_itrs + 1)):\n",
        "        itr_start = time.time()\n",
        "        \n",
        "        sols = scheduler.ask_dqd()\n",
        "        objs, jacobian_obj = classifier.compute_objective(sols)\n",
        "        objs = transform_obj(objs)\n",
        "        best = max(best, objs)\n",
        "\n",
        "        measures, jacobian_measure = classifier.compute_measures(sols)\n",
        "\n",
        "        jacobian_obj = np.expand_dims(jacobian_obj, axis=0)\n",
        "        jacobian = np.concatenate((jacobian_obj, jacobian_measure), axis=0)\n",
        "        jacobian = np.expand_dims(jacobian, axis=0)\n",
        "\n",
        "        measures = np.transpose(measures) \n",
        "\n",
        "        objs = objs.astype(np.float32)\n",
        "        measures = measures.astype(np.float32)\n",
        "        jacobian = jacobian.astype(np.float32)\n",
        "\n",
        "        scheduler.tell_dqd(objs, measures, jacobian)\n",
        "\n",
        "        sols = scheduler.ask()\n",
        "        values = classifier.compute_all(sols)\n",
        "        values = np.transpose(values)\n",
        "\n",
        "        objs = values[:,0]\n",
        "        measures = values[:,1:3]\n",
        "\n",
        "        objs = transform_obj(np.array(objs, dtype=np.float32))\n",
        "        measures = np.array(measures, dtype=np.float32)\n",
        "        \n",
        "        best_gen = max(objs) \n",
        "        best = max(best, best_gen)\n",
        "\n",
        "        scheduler.tell(objs, measures)\n",
        "\n",
        "        non_logging_time += time.time() - itr_start\n",
        "\n",
        "        if itr%50 == 0:\n",
        "          print('best', best, best_gen)\n",
        "\n",
        "        if image_monitor and itr % image_monitor_freq == 0:\n",
        "            best_index = np.argmax(objs)\n",
        "            latent_code = sols[best_index]\n",
        "\n",
        "            img = classifier.generate_image(latent_code)\n",
        "            img = tensor_to_pil_img(img)\n",
        "            img.save(os.path.join(gen_output_dir, f'{itr}.png'))\n",
        "\n",
        "        # Save the archive at the given frequency.\n",
        "        # Always save on the final iteration.\n",
        "        final_itr = itr == total_itrs\n",
        "        result_archive = scheduler.result_archive\n",
        "        if (itr > 0 and itr % log_arch_freq == 0) or final_itr:\n",
        "            # Save a full archive for analysis.\n",
        "            df = result_archive.as_pandas(include_solutions = final_itr)\n",
        "            df.to_pickle(os.path.join(s_logdir, f\"archive_{itr:08d}.pkl\"))\n",
        "\n",
        "            # Save a heatmap image to observe how the trial is doing.\n",
        "            save_heatmap(result_archive, os.path.join(s_logdir, f\"heatmap_{itr:08d}.png\"))\n",
        "\n",
        "        # Update the summary statistics for the archive\n",
        "        if (itr > 0 and itr % log_freq == 0) or final_itr:\n",
        "            with open(summary_filename, 'a') as summary_file:\n",
        "                writer = csv.writer(summary_file)\n",
        "                stats = result_archive.stats\n",
        "                qd_score = stats.qd_score/result_archive.cells \n",
        "                average = stats.obj_mean\n",
        "                coverage = stats.coverage\n",
        "                data = [itr, qd_score, coverage, best, average]\n",
        "                writer.writerow(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgLc1FnGMYQg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Create a Collage** ðŸ“¸\n",
        "archive_filename = '/content/logs/cma_maega/trial_0/archive_00003000.pkl'\n",
        "\n",
        "# Controls that x rows and y columns are generated\n",
        "# Images are \"evenly\" (as possible) sampled based on this criteria\n",
        "picture_frequency = (8, 5) \n",
        "\n",
        "# Use the CPU while we are running exps.\n",
        "#device = \"cpu\"\n",
        "device = \"cuda\"\n",
        "\n",
        "# Uncomment to save all grid images separately.\n",
        "gen_output_dir = os.path.join('grid_imgs')\n",
        "logdir = Path(gen_output_dir)\n",
        "if not logdir.is_dir():\n",
        "    logdir.mkdir()\n",
        "\n",
        "model_filename = '/content/stylegan2-ffhqu-256x256.pkl'\n",
        "with open(model_filename, 'rb') as fp:\n",
        "    model = pickle.load(fp)['G_ema'].to(device)\n",
        "    model.eval()\n",
        "latent_shape = (1, -1, 512)\n",
        "\n",
        "zs = torch.randn([10000, model.mapping.z_dim], device=device)\n",
        "ws = model.mapping(zs, None)\n",
        "w_stds = ws.std(0)\n",
        "qs = ((ws - model.mapping.w_avg) / w_stds).reshape(10000, -1)\n",
        "q_norm = torch.norm(qs, dim=1).mean() * 0.1\n",
        "\n",
        "# Read the archive from the log (pickle file)\n",
        "df = pd.read_pickle(archive_filename)\n",
        "# print(df.describe()) \n",
        "\n",
        "imgs = []\n",
        "\n",
        "for j in np.arange(measure_space_bounds[1][0], measure_space_bounds[1][1], 0.08):\n",
        "  for i in np.arange(measure_space_bounds[0][0], measure_space_bounds[0][1], 0.05):\n",
        "\n",
        "        query_string = f\"{i} <= measure_0 & measure_0 <= {i+0.05} & \"\n",
        "        query_string += f\"{j} <= measure_1 & measure_1 <= {j+0.08}\" \n",
        "\n",
        "        # print(query_string)\n",
        "        df_cell = df.query(query_string)\n",
        "        # print(df_cell)\n",
        "\n",
        "        if not df_cell.empty:\n",
        "          sol = df_cell.iloc[df_cell['objective'].argmax()]\n",
        "\n",
        "          q = torch.tensor(\n",
        "                  sol[4:].values.reshape(latent_shape),\n",
        "                  device=device,\n",
        "                  requires_grad=True,\n",
        "              )\n",
        "          w = q * w_stds + model.mapping.w_avg\n",
        "\n",
        "          img = model.synthesis(w, noise_mode='const')\n",
        "          img = (img.clamp(-1, 1) + 1) / 2.0 # Normalize from [0,1]\n",
        "\n",
        "          pil_img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
        "          pil_img = Image.fromarray(pil_img.astype('uint8'))\n",
        "          pil_img.save(os.path.join(gen_output_dir, f'{j}_{i}.png'))\n",
        "\n",
        "          img = img[0].detach().cpu()\n",
        "          imgs.append(img)\n",
        "        else:\n",
        "          imgs.append(torch.zeros((3,256,256)))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "img_grid = make_grid(imgs, nrow=picture_frequency[0], padding=0)\n",
        "img_grid = np.transpose(img_grid.cpu().numpy(), (1,2,0))\n",
        "plt.imshow(img_grid)\n",
        "\n",
        "plt.xlabel(\"Age\")\n",
        "plt.ylabel(\"Hair Length\")\n",
        "\n",
        "def create_archive_tick_labels(measure_range, num_ticks):\n",
        "    low_pos = measure_range[0]\n",
        "    high_pos = measure_range[1]\n",
        "\n",
        "    tick_offset = [\n",
        "        (high_pos - low_pos) * (p / num_ticks) + low_pos\n",
        "        for p in range(num_ticks+1) \n",
        "    ]\n",
        "    ticklabels = [\n",
        "        round((measure_range[1]-measure_range[0]) * p + measure_range[0], 2)\n",
        "        for p in tick_offset\n",
        "    ]\n",
        "    return ticklabels\n",
        "\n",
        "num_x_ticks = 6\n",
        "num_y_ticks = 6\n",
        "x_ticklabels = create_archive_tick_labels( measure_space_bounds[0], num_x_ticks)\n",
        "y_ticklabels = create_archive_tick_labels(measure_space_bounds[1], num_y_ticks)\n",
        "y_ticklabels.reverse()\n",
        "\n",
        "x_tick_range = img_grid.shape[1]\n",
        "x_ticks = np.arange(0, x_tick_range+1e-9, step=x_tick_range/num_x_ticks)\n",
        "y_tick_range = img_grid.shape[0]\n",
        "y_ticks = np.arange(0, y_tick_range+1e-9, step=y_tick_range/num_y_ticks)\n",
        "plt.xticks(x_ticks, x_ticklabels)\n",
        "plt.yticks(y_ticks, y_ticklabels)\n",
        "plt.tight_layout()\n",
        "ax = plt.axes()\n",
        "ax.set_facecolor(\"white\")\n",
        "plt.savefig('collage.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HUTQpHG1rRay"
      },
      "outputs": [],
      "source": [
        "# !zip -r /content/images.zip /content/generations/trial_0"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}