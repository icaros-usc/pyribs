{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c0746a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/pyribs/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import fire\n",
    "import clip\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from einops import rearrange\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from alive_progress import alive_bar\n",
    "from PIL import Image\n",
    "\n",
    "from ribs.archives import CVTArchive, GridArchive\n",
    "from ribs.emitters import GradientAborescenceEmitter\n",
    "from ribs.schedulers import Scheduler\n",
    "from ribs.visualize import grid_archive_heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3277cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_pil_img(img):\n",
    "    img = (img.clamp(-1, 1) + 1) / 2.0\n",
    "    img = img[0].permute(1, 2, 0).detach().cpu().numpy() * 255\n",
    "    img = Image.fromarray(img.astype('uint8'))\n",
    "    return img\n",
    "\n",
    "def norm1(prompt):\n",
    "    return prompt / prompt.square().sum(dim=-1, keepdim=True).sqrt()\n",
    "\n",
    "def spherical_dist_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
    "\n",
    "def cos_sim_loss(x, y):\n",
    "    x = F.normalize(x, dim=-1)\n",
    "    y = F.normalize(y, dim=-1)\n",
    "    return (x - y).norm(dim=-1).div(2).arcsin().mul(2)\n",
    "\n",
    "def prompts_dist_loss(x, targets, loss):\n",
    "    distances = [loss(x, target) for target in targets]\n",
    "    return torch.stack(distances, dim=-1).sum(dim=-1)\n",
    "\n",
    "class MakeCutouts(torch.nn.Module):\n",
    "    def __init__(self, cut_size, cutn, cut_pow=1.0):\n",
    "        super().__init__()\n",
    "        self.cut_size = cut_size\n",
    "        self.cutn = cutn\n",
    "        self.cut_pow = cut_pow\n",
    "\n",
    "    def forward(self, x):\n",
    "        sideY, sideX = x.shape[2:4]\n",
    "        max_size = min(sideX, sideY)\n",
    "        min_size = min(sideX, sideY, self.cut_size)\n",
    "        cutouts = []\n",
    "        for _ in range(self.cutn):\n",
    "            size = int(torch.rand([]) ** self.cut_pow * (max_size - min_size) + min_size)\n",
    "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
    "            offsety = torch.randint(0, sideY - size + 1, ())\n",
    "            cutout = x[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
    "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
    "        return torch.cat(cutouts)\n",
    "\n",
    "make_cutouts = MakeCutouts(224, 32, 0.5)\n",
    "\n",
    "class CLIP(object):\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        clip_model_name = \"ViT-B/32\"\n",
    "        self.model, _ = clip.load(clip_model_name, device=device)\n",
    "        self.model = self.model.requires_grad_(False)\n",
    "        self.model.eval()\n",
    "        self.normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
    "                                              std=[0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def embed_text(self, prompt):\n",
    "        return norm1(self.model.encode_text(clip.tokenize(prompt)\n",
    "               .to(self.device)).float())\n",
    "\n",
    "    def embed_cutout(self, image):\n",
    "        return norm1(self.model.encode_image(self.normalize(image)))\n",
    "\n",
    "    def embed_image(self, image):\n",
    "        n = image.shape[0]\n",
    "        cutouts = make_cutouts(image)\n",
    "        embeds = self.embed_cutout(cutouts)\n",
    "        embeds = rearrange(embeds, '(cc n) c -> cc n c', n=n)\n",
    "        return embeds\n",
    "\n",
    "class Generator(object):\n",
    "\n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        model_filename = 'models/stylegan2-ffhq-1024x1024.pkl'\n",
    "        with open(model_filename, 'rb') as fp:\n",
    "            self.model = pickle.load(fp)['G_ema'].to(device)\n",
    "            self.model.eval()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad_(False)\n",
    "        self.init_stats()\n",
    "        self.latent_shape = (-1, 512)\n",
    "\n",
    "    def init_stats(self):\n",
    "        zs = torch.randn([10000, self.model.mapping.z_dim], device=self.device)\n",
    "        ws = self.model.mapping(zs, None)\n",
    "        self.w_stds = ws.std(0)\n",
    "        qs = ((ws - self.model.mapping.w_avg) / self.w_stds).reshape(10000, -1)\n",
    "        self.q_norm = torch.norm(qs, dim=1).mean() * 0.1\n",
    "\n",
    "class Classifier(object):\n",
    "\n",
    "    def __init__(self, gen_model, class_model, celebrity_id='Lopez'):\n",
    "        self.device = gen_model.device\n",
    "        self.gen_model = gen_model\n",
    "        self.class_model = class_model\n",
    "        self.measures = []\n",
    "        if celebrity_id == 'Beyonce':\n",
    "            self.init_objective('A photo of the face of Beyonce.')\n",
    "            self.add_measure('A small child.', 'An elderly person.')\n",
    "            self.add_measure('A person with short hair.', \n",
    "                             'A person with long hair.')\n",
    "        elif celebrity_id == 'Cruise':\n",
    "            self.init_objective('A photo of the face of Tom Cruise.')\n",
    "            self.add_measure('A small child.', 'An elderly person.')\n",
    "            self.add_measure('A person with short hair.', \n",
    "                             'A person with long hair.')\n",
    "        elif celebrity_id == 'Lopez':\n",
    "            self.init_objective('A photo of the face of Jennifer Lopez.')\n",
    "            self.add_measure('A small child.', 'An elderly person.')\n",
    "            self.add_measure('A person with short hair.', \n",
    "                             'A person with long hair.')\n",
    "        elif celebrity_id == 'Musk':\n",
    "            self.init_objective('A photo of the face of Elon Musk.')\n",
    "            self.add_measure('A small child.', 'An elderly person.')\n",
    "            self.add_measure('A person with short hair.', \n",
    "                             'A person with long hair.')\n",
    "        elif celebrity_id == 'Zuckerberg':\n",
    "            self.init_objective('A photo of the face of Mark Zuckerberg.')\n",
    "            self.add_measure('A small child.', 'An elderly person.')\n",
    "            self.add_measure('A person with short hair.', \n",
    "                             'A person with long hair.')\n",
    "        else:\n",
    "            print('The celebrity \\\"{}\\\" is not a valid option.'.format(celebrity_id))\n",
    "            exit(0)\n",
    "\n",
    "    def init_objective(self, text_prompt):\n",
    "        texts = [frase.strip() for frase in text_prompt.split(\"|\") if frase]\n",
    "        self.obj_targets = [self.class_model.embed_text(text) for text in texts]\n",
    "\n",
    "    def add_measure(self, positive_text, negative_text):\n",
    "        texts = [frase.strip() for frase in positive_text.split(\"|\") if frase]\n",
    "        negative_targets = [self.class_model.embed_text(text) for text in texts]\n",
    "        \n",
    "        texts = [frase.strip() for frase in negative_text.split(\"|\") if frase]\n",
    "        positive_targets = [self.class_model.embed_text(text) for text in texts]\n",
    "        \n",
    "        self.measures.append((negative_targets, positive_targets))\n",
    "\n",
    "    def find_good_start_latent(self, batch_size=16, num_batches=32):\n",
    "        with torch.no_grad():\n",
    "            qs = []\n",
    "            losses = []\n",
    "            G = self.gen_model.model\n",
    "            w_stds = self.gen_model.w_stds\n",
    "            for _ in range(num_batches):\n",
    "                q = (G.mapping(torch.randn([batch_size, G.mapping.z_dim], device=self.device),\n",
    "                    None, truncation_psi=0.7) - G.mapping.w_avg) / w_stds\n",
    "                images = G.synthesis(q * w_stds + G.mapping.w_avg)\n",
    "                embeds = self.class_model.embed_image(images.add(1).div(2))\n",
    "                loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
    "                i = torch.argmin(loss)\n",
    "                qs.append(q[i])\n",
    "                losses.append(loss[i])\n",
    "            qs = torch.stack(qs)\n",
    "            losses = torch.stack(losses)\n",
    "            print(losses)\n",
    "            print(losses.shape, qs.shape)\n",
    "            i = torch.argmin(losses)\n",
    "            q = qs[i].unsqueeze(0).requires_grad_()\n",
    "\n",
    "        return q.flatten()\n",
    "\n",
    "    def generate_image(self, latent_code):\n",
    "        ws, _ = self.transform_to_w([latent_code])\n",
    "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
    "        return images\n",
    "\n",
    "    def transform_to_w(self, latent_codes):\n",
    "        qs = []\n",
    "        ws = []\n",
    "        for cur_code in latent_codes:\n",
    "            q = torch.tensor(\n",
    "                    cur_code.reshape(self.gen_model.latent_shape), \n",
    "                    device=self.device,\n",
    "                    requires_grad=True,\n",
    "                )\n",
    "            qs.append(q)\n",
    "            w = q * self.gen_model.w_stds + self.gen_model.model.mapping.w_avg\n",
    "            ws.append(w)\n",
    "\n",
    "        ws = torch.stack(ws, dim=0)\n",
    "        return ws, qs\n",
    "\n",
    "    def compute_objective(self, sols):\n",
    "        ws, qs = self.transform_to_w(sols)\n",
    "\n",
    "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
    "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
    "    \n",
    "        loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
    "        loss = loss + 0.01 * (self.gen_model.q_norm - torch.norm(qs[0])).pow(2)\n",
    "        loss.backward()\n",
    "\n",
    "        value = loss.cpu().detach().numpy()\n",
    "        jacobian = -qs[0].grad.cpu().detach().numpy()\n",
    "        return value, jacobian.flatten()\n",
    "\n",
    "    def compute_measure(self, index, sols):\n",
    "        ws, qs = self.transform_to_w(sols)\n",
    "\n",
    "        images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
    "        embeds = self.class_model.embed_image(images.add(1).div(2))\n",
    "\n",
    "        measure_targets = self.measures[index]\n",
    "        pos_loss = prompts_dist_loss(embeds, measure_targets[0], cos_sim_loss).mean(0)\n",
    "        neg_loss = prompts_dist_loss(embeds, measure_targets[1], cos_sim_loss).mean(0)\n",
    "        loss = pos_loss - neg_loss\n",
    "        loss.backward()\n",
    "\n",
    "        value = loss.cpu().detach().numpy()\n",
    "        jacobian = qs[0].grad.cpu().detach().numpy()\n",
    "        return value, jacobian.flatten()\n",
    "\n",
    "    def compute_measures(self, sols):\n",
    "    \n",
    "        values = []\n",
    "        jacobian = []\n",
    "        for i in range(len(self.measures)):\n",
    "            value, jac = self.compute_measure(i, sols)\n",
    "            values.append(value)\n",
    "            jacobian.append(jac)\n",
    "\n",
    "        return np.stack(values, axis=0), np.stack(jacobian, axis=0)\n",
    "\n",
    "    def compute_all(self, sols):\n",
    "        with torch.inference_mode():\n",
    "\n",
    "            ws, qs = self.transform_to_w(sols)\n",
    "            qs = torch.stack(qs, dim=0)\n",
    "\n",
    "            images = self.gen_model.model.synthesis(ws, noise_mode='const')\n",
    "            embeds = self.class_model.embed_image(images.add(1).div(2))\n",
    "            \n",
    "            values = []\n",
    "            loss = prompts_dist_loss(embeds, self.obj_targets, spherical_dist_loss).mean(0)\n",
    "            loss = loss + 0.01 * (self.gen_model.q_norm - torch.norm(qs, dim=(1,2))).pow(2)\n",
    "            value = loss.cpu().detach().numpy()\n",
    "            values.append(value)\n",
    "            \n",
    "            for i in range(len(self.measures)):\n",
    "                measure_targets = self.measures[i]\n",
    "                pos_loss = prompts_dist_loss(\n",
    "                        embeds, \n",
    "                        measure_targets[0], \n",
    "                        cos_sim_loss,\n",
    "                    ).mean(0)\n",
    "                neg_loss = prompts_dist_loss(\n",
    "                        embeds, \n",
    "                        measure_targets[1], \n",
    "                        cos_sim_loss\n",
    "                    ).mean(0)\n",
    "                loss = pos_loss - neg_loss\n",
    "                value = loss.cpu().detach().numpy()\n",
    "                values.append(value)\n",
    "\n",
    "        return np.stack(values, axis=0)\n",
    "\n",
    "\n",
    "def transform_obj(objs):\n",
    "    # Remap the objective from minimizing [0, 20] to maximizing [0, 100]\n",
    "    return (20.0-objs)*5.0\n",
    "\n",
    "def create_optimizer(algorithm, classifier, seed):\n",
    "    \"\"\"Creates an optimizer based on the algorithm name.\n",
    "\n",
    "    Args:\n",
    "        algorithm (str): Name of the algorithm passed into sphere_main.\n",
    "        classifier (Classifier): The models for the search.\n",
    "        seed (int): Main seed or the various components.\n",
    "    Returns:\n",
    "        Optimizer: A ribs Optimizer for running the algorithm.\n",
    "    \"\"\"\n",
    "    bounds = [(-0.3, 0.3), (-0.3, 0.3)]\n",
    "    print(\"Finding good start latent.\")\n",
    "    initial_sol = classifier.find_good_start_latent().cpu().detach().numpy()\n",
    "    dim = len(initial_sol)\n",
    "    batch_size = 36\n",
    "    num_emitters = 1\n",
    "    resolution = 200\n",
    "    grid_dims = (resolution, resolution)\n",
    "\n",
    "    # Create archive.\n",
    "    archive = GridArchive(\n",
    "            grid_dims, bounds, \n",
    "            archive_learning_rate=0.02,\n",
    "            threshold_floor=0.0,\n",
    "            seed=seed,\n",
    "    )\n",
    "\n",
    "    # Maintain a passive elitist archive\n",
    "    passive_archive = GridArchive(grid_dims, bounds, seed=seed)\n",
    "    passive_archive.initialize(dim)\n",
    "\n",
    "    # Create emitters. Each emitter needs a different seed, so that they do not\n",
    "    # all do the same thing.\n",
    "    emitter_seeds = [None] * num_emitters if seed is None else list(\n",
    "        range(seed, seed + num_emitters))\n",
    "    emitters = [\n",
    "        GradientAborescenceEmitter(archive,\n",
    "                         initial_sol,\n",
    "                         0.03,\n",
    "                         restart_rule='basic',\n",
    "                         timeout=300,\n",
    "                         batch_size=batch_size,\n",
    "                         seed=s) for s in emitter_seeds\n",
    "    ]\n",
    "\n",
    "    return Scheduler(archive, emitters), passive_archive\n",
    "\n",
    "def save_heatmap(archive, heatmap_path):\n",
    "    \"\"\"Saves a heatmap of the archive to the given path.\n",
    "\n",
    "    Args:\n",
    "        archive (GridArchive or CVTArchive): The archive to save.\n",
    "        heatmap_path: Image path for the heatmap.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    grid_archive_heatmap(archive, vmin=0, vmax=100, cmap=\"viridis\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(heatmap_path)\n",
    "    plt.close(plt.gcf())\n",
    "\n",
    "# def run_experiment(algorithm,\n",
    "#                    trial_id,\n",
    "#                    classifier,\n",
    "#                    device,\n",
    "#                    init_pop=100,\n",
    "#                    itrs=10000,\n",
    "#                    outdir=\"logs\",\n",
    "#                    log_freq=1,\n",
    "#                    log_arch_freq=1000,\n",
    "#                    image_monitor=False,\n",
    "#                    image_monitor_freq=5,\n",
    "#                    seed=None):\n",
    " \n",
    "#     # Create a directory for this specific trial.\n",
    "#     s_logdir = os.path.join(outdir, f\"{algorithm}\", f\"trial_{trial_id}\")\n",
    "#     logdir = Path(s_logdir)\n",
    "#     if not logdir.is_dir():\n",
    "#         logdir.mkdir()\n",
    "\n",
    "#     # Create a directory for logging intermediate images if the monitor is on.\n",
    "#     if image_monitor:\n",
    "#         image_monitor_freq = max(1, image_monitor_freq)\n",
    "#         gen_output_dir = os.path.join('generations')\n",
    "#         logdir = Path(gen_output_dir)\n",
    "#         if not logdir.is_dir():\n",
    "#             logdir.mkdir()\n",
    "#         gen_output_dir = os.path.join('generations', f\"trial_{trial_id}\")\n",
    "#         logdir = Path(gen_output_dir)\n",
    "#         if not logdir.is_dir():\n",
    "#             logdir.mkdir()\n",
    "\n",
    "#     # Create a new summary file\n",
    "#     summary_filename = os.path.join(s_logdir, \"summary.csv\")\n",
    "#     if os.path.exists(summary_filename):\n",
    "#         os.remove(summary_filename)\n",
    "#     with open(summary_filename, 'w') as summary_file:\n",
    "#         writer = csv.writer(summary_file)\n",
    "#         writer.writerow(['Iteration', 'QD-Score', 'Coverage', 'Maximum', 'Average'])\n",
    "\n",
    "#     is_dqd = algorithm in ['cma_mega', 'cma_mega_adam', 'cma_maega']\n",
    "    \n",
    "#     print(\"Creating Optimizer\")\n",
    "#     scheduler, passive_archive = create_optimizer(algorithm, classifier, seed)\n",
    "#     archive = optimizer.archive\n",
    "\n",
    "#     best = -1000\n",
    "#     non_logging_time = 0.0\n",
    "#     print(\"starting training\")\n",
    "#     for itr in tqdm(range(1, itrs + 1)):\n",
    "#         itr_start = time.time()\n",
    "\n",
    "# #         if is_dqd:\n",
    "# #             sols = optimizer.ask(grad_estimate=True)\n",
    "\n",
    "# #             objs, jacobian_obj = classifier.compute_objective(sols)\n",
    "# #             objs = transform_obj(objs)\n",
    "# #             best = max(best, max(objs))\n",
    "\n",
    "# #             measures, jacobian_measure = classifier.compute_measures(sols)\n",
    "\n",
    "# #             jacobian_obj = np.expand_dims(jacobian_obj, axis=0) \n",
    "# #             jacobian = np.concatenate((jacobian_obj, jacobian_measure), axis=0)\n",
    "# #             jacobian = np.expand_dims(jacobian, axis=0)\n",
    "\n",
    "# #             measures = np.transpose(measures) \n",
    "# #             print(measures)\n",
    "\n",
    "# #             objs = objs.astype(np.float32)\n",
    "# #             measures = measures.astype(np.float32)\n",
    "# #             jacobian = jacobian.astype(np.float32)\n",
    "\n",
    "# #             optimizer.tell(objs, measures, jacobian=jacobian)\n",
    "\n",
    "# #             # Update the passive elitist archive.\n",
    "# #             for i in range(len(sols)):\n",
    "# #                 passive_archive.add(sols[i], objs[i], measures[i])\n",
    "\n",
    "#         sols = scheduler.ask()\n",
    "\n",
    "#         values = classifier.compute_all(sols)\n",
    "#         values = np.transpose(values)\n",
    "\n",
    "#         objs = values[:,0]\n",
    "#         measures = values[:,1:3]\n",
    "\n",
    "#         objs = transform_obj(np.array(objs, dtype=np.float32))\n",
    "#         measures = np.array(measures, dtype=np.float32)\n",
    "\n",
    "#         best_gen = max(objs) \n",
    "#         best = max(best, best_gen)\n",
    "\n",
    "#         scheduler.tell(objs, measures)\n",
    "\n",
    "#         # Update the passive elitist archive.\n",
    "#         for i in range(len(sols)):\n",
    "#             passive_archive.add(sols[i], objs[i], measures[i])\n",
    "\n",
    "#         non_logging_time += time.time() - itr_start\n",
    "\n",
    "#         print('best', best, best_gen)\n",
    "\n",
    "#         if image_monitor and itr % image_monitor_freq == 0:\n",
    "#             best_index = np.argmax(objs)\n",
    "#             latent_code = sols[best_index]\n",
    "\n",
    "#             img = classifier.generate_image(latent_code)\n",
    "#             img = tensor_to_pil_img(img)\n",
    "#             img.save(os.path.join(gen_output_dir, f'{itr}.png'))\n",
    "\n",
    "#         # Save the archive at the given frequency.\n",
    "#         # Always save on the final iteration.\n",
    "#         final_itr = itr == itrs\n",
    "#         if (itr > 0 and itr % log_arch_freq == 0) or final_itr:\n",
    "#             # Save a full archive for analysis.\n",
    "#             df = passive_archive.as_pandas(include_solutions = final_itr)\n",
    "#             df.to_pickle(os.path.join(s_logdir, f\"archive_{itr:08d}.pkl\"))\n",
    "\n",
    "#             # Save a heatmap image to observe how the trial is doing.\n",
    "#             save_heatmap(passive_archive, os.path.join(s_logdir, f\"heatmap_{itr:08d}.png\"))\n",
    "\n",
    "#         # Update the summary statistics for the archive\n",
    "#         if (itr > 0 and itr % log_freq == 0) or final_itr:\n",
    "#             with open(summary_filename, 'a') as summary_file:\n",
    "#                 writer = csv.writer(summary_file)\n",
    "\n",
    "#                 sum_obj = 0\n",
    "#                 num_filled = 0\n",
    "#                 num_bins = passive_archive.bins\n",
    "#                 for sol, obj, beh, idx, meta in zip(*passive_archive.data()):\n",
    "#                     num_filled += 1\n",
    "#                     sum_obj += obj\n",
    "#                 qd_score = sum_obj / num_bins\n",
    "#                 average = sum_obj / num_filled\n",
    "#                 coverage = 100.0 * num_filled / num_bins\n",
    "#                 data = [itr, qd_score, coverage, best, average]\n",
    "#                 writer.writerow(data)\n",
    "\n",
    "\n",
    "# def lsi_main(algorithm, \n",
    "#              trials=5,\n",
    "#              init_pop=100,\n",
    "#              itrs=10000,\n",
    "#              celebrity='Lopez',\n",
    "#              outdir='logs',\n",
    "#              log_freq=1,\n",
    "#              log_arch_freq=1000,\n",
    "#              image_monitor=False,\n",
    "#              image_monitor_freq=5,\n",
    "#              seed=None):\n",
    "#     \"\"\"Experimental tool for the StyleGAN+CLIP LSI experiments.\n",
    "\n",
    "#     Args:\n",
    "#         algorithm (str): Name of the algorithm.\n",
    "#         trials (int): Number of experimental trials to run.\n",
    "#         init_pop (int): Initial population size for MAP-Elites (ignored for CMA variants).\n",
    "#         itrs (int): Iterations to run.\n",
    "#         celebrity (str): Which celebrity experiment to run. Options: {Beyonce, Cruise, Lopez, Musk, Zuckerberg}. \n",
    "#         outdir (str): Directory to save output.\n",
    "#         log_freq (int): Number of iterations between computing QD metrics and updating logs.\n",
    "#         log_arch_freq (int): Number of iterations between saving an archive and generating heatmaps.\n",
    "#         image_monitor (bool): Flags if images should be saved every few iterations.\n",
    "#         image_monitor_freq (int): Number of iterations between saving images.\n",
    "#         seed (int): Seed for the algorithm. By default, there is no seed.\n",
    "#     \"\"\"\n",
    "   \n",
    "#     # Create a shared logging directory for the experiments for this algorithm.\n",
    "#     s_logdir = os.path.join(outdir, f\"{algorithm}\")\n",
    "#     logdir = Path(s_logdir)\n",
    "#     outdir = Path(outdir)\n",
    "#     if not outdir.is_dir():\n",
    "#         outdir.mkdir()\n",
    "#     if not logdir.is_dir():\n",
    "#         logdir.mkdir()\n",
    "\n",
    "#     use_cuda = torch.cuda.is_available()\n",
    "#     device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    \n",
    "#     clip_model = CLIP(device=device)\n",
    "#     gen_model = Generator(device=device)\n",
    "#     classifier = Classifier(gen_model, clip_model, celebrity_id=celebrity)\n",
    "\n",
    "#     for cur_id in range(trials):\n",
    "#         run_experiment(algorithm, cur_id, classifier, device, \n",
    "#                        init_pop=init_pop, itrs=itrs,\n",
    "#                        outdir=outdir, log_freq=log_freq, \n",
    "#                        log_arch_freq=log_arch_freq, \n",
    "#                        image_monitor=image_monitor, \n",
    "#                        image_monitor_freq=image_monitor_freq, \n",
    "#                        seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e8e1ab2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization.\n",
    "celebrity='Lopez'\n",
    "outdir='logs'\n",
    "            \n",
    "# Create a shared logging directory for the experiments for this algorithm.\n",
    "s_logdir = os.path.join(outdir, \"cma_mae\")\n",
    "logdir = Path(s_logdir)\n",
    "outdir = Path(outdir)\n",
    "if not outdir.is_dir():\n",
    "    outdir.mkdir()\n",
    "if not logdir.is_dir():\n",
    "    logdir.mkdir()\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "clip_model = CLIP(device=device)\n",
    "gen_model = Generator(device=device)\n",
    "classifier = Classifier(gen_model, clip_model, celebrity_id=celebrity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e4d8481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trial 0\n",
      "Creating Optimizer\n",
      "Finding good start latent.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQD-Score\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCoverage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaximum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating Optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m scheduler, passive_archive \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_optimizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcma_mae\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m archive \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39marchive\n\u001b[1;32m     43\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mcreate_optimizer\u001b[0;34m(algorithm, classifier, seed)\u001b[0m\n\u001b[1;32m    274\u001b[0m bounds \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m), (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.3\u001b[39m)]\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding good start latent.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 276\u001b[0m initial_sol \u001b[38;5;241m=\u001b[39m \u001b[43mclassifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_good_start_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    277\u001b[0m dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(initial_sol)\n\u001b[1;32m    278\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m36\u001b[39m\n",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36mClassifier.find_good_start_latent\u001b[0;34m(self, batch_size, num_batches)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[1;32m    147\u001b[0m     q \u001b[38;5;241m=\u001b[39m (G\u001b[38;5;241m.\u001b[39mmapping(torch\u001b[38;5;241m.\u001b[39mrandn([batch_size, G\u001b[38;5;241m.\u001b[39mmapping\u001b[38;5;241m.\u001b[39mz_dim], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m, truncation_psi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m) \u001b[38;5;241m-\u001b[39m G\u001b[38;5;241m.\u001b[39mmapping\u001b[38;5;241m.\u001b[39mw_avg) \u001b[38;5;241m/\u001b[39m w_stds\n\u001b[0;32m--> 149\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw_stds\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_avg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_model\u001b[38;5;241m.\u001b[39membed_image(images\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    151\u001b[0m     loss \u001b[38;5;241m=\u001b[39m prompts_dist_loss(embeds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj_targets, spherical_dist_loss)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/pyribs/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m<string>:507\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, ws, **block_kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/pyribs/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m<string>:437\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, img, ws, force_fp32, fused_modconv, update_emas, **layer_kwargs)\u001b[0m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/pyribs/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m<string>:314\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(self, x, w, noise_mode, fused_modconv, gain)\u001b[0m\n",
      "File \u001b[0;32m~/dev/pyribs/examples/tutorials/torch_utils/misc.py:103\u001b[0m, in \u001b[0;36mprofiled_function.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:77\u001b[0m, in \u001b[0;36mmodulated_conv2d\u001b[0;34m(x, weight, styles, noise, up, down, padding, resample_filter, demodulate, flip_weight, fused_modconv)\u001b[0m\n",
      "File \u001b[0;32m~/dev/pyribs/examples/tutorials/torch_utils/misc.py:103\u001b[0m, in \u001b[0;36mprofiled_function.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/pyribs/examples/tutorials/torch_utils/ops/conv2d_resample.py:126\u001b[0m, in \u001b[0;36mconv2d_resample\u001b[0;34m(x, w, f, up, down, padding, groups, flip_weight, flip_filter)\u001b[0m\n\u001b[1;32m    124\u001b[0m pyt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m-\u001b[39mpy0, \u001b[38;5;241m-\u001b[39mpy1), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    125\u001b[0m x \u001b[38;5;241m=\u001b[39m _conv2d_wrapper(x\u001b[38;5;241m=\u001b[39mx, w\u001b[38;5;241m=\u001b[39mw, stride\u001b[38;5;241m=\u001b[39mup, padding\u001b[38;5;241m=\u001b[39m[pyt,pxt], groups\u001b[38;5;241m=\u001b[39mgroups, transpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, flip_weight\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m flip_weight))\n\u001b[0;32m--> 126\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mupfirdn2d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupfirdn2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mpx0\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mpxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpx1\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mpxt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpy0\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mpyt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpy1\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mpyt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mup\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflip_filter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m down \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    128\u001b[0m     x \u001b[38;5;241m=\u001b[39m upfirdn2d\u001b[38;5;241m.\u001b[39mupfirdn2d(x\u001b[38;5;241m=\u001b[39mx, f\u001b[38;5;241m=\u001b[39mf, down\u001b[38;5;241m=\u001b[39mdown, flip_filter\u001b[38;5;241m=\u001b[39mflip_filter)\n",
      "File \u001b[0;32m~/dev/pyribs/examples/tutorials/torch_utils/ops/upfirdn2d.py:162\u001b[0m, in \u001b[0;36mupfirdn2d\u001b[0;34m(x, f, up, down, padding, flip_filter, gain, impl)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m impl \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _init():\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _upfirdn2d_cuda(up\u001b[38;5;241m=\u001b[39mup, down\u001b[38;5;241m=\u001b[39mdown, padding\u001b[38;5;241m=\u001b[39mpadding, flip_filter\u001b[38;5;241m=\u001b[39mflip_filter, gain\u001b[38;5;241m=\u001b[39mgain)\u001b[38;5;241m.\u001b[39mapply(x, f)\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_upfirdn2d_ref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdown\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdown\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflip_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflip_filter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/pyribs/examples/tutorials/torch_utils/misc.py:103\u001b[0m, in \u001b[0;36mprofiled_function.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m):\n\u001b[0;32m--> 103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/pyribs/examples/tutorials/torch_utils/ops/upfirdn2d.py:204\u001b[0m, in \u001b[0;36m_upfirdn2d_ref\u001b[0;34m(x, f, up, down, padding, flip_filter, gain)\u001b[0m\n\u001b[1;32m    202\u001b[0m f \u001b[38;5;241m=\u001b[39m f[np\u001b[38;5;241m.\u001b[39mnewaxis, np\u001b[38;5;241m.\u001b[39mnewaxis]\u001b[38;5;241m.\u001b[39mrepeat([num_channels, \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m f\u001b[38;5;241m.\u001b[39mndim)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mconv2d_gradfix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_channels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv2d_gradfix\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mx, weight\u001b[38;5;241m=\u001b[39mf\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m), groups\u001b[38;5;241m=\u001b[39mnum_channels)\n",
      "File \u001b[0;32m~/dev/pyribs/examples/tutorials/torch_utils/ops/conv2d_gradfix.py:38\u001b[0m, in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _should_use_custom_op(\u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _conv2d_gradfix(transpose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, weight_shape\u001b[38;5;241m=\u001b[39mweight\u001b[38;5;241m.\u001b[39mshape, stride\u001b[38;5;241m=\u001b[39mstride, padding\u001b[38;5;241m=\u001b[39mpadding, output_padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, dilation\u001b[38;5;241m=\u001b[39mdilation, groups\u001b[38;5;241m=\u001b[39mgroups)\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28minput\u001b[39m, weight, bias)\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# StyleGAN+CLIP LSI experiments.\n",
    "trials=5\n",
    "init_pop=100\n",
    "itrs=10000\n",
    "log_freq=1\n",
    "log_arch_freq=1000\n",
    "image_monitor=False\n",
    "image_monitor_freq=5\n",
    "seed=None\n",
    "\n",
    "for trial_id in range(trials):\n",
    "    print(\"Running trial\", trial_id)\n",
    "    # Create a directory for this specific trial.\n",
    "    s_logdir = os.path.join(outdir, \"cma_mae\", f\"trial_{trial_id}\")\n",
    "    logdir = Path(s_logdir)\n",
    "    if not logdir.is_dir():\n",
    "        logdir.mkdir()\n",
    "\n",
    "    # Create a directory for logging intermediate images if the monitor is on.\n",
    "    if image_monitor:\n",
    "        image_monitor_freq = max(1, image_monitor_freq)\n",
    "        gen_output_dir = os.path.join('generations')\n",
    "        logdir = Path(gen_output_dir)\n",
    "        if not logdir.is_dir():\n",
    "            logdir.mkdir()\n",
    "        gen_output_dir = os.path.join('generations', f\"trial_{trial_id}\")\n",
    "        logdir = Path(gen_output_dir)\n",
    "        if not logdir.is_dir():\n",
    "            logdir.mkdir()\n",
    "\n",
    "    # Create a new summary file\n",
    "    summary_filename = os.path.join(s_logdir, \"summary.csv\")\n",
    "    if os.path.exists(summary_filename):\n",
    "        os.remove(summary_filename)\n",
    "    with open(summary_filename, 'w') as summary_file:\n",
    "        writer = csv.writer(summary_file)\n",
    "        writer.writerow(['Iteration', 'QD-Score', 'Coverage', 'Maximum', 'Average'])\n",
    "    \n",
    "    print(\"Creating Optimizer\")\n",
    "    scheduler, passive_archive = create_optimizer(\"cma_mae\", classifier, seed)\n",
    "    archive = optimizer.archive\n",
    "\n",
    "    best = -1000\n",
    "    non_logging_time = 0.0\n",
    "    print(\"starting training\")\n",
    "    for itr in tqdm(range(1, itrs + 1)):\n",
    "        itr_start = time.time()\n",
    "\n",
    "        sols = scheduler.ask()\n",
    "\n",
    "        values = classifier.compute_all(sols)\n",
    "        values = np.transpose(values)\n",
    "\n",
    "        objs = values[:,0]\n",
    "        measures = values[:,1:3]\n",
    "\n",
    "        objs = transform_obj(np.array(objs, dtype=np.float32))\n",
    "        measures = np.array(measures, dtype=np.float32)\n",
    "\n",
    "        best_gen = max(objs) \n",
    "        best = max(best, best_gen)\n",
    "\n",
    "        scheduler.tell(objs, measures)\n",
    "\n",
    "        # Update the passive elitist archive.\n",
    "        for i in range(len(sols)):\n",
    "            passive_archive.add(sols[i], objs[i], measures[i])\n",
    "\n",
    "        non_logging_time += time.time() - itr_start\n",
    "\n",
    "        print('best', best, best_gen)\n",
    "\n",
    "        if image_monitor and itr % image_monitor_freq == 0:\n",
    "            best_index = np.argmax(objs)\n",
    "            latent_code = sols[best_index]\n",
    "\n",
    "            img = classifier.generate_image(latent_code)\n",
    "            img = tensor_to_pil_img(img)\n",
    "            img.save(os.path.join(gen_output_dir, f'{itr}.png'))\n",
    "\n",
    "        # Save the archive at the given frequency.\n",
    "        # Always save on the final iteration.\n",
    "        final_itr = itr == itrs\n",
    "        if (itr > 0 and itr % log_arch_freq == 0) or final_itr:\n",
    "            # Save a full archive for analysis.\n",
    "            df = passive_archive.as_pandas(include_solutions = final_itr)\n",
    "            df.to_pickle(os.path.join(s_logdir, f\"archive_{itr:08d}.pkl\"))\n",
    "\n",
    "            # Save a heatmap image to observe how the trial is doing.\n",
    "            save_heatmap(passive_archive, os.path.join(s_logdir, f\"heatmap_{itr:08d}.png\"))\n",
    "\n",
    "        # Update the summary statistics for the archive\n",
    "        if (itr > 0 and itr % log_freq == 0) or final_itr:\n",
    "            with open(summary_filename, 'a') as summary_file:\n",
    "                writer = csv.writer(summary_file)\n",
    "\n",
    "                sum_obj = 0\n",
    "                num_filled = 0\n",
    "                num_bins = passive_archive.bins\n",
    "                for sol, obj, beh, idx, meta in zip(*passive_archive.data()):\n",
    "                    num_filled += 1\n",
    "                    sum_obj += obj\n",
    "                qd_score = sum_obj / num_bins\n",
    "                average = sum_obj / num_filled\n",
    "                coverage = 100.0 * num_filled / num_bins\n",
    "                data = [itr, qd_score, coverage, best, average]\n",
    "                writer.writerow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a285b2b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyribs]",
   "language": "python",
   "name": "conda-env-pyribs-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
