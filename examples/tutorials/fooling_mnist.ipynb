{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Images to Fool an MNIST Classifier\n",
    "\n",
    "Despite their high performance on classification tasks such as MNIST, neural networks like the [LeNet-5](https://en.wikipedia.org/wiki/LeNet) have a weakness: they are easy to fool. Namely, given images like the ones below, a classifier may confidently believe that it is seeing certain digits, even though the images look like random noise to humans. Naturally, this phenomenon raises some concerns, especially when the network in question is used in a safety-critical system like a self-driving car. Given such unrecognizable input, one would hope that the network at least has low confidence in its prediction.\n",
    "\n",
    "![fooling images example](_static/fooling_mnist_example.png)\n",
    "\n",
    "To make matters worse for neural networks, generating such images is incredibly easy with QD algorithms. As shown in [Nguyen 2015](http://anhnguyen.me/project/fooling/), one can use simple MAP-Elites to generate these images. In this tutorial, we will instead use the pyribs version of CMA-ME to solve exactly the same task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we install pyribs and PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ribs in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code (0.1.1)\n",
      "Requirement already satisfied: torch in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: torchvision in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.19.4)\n",
      "Requirement already satisfied: numba>=0.45.1 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (0.52.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.1.4)\n",
      "Requirement already satisfied: toml>=0.10.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (0.10.2)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (2.3.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (0.24.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.5.4)\n",
      "Requirement already satisfied: decorator>=4.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (4.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions in /home/btjanaka/.local/lib/python3.7/site-packages (from torch) (3.7.4.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.19.4)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.19.4)\n",
      "Requirement already satisfied: torch in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.19.4)\n",
      "Requirement already satisfied: setuptools in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from numba>=0.45.1->ribs) (52.0.0)\n",
      "Requirement already satisfied: llvmlite<0.36,>=0.35.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from numba>=0.45.1->ribs) (0.35.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from pandas>=1.0.0->ribs) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.19.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from pandas>=1.0.0->ribs) (2020.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas>=1.0.0->ribs) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (2.1.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.19.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from scikit-learn>=0.20->ribs) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs) (1.19.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ribs torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we import PyTorch and some utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we check what device is available for PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary: MNIST Network\n",
    "\n",
    "For our classifier network, we train a LeNet-5 to classify MNIST. If you are not familiar with PyTorch, we recommend referring to the [PyTorch 60-minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html). On the other hand, if you are familiar, feel free to skip to the next section, where we demonstrate how to fool the network.\n",
    "\n",
    "**Note**: This section is adapted from the [Training a Classifier](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) tutorial in the 60-minute blitz.\n",
    "\n",
    "Before the training the network, we load and preprocess the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each image by turning it into a tensor and then\n",
    "# normalizing the values.\n",
    "MEAN_TRANSFORM = 0.1307\n",
    "STD_DEV_TRANSFORM = 0.3081\n",
    "mnist_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((MEAN_TRANSFORM,), (STD_DEV_TRANSFORM,))\n",
    "])\n",
    "\n",
    "TRAIN_BATCH_SIZE = 64\n",
    "TRAINLOADER = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "    './data', train=True, download=True, transform=mnist_transforms),\n",
    "                                          batch_size=TRAIN_BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "TEST_BATCH_SIZE = 1000\n",
    "TESTLOADER = torch.utils.data.DataLoader(torchvision.datasets.MNIST(\n",
    "    './data', train=False, transform=mnist_transforms),\n",
    "                                         batch_size=TEST_BATCH_SIZE,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our training function. We use negative log likelihood loss and Adam optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, epochs):\n",
    "    \"\"\"Trains net for the given number of epochs.\"\"\"\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"=== Epoch {epoch + 1} ===\")\n",
    "        total_loss = 0.0\n",
    "\n",
    "        # Iterate through batches in the shuffled training dataset.\n",
    "        for batch_i, data in enumerate(TRAINLOADER):\n",
    "            inputs = data[0].to(device)\n",
    "            labels = data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if (batch_i + 1) % 100 == 0:\n",
    "                print(f\"Batch {batch_i + 1:5d}: {total_loss}\")\n",
    "                total_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the LeNet-5 and train it for 2 epochs. We have annotated the shapes of the data (excluding the batch dimension) as they pass through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch 1 ===\n",
      "Batch   100: 103.81300377845764\n",
      "Batch   200: 35.582998260855675\n",
      "Batch   300: 24.050891116261482\n",
      "Batch   400: 18.998712450265884\n",
      "Batch   500: 15.099867386743426\n",
      "Batch   600: 13.794951394200325\n",
      "Batch   700: 11.926297828555107\n",
      "Batch   800: 9.48637262918055\n",
      "Batch   900: 10.825607785955071\n",
      "=== Epoch 2 ===\n",
      "Batch   100: 8.530220963992178\n",
      "Batch   200: 7.660911009646952\n",
      "Batch   300: 8.08688993845135\n",
      "Batch   400: 7.816782149486244\n",
      "Batch   500: 7.209920441731811\n",
      "Batch   600: 7.131124372128397\n",
      "Batch   700: 6.255022646393627\n",
      "Batch   800: 7.037217793054879\n",
      "Batch   900: 6.8473530248738825\n"
     ]
    }
   ],
   "source": [
    "LENET5 = nn.Sequential(\n",
    "    nn.Conv2d(1, 6, (5, 5), stride=1, padding=0),  # (1,28,28) -> (6,24,24)\n",
    "    nn.MaxPool2d(2),  # (6,24,24) -> (6,12,12)\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(6, 16, (5, 5), stride=1, padding=0),  # (6,12,12) -> (16,8,8)\n",
    "    nn.MaxPool2d(2),  # (16,8,8) -> (16,4,4)\n",
    "    nn.ReLU(),\n",
    "    nn.Flatten(),  # (16,4,4) -> (256,)\n",
    "    nn.Linear(256, 120),  # (256,) -> (120,)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120, 84),  # (120,) -> (84,)\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(84, 10),  # (84,) -> (10,)\n",
    "    nn.LogSoftmax(dim=1),  # (10,) log probabilities\n",
    ").to(device)\n",
    "\n",
    "fit(LENET5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the network on the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, loader):\n",
    "    \"\"\"Evaluates the network's accuracy on the images in the dataloader.\"\"\"\n",
    "    correct_per_num = [0 for _ in range(10)]\n",
    "    total_per_num = [0 for _ in range(10)]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            outputs = net(images.to(device))\n",
    "            _, predicted = torch.max(outputs.to(\"cpu\"), 1)\n",
    "            c = (predicted == labels).squeeze()\n",
    "            for i in range(len(c)):\n",
    "                label = labels[i]\n",
    "                correct_per_num[label] += c[i].item()\n",
    "                total_per_num[label] += 1\n",
    "\n",
    "    for i in range(10):\n",
    "        print(f\"Class {i}: {correct_per_num[i] / total_per_num[i]:5.3f}\"\n",
    "              f\" ({correct_per_num[i]} / {total_per_num[i]})\")\n",
    "    print(f\"TOTAL  : {sum(correct_per_num) / sum(total_per_num):5.3f}\"\n",
    "          f\" ({sum(correct_per_num)} / {sum(total_per_num)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 0.995 (5894 / 5923)\n",
      "Class 1: 0.997 (6721 / 6742)\n",
      "Class 2: 0.977 (5818 / 5958)\n",
      "Class 3: 0.982 (6018 / 6131)\n",
      "Class 4: 0.990 (5784 / 5842)\n",
      "Class 5: 0.989 (5359 / 5421)\n",
      "Class 6: 0.991 (5863 / 5918)\n",
      "Class 7: 0.995 (6235 / 6265)\n",
      "Class 8: 0.968 (5663 / 5851)\n",
      "Class 9: 0.969 (5766 / 5949)\n",
      "TOTAL  : 0.985 (59121 / 60000)\n"
     ]
    }
   ],
   "source": [
    "evaluate(LENET5, TRAINLOADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 0.993 (973 / 980)\n",
      "Class 1: 0.996 (1131 / 1135)\n",
      "Class 2: 0.979 (1010 / 1032)\n",
      "Class 3: 0.984 (994 / 1010)\n",
      "Class 4: 0.992 (974 / 982)\n",
      "Class 5: 0.988 (881 / 892)\n",
      "Class 6: 0.980 (939 / 958)\n",
      "Class 7: 0.993 (1021 / 1028)\n",
      "Class 8: 0.969 (944 / 974)\n",
      "Class 9: 0.959 (968 / 1009)\n",
      "TOTAL  : 0.984 (9835 / 10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate(LENET5, TESTLOADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fooling the Classifier with CMA-ME\n",
    "\n",
    "Above, we trained a reasonably high-performing classifier. In order to fool the classifier into seeing various digits, we use CMA-ME. As we have 10 distinct digits (0-9), we have a discrete behavior space with 10 values. Note that while pyribs is designed to search continuous spaces, the behavior space can be either continuous or discrete.\n",
    "\n",
    "Our classifier outputs a log probability vector with its belief that it is seeing each digit. Thus, our objective for each digit is to maximize the probability that the classifier assigns to the image associated with it. For instance, for digit 5, we want to generate an image that makes the classifier believe with high probability that it is seeing a 5.\n",
    "\n",
    "In pyribs, we implement CMA-ME with a `GridArchive` and an `ImprovementEmitter`. Below, we start by constructing the `GridArchive`. The archive has 10 bins and a range of (0,10). Since `GridArchive` was originally designed for continuous spaces, it does not directly support discrete spaces, but by using these settings, we have a bin for each digit from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.archives import GridArchive\n",
    "\n",
    "archive = GridArchive([10], [(0, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use 5 improvement emitters, each with batch size of 30. Each emitter begins with an image filled with 0.5 (i.e. grey, since pixels are in the range $[0,1]$) and has an initial step size of 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.emitters import ImprovementEmitter\n",
    "\n",
    "img_size = (28, 28)\n",
    "flat_img_size = 784  # 28 * 28\n",
    "emitters = [\n",
    "    ImprovementEmitter(\n",
    "        archive,\n",
    "        # Start with a grey image.\n",
    "        np.full(flat_img_size, 0.5),\n",
    "        0.1,\n",
    "        # Bound the generated images to the pixel range.\n",
    "        bounds=[(0, 1)] * flat_img_size,\n",
    "        batch_size=30,\n",
    "    ) for _ in range(5)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we construct the optimizer to connect the archive and emitters together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.optimizers import Optimizer\n",
    "\n",
    "optimizer = Optimizer(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the components created, we now generate the images. As we use 5 emitters each with batch size of 30 and run 700 iterations, we evaluate 105,000 images in total. Due to the high dimensionality of the images, sampling from the covariance matrix in CMA-ME takes longer than in lower-dimensional spaces, so this loop may take around an hour to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 784)\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-0a3f5d4596cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mitr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_itrs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Resistance/usc/icaros/projects/pyribs/code/ribs/optimizers/_optimizer.py\u001b[0m in \u001b[0;36mask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mthreadpool_limits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_api\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"blas\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0memitter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_emitters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solutions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solutions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solutions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Resistance/usc/icaros/projects/pyribs/code/ribs/emitters/_improvement_emitter.py\u001b[0m in \u001b[0;36mask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mnew\u001b[0m \u001b[0msolutions\u001b[0m \u001b[0mto\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_bounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_restart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Resistance/usc/icaros/projects/pyribs/code/ribs/emitters/opt/_cma_es.py\u001b[0m in \u001b[0;36mask\u001b[0;34m(self, lower_bounds, upper_bounds)\u001b[0m\n\u001b[1;32m    203\u001b[0m             new_solutions, out_of_bounds = self._transform_and_check_sol(\n\u001b[1;32m    204\u001b[0m                 \u001b[0munscaled_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 upper_bounds)\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0msolutions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mremaining_indices\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_solutions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_itrs = 700\n",
    "start_time = time.time()\n",
    "\n",
    "for itr in range(1, total_itrs + 1):\n",
    "    sols = optimizer.ask()\n",
    "    \n",
    "    print(sols.shape)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Reshape and normalize the image and pass it through the network.\n",
    "        imgs = sols.reshape((-1, 1, *img_size))\n",
    "        imgs = (imgs - MEAN_TRANSFORM) / STD_DEV_TRANSFORM\n",
    "        imgs = torch.tensor(imgs, dtype=torch.float32, device=device)\n",
    "        output = LENET5(imgs)\n",
    "\n",
    "        # The BC is the digit that the network believes it is seeing, i.e. the\n",
    "        # digit with the maximum probability. The objective is the probability\n",
    "        # associated with that digit.\n",
    "        scores, predicted = torch.max(output.to(\"cpu\"), 1)\n",
    "        scores = torch.exp(scores)\n",
    "        objs = scores.numpy()\n",
    "        bcs = predicted.numpy()\n",
    "\n",
    "    optimizer.tell(objs, bcs)\n",
    "    \n",
    "    print(itr)\n",
    "\n",
    "    if itr % 50 == 0:\n",
    "        print(f\"Iteration {itr} complete after {time.time() - start_time} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we display the results we found with CMA-ME. The `index_0` column shows the digit associated with each image, and the `objective` column shows the network's belief that the image is that digit. The `solution` columns show the image's pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.as_pandas().sort_values(\"index_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we display the images found. Interestingly, though the images look mostly like noise, we can occasionally make out traces of the original digit. Note that CMA-ME may not find images for all the digits. This is mostly due to the small behavior space. Usually, QD algorithms run with fairly large behavior spaces. This is something to keep in mind when tuning QD algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
    "fig.tight_layout()\n",
    "ax = ax.flatten()\n",
    "found = set()\n",
    "\n",
    "# Display images.\n",
    "for _, row in archive.as_pandas().iterrows():\n",
    "    i = int(row.loc[\"index_0\"])\n",
    "    found.add(i)\n",
    "    obj = row.loc[\"objective\"]\n",
    "    ax[i].set_title(f\"{i} | Score: {obj:.3f}\", pad=8)\n",
    "    img = row.loc[\"solution_0\":].to_numpy().reshape(28, 28)\n",
    "\n",
    "    # No need to normalize image because we want to see the original.\n",
    "    ax[i].imshow(img, cmap=\"Greys\")\n",
    "    ax[i].set_axis_off()\n",
    "\n",
    "# Mark digits that we did not generate images for.\n",
    "for i in range(10):\n",
    "    if i not in found:\n",
    "        ax[i].set_title(f\"{i} | (no solution)\", pad=8)\n",
    "        ax[i].set_axis_off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we used CMA-ME to generate images that fool a LeNet-5 MNIST classifier. For further exploration, we recommend referring to [Nguyen 2015](http://anhnguyen.me/project/fooling/) and replicating or extending the other experiments described in the paper."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
