{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ne0mog-NFKhE"
   },
   "source": [
    "# Using CMA-ME to Land a Lunar Lander Like a Space Shuttle\n",
    "\n",
    "_This tutorial is part of the series of pyribs tutorials! See [here](https://docs.pyribs.org/en/latest/tutorials.html) for the list of all tutorials and the order in which they should be read._\n",
    "\n",
    "In the [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment, an agent controls a spaceship to touch down gently within a goal zone near the bottom of the screen. Typically, agents in Lunar Lander take a direct approach, hovering straight down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "Powj48a5GIjH",
    "outputId": "5aef86af-d218-4cff-c39d-7c96e5614564"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"360\" height=\"auto\" controls><source src=\"https://raw.githubusercontent.com/icaros-usc/pyribs/master/docs/_static/imgs/lunar-lander-vertical.mp4\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"\"\"<video width=\"360\" height=\"auto\" controls><source src=\"https://raw.githubusercontent.com/icaros-usc/pyribs/master/docs/_static/imgs/lunar-lander-vertical.mp4\" type=\"video/mp4\" /></video>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRSA2NXJGb8u"
   },
   "source": [
    "Of course, this works fine, and the lander safely lands on the landing pad. However, there are many (and more theatric) ways we can safely achieve our goal. For instance, a different solution is to land like a space shuttle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "Q9-J8MePGY9L",
    "outputId": "4a4c2863-ebdf-4d93-a67c-591ab7288c89"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"360\" height=\"auto\" controls><source src=\"https://raw.githubusercontent.com/icaros-usc/pyribs/master/docs/_static/imgs/lunar-lander-left.mp4\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"<video width=\"360\" height=\"auto\" controls><source src=\"https://raw.githubusercontent.com/icaros-usc/pyribs/master/docs/_static/imgs/lunar-lander-left.mp4\" type=\"video/mp4\" /></video>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k1L8dvGuGkSJ"
   },
   "source": [
    "And we can also approach from the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "TFTEeCHNGkrt",
    "outputId": "b899fe71-7ea8-459e-87a5-e12fea10aaa8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video width=\"360\" height=\"auto\" controls><source src=\"https://raw.githubusercontent.com/icaros-usc/pyribs/master/docs/_static/imgs/lunar-lander-right.mp4\" type=\"video/mp4\" /></video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"\"\"<video width=\"360\" height=\"auto\" controls><source src=\"https://raw.githubusercontent.com/icaros-usc/pyribs/master/docs/_static/imgs/lunar-lander-right.mp4\" type=\"video/mp4\" /></video>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3GeMrGjGk41"
   },
   "source": [
    "The primary difference between these trajectories is their \"point of impact,\" that is, the $x$-position of the lander when one of its legs hits the ground for the first time. In the vertical trajectory, the lander first impacts the ground at $x \\approx -0.1$, and when approaching from the left and right, it first impacts at $x \\approx -0.5$ and $x \\approx 0.6$, respectively.\n",
    "\n",
    "Though these trajectories look different, they all achieve good performance (200+), leading to an important insight: there are characteristics of a lunar lander that are not necessarily important for performance, but nonetheless determine the behavior of the lander. In quality diversity (QD) terms, we call these measures. In this tutorial, we will search for policies that yield different trajectories using the pyribs implementation of the QD algorithm [CMA-ME](https://arxiv.org/abs/1912.02400).\n",
    "\n",
    "**_By the way: Recent work introduced [CMA-MAE](https://arxiv.org/abs/2205.10752), an algorithm which builds on CMA-ME and achieves higher performance in a variety of domains. Once you finish this tutorial, be sure to check out the [next tutorial](https://docs.pyribs.org/en/latest/tutorials/cma_mae.html) to learn about CMA-MAE._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pry368FHFKhO"
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install pyribs and Gymnasium. [Gymnasium](https://gymnasium.farama.org) is the successor to [OpenAI Gym](https://www.gymlibrary.dev), which was deprecated in late 2022. We use the visualize extra of pyribs (`ribs[visualize]` instead of just `ribs`) so that we obtain access to the [`ribs.visualize`](https://docs.pyribs.org/en/latest/api/ribs.visualize.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-83S1zt1FKhQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ribs[visualize] in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code (0.4.0)\n",
      "Requirement already satisfied: gymnasium[box2d]==0.27.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (0.27.0)\n",
      "Requirement already satisfied: moviepy>=1.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (1.0.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (1.21.6)\n",
      "Requirement already satisfied: gymnasium-notices>=0.0.1 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (0.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (4.12.0)\n",
      "Requirement already satisfied: jax-jumpy>=0.2.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (0.2.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (1.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (4.3.0)\n",
      "Requirement already satisfied: shimmy<1.0,>=0.1.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (0.2.0)\n",
      "Requirement already satisfied: pygame==2.1.3.dev8 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (2.1.3.dev8)\n",
      "Requirement already satisfied: swig==4.* in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (4.1.1)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from gymnasium[box2d]==0.27.0) (2.3.5)\n",
      "Requirement already satisfied: numpy_groupies>=0.9.16 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (0.9.19)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (0.56.0)\n",
      "Requirement already satisfied: pandas>=1.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (1.3.5)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (2.4.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.4.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (3.1.0)\n",
      "Requirement already satisfied: semantic-version>=2.10 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (2.10.0)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from ribs[visualize]) (3.5.2)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from moviepy>=1.0.0) (0.1.10)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from moviepy>=1.0.0) (4.4.2)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from moviepy>=1.0.0) (0.4.7)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from moviepy>=1.0.0) (2.23.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from moviepy>=1.0.0) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from moviepy>=1.0.0) (2.28.1)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from imageio<3.0,>=2.5->moviepy>=1.0.0) (9.2.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from importlib-metadata>=4.8.0->gymnasium[box2d]==0.27.0) (3.8.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[visualize]) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[visualize]) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[visualize]) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[visualize]) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[visualize]) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from matplotlib>=3.0.0->ribs[visualize]) (4.34.4)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from numba>=0.51.0->ribs[visualize]) (0.39.0)\n",
      "Requirement already satisfied: setuptools in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from numba>=0.51.0->ribs[visualize]) (67.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from pandas>=1.0.0->ribs[visualize]) (2022.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0) (2.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from requests<3.0,>=2.8.1->moviepy>=1.0.0) (1.26.11)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from scikit-learn>=0.20.0->ribs[visualize]) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->ribs[visualize]) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'decorator' from '/home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages/decorator.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install ribs[visualize] gymnasium[box2d]==0.27.0 \"moviepy>=1.0.0\"\n",
    "\n",
    "# An uninstalled version of decorator is occasionally loaded. This loads the\n",
    "# newly installed version of decorator so that moviepy works properly -- see\n",
    "# https://github.com/Zulko/moviepy/issues/1625\n",
    "import importlib\n",
    "import decorator\n",
    "importlib.reload(decorator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oNMaEGunFKhU"
   },
   "source": [
    "Now, we import Gymnasium and several utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WhemB036FKhV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/btjanaka/Resistance/usc/icaros/projects/pyribs/code/env/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import gymnasium as gym\n",
    "import multiprocessing\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jva_esiqFKhW"
   },
   "source": [
    "## Problem Description\n",
    "\n",
    "We treat the Lunar Lander as a quality diversity (QD) problem. For the objective, we use the default rewards provided by the environment, which encourages landing on the landing pad and penalizes engine usage.\n",
    "\n",
    "For the measure functions, we are interested in several factors at the time of \"impact.\" We define impact to be the first time when either of the lunar lander's legs touches the ground. When this happens, we measure the following:\n",
    "\n",
    "- $x$-position: This will lead to markedly different trajectories, as seen earlier.\n",
    "- $y$-velocity: Different velocities will determine how hard the lander impacts the ground.\n",
    "\n",
    "If the lunar lander never impacts the ground, we default the $x$-position to be the last $x$-position of the lander, and the $y$-velocity to be the maximum velocity of the lander (technically minimum since velocities are negative).\n",
    "\n",
    "We will search for policies that produce high-performing trajectories with these measures. For simplicity, we will use a linear policy to control the lunar lander. As the default lunar lander has discrete controls, the equation for this policy is:\n",
    "\n",
    "$$a = argmax(Ws)$$\n",
    "\n",
    "where $a$ is the action to take, $s$ is the state vector, and $W$ is our model, a matrix of weights that stays constant. Essentially, we transform the state to a vector with a \"signal\" for each possible action in the action space, and we choose the action with the highest signal. To search for a different policy, we explore the space of models $W$.\n",
    "\n",
    "In this tutorial, we will search for policies solving a fixed scenario with a flat terrain. To create this scenario, we use a fixed seed of 52 in the environment.\n",
    "\n",
    "**Note: Determinism**\n",
    "\n",
    "> Since our policy and environment are both deterministic, we only have to simulate the policy once to find its performance. Typically, we would run our policy multiple times to gauge average performance and have it generalize, but we ignore that to keep this example simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iEnaEBMgFKhX"
   },
   "outputs": [],
   "source": [
    "# Create an environment so that we can obtain information about it.\n",
    "reference_env = gym.make(\"LunarLander-v2\")\n",
    "action_dim = reference_env.action_space.n\n",
    "obs_dim = reference_env.observation_space.shape[0]\n",
    "\n",
    "seed = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NI2KJcyAFKhY"
   },
   "source": [
    "We can summarize our problem description with the following `simulate` function, which takes in the model and rolls it out in the Lunar Lander environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UlhzwjDlFKhZ"
   },
   "outputs": [],
   "source": [
    "def simulate(model, seed=None):\n",
    "    \"\"\"Simulates the lunar lander model.\n",
    "\n",
    "    Args:\n",
    "        model (np.ndarray): The array of weights for the linear policy. The\n",
    "            weights are passed in as a 1D array and reshaped into a matrix.\n",
    "        seed (int): The seed for the environment.\n",
    "    Returns:\n",
    "        total_reward (float): The reward accrued by the lander throughout its\n",
    "            trajectory.\n",
    "        impact_x_pos (float): The x position of the lander when it touches the\n",
    "            ground for the first time.\n",
    "        impact_y_vel (float): The y velocity of the lander when it touches the\n",
    "            ground for the first time.\n",
    "    \"\"\"\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    action_dim = env.action_space.n\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    model = model.reshape((action_dim, obs_dim))\n",
    "\n",
    "    total_reward = 0.0\n",
    "    impact_x_pos = None\n",
    "    impact_y_vel = None\n",
    "    all_y_vels = []\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(model @ obs)  # Linear policy.\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "\n",
    "        # Refer to the definition of state here:\n",
    "        # https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
    "        x_pos = obs[0]\n",
    "        y_vel = obs[3]\n",
    "        leg0_touch = bool(obs[6])\n",
    "        leg1_touch = bool(obs[7])\n",
    "        all_y_vels.append(y_vel)\n",
    "\n",
    "        # Check if the lunar lander is impacting for the first time.\n",
    "        if impact_x_pos is None and (leg0_touch or leg1_touch):\n",
    "            impact_x_pos = x_pos\n",
    "            impact_y_vel = y_vel\n",
    "\n",
    "    # If the lunar lander did not land, set the x-pos to the one from the final\n",
    "    # timestep, and set the y-vel to the max y-vel (we use min since the lander\n",
    "    # goes down).\n",
    "    if impact_x_pos is None:\n",
    "        impact_x_pos = x_pos\n",
    "        impact_y_vel = min(all_y_vels)\n",
    "\n",
    "    return total_reward, impact_x_pos, impact_y_vel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3PYgVwuFKha"
   },
   "source": [
    "## CMA-ME with pyribs\n",
    "\n",
    "To train our policy, we will use the CMA-ME algorithm (if you are not familiar with CMA-ME, please refer to the corresponding [paper](https://arxiv.org/abs/1912.02400)). This means we need to import and initialize the `GridArchive`, `EvolutionStrategyEmitter`, and `Scheduler` from pyribs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hd8Fa-uoNg_6"
   },
   "source": [
    "### GridArchive\n",
    "\n",
    "First, the [`GridArchive`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html) stores solutions (i.e. models for our policy) in a rectangular grid. Each dimension of the `GridArchive` corresponds to a dimension in measure space that is segmented into equally sized cells. As we have two measure functions for our lunar lander, we have two dimensions in the `GridArchive`. The first dimension is the impact $x$-position, which ranges from -1 to 1, and the second is the impact $y$-velocity, which ranges from -3 (smashing into the ground) to 0 (gently touching down). We divide both dimensions into 50 cells.\n",
    "\n",
    "We additionally specify the dimensionality of solutions which will be stored in the archive. While each model is a 2D matrix, pyribs archives only allow 1D arrays for efficiency; hence, we create an `initial_model` below and retrieve the size of its flattened, 1D form with `initial_model.size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EUipJ5cDFKhc"
   },
   "outputs": [],
   "source": [
    "from ribs.archives import GridArchive\n",
    "initial_model = np.zeros((action_dim, obs_dim))\n",
    "\n",
    "archive = GridArchive(\n",
    "    solution_dim=initial_model.size,  # Dimensionality of solutions in the archive.\n",
    "    dims=[50, 50],  # 50 cells along each dimension.\n",
    "    ranges=[(-1.0, 1.0), (-3.0, 0.0)],  # (-1, 1) for x-pos and (-3, 0) for y-vel.\n",
    "    qd_score_offset=-600,  # See the note below.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: QD Score Offset**\n",
    "\n",
    "> Above, we specified `qd_score_offset=-600` when initializing the archive. The QD score ([Pugh 2016](https://doi.org/10.3389/frobt.2016.00040)) is a metric for QD algorithms which sums the objective values of all elites in the archive. However, if objectives can be negative, this metric will penalize an algorithm for discovering new cells with a negative objective. To prevent this, it is common to normalize each objective to be non-negative by subtracting an offset, typically the minimum objective, before computing the QD score. While lunar lander does not have a predefined minimum objective, we know from previous experiments that almost all solutions score above -600, so we have set the offset accordingly. Thus, if a solution has, for example, an objective of -300, then its objective will be normalized to -300 - (-600) = 300."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdCmEwtOFKhd"
   },
   "source": [
    "### EvolutionStrategyEmitter\n",
    "\n",
    "Next, the [`EvolutionStrategyEmitter`](https://docs.pyribs.org/en/latest/api/ribs.emitters.EvolutionStrategyEmitter.html) with two-stage improvement ranking (\"2imp\") uses CMA-ES to search for policies that add new entries to the archive or improve existing ones. Since we do not have any prior knowledge of what the model will be, we set the initial model to be the zero vector, and we set the initial step size for CMA-ES to be 1.0, so that initial solutions are sampled from a standard isotropic Gaussian. Furthermore, we use 5 emitters so that the algorithm simultaneously searches several areas of the measure space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IuyDI0HaFKhe"
   },
   "outputs": [],
   "source": [
    "from ribs.emitters import EvolutionStrategyEmitter\n",
    "\n",
    "emitters = [\n",
    "    EvolutionStrategyEmitter(\n",
    "        archive=archive,\n",
    "        x0=initial_model.flatten(),\n",
    "        sigma0=1.0,  # Initial step size.\n",
    "        ranker=\"2imp\",\n",
    "        batch_size=30,  # If we do not specify a batch size, the emitter will\n",
    "                        # automatically use a batch size equal to the default\n",
    "                        # population size of CMA-ES.\n",
    "    ) for _ in range(5)  # Create 5 separate emitters.\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwS_5rnPRCbr"
   },
   "source": [
    "**Note: Two-Stage Improvement Ranking**\n",
    "\n",
    "> The term \"two-stage\" refers to how this ranking mechanism ranks solutions by a tuple of `(status, value)` defined as follows:\n",
    ">\n",
    "> 1. `status`: Whether the solution creates a new cell in the archive, improves an existing cell, or is not inserted.\n",
    "> 2. `value`: Consider the objective $f$ of the solution and the objective $f'$ of the solution currently in the cell where the solution will be inserted. When the solution creates a new cell, $f'$ is undefined because the cell was previously empty, so the value is defined as $f$. Otherwise, when the solution improves an existing cell or is not inserted at all, the value is $f - f'$.\n",
    ">\n",
    "> During ranking, this two-stage improvement ranker will first sort by `status`, prioritizing new solutions, followed by solutions which improve existing cells, followed by solutions which are not inserted. Within each group, solutions are further ranked by their corresponding `value`. See the archive [`add`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html#ribs.archives.GridArchive.add) method for more information on statuses and values.\n",
    "> \n",
    "> Additional rankers are available in the [ribs.emitters.rankers](https://docs.pyribs.org/en/latest/api/ribs.emitters.rankers.html) module. These rankers include those corresponding to different emitters described in the CMA-ME paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYJ5zor9FKhg"
   },
   "source": [
    "### Scheduler\n",
    "\n",
    "Finally, the [`Scheduler`](https://docs.pyribs.org/en/latest/api/ribs.schedulers.Scheduler.html) controls how the emitters interact with the archive. On every iteration, the scheduler calls the emitters to generate solutions. After the user evaluates these generated solutions, the scheduler inserts the solutions into the archive and passes the feedback to the emitters (this feedback consists of the `status` and `value` for each solution that we described in the note above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ytg5qUP2FKhj"
   },
   "outputs": [],
   "source": [
    "from ribs.schedulers import Scheduler\n",
    "\n",
    "scheduler = Scheduler(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5NOJy6YFKhk"
   },
   "source": [
    "## QD Search\n",
    "\n",
    "With the pyribs components defined, we start searching with CMA-ME. Since we use 5 emitters each with a batch size of 30 and we run 250 iterations, we run 5 x 30 x 250 = 37,500 lunar lander simulations. We also keep track of some logging info via `archive.stats`, which is an [`ArchiveStats`](https://docs.pyribs.org/en/latest/api/ribs.archives.ArchiveStats.html) object.\n",
    "\n",
    "To speed up this loop, we parallelize the evaluation of multiple solutions with Python's [multiprocessing module](https://docs.python.org/3/library/multiprocessing.html), specifically the [`starmap`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool.starmap) method of [`multiprocessing.Pool`](https://docs.python.org/3/library/multiprocessing.html#multiprocessing.pool.Pool). With two workers, the loop should take **2 hours** to run. With two workers, it should take **1 hour** to run. Feel free to increase the number of workers based on what your system has available to speed up the loop further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pd2abvstFKhm",
    "outputId": "1dfc615d-331f-4657-e301-c6eda2b70d59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 25 itrs completed after 258.67s                                        \n",
      "  - Size: 1080                                                           \n",
      "  - Coverage: 0.432                                                      \n",
      "  - QD Score: 474862.76574713085                                         \n",
      "  - Max Obj: 304.1126503737456                                           \n",
      "  - Mean Obj: -160.31225393784183                                        \n",
      "Iterations:  12%|██▌                  | 30/250 [06:02<1:13:27, 20.03s/it]"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "total_itrs = 250\n",
    "workers = 2  # Adjust the number of workers based on what you have available.\n",
    "\n",
    "for itr in trange(1, total_itrs + 1, file=sys.stdout, desc='Iterations'):\n",
    "    # Request models from the scheduler.\n",
    "    sols = scheduler.ask()\n",
    "\n",
    "    # Evaluate the models and record the objectives and measuress.\n",
    "    with multiprocessing.Pool(workers) as pool:\n",
    "        results = pool.starmap(simulate, [(model, seed) for model in sols])\n",
    "\n",
    "    objs, meas = [], []\n",
    "    for obj, impact_x_pos, impact_y_vel in results:\n",
    "        objs.append(obj)\n",
    "        meas.append([impact_x_pos, impact_y_vel])\n",
    "\n",
    "    # Send the results back to the scheduler.\n",
    "    scheduler.tell(objs, meas)\n",
    "\n",
    "    # Logging.\n",
    "    if itr % 25 == 0:\n",
    "        tqdm.write(f\"> {itr} itrs completed after {time.time() - start_time:.2f}s\")\n",
    "        tqdm.write(f\"  - Size: {archive.stats.num_elites}\")    # Number of elites in the archive. len(archive) also provides this info.\n",
    "        tqdm.write(f\"  - Coverage: {archive.stats.coverage}\")  # Proportion of archive cells which have an elite.\n",
    "        tqdm.write(f\"  - QD Score: {archive.stats.qd_score}\")  # QD score, i.e. sum of objective values of all elites in the archive.\n",
    "                                                               # Accounts for qd_score_offset as described in the GridArchive section.\n",
    "        tqdm.write(f\"  - Max Obj: {archive.stats.obj_max}\")    # Maximum objective value in the archive.\n",
    "        tqdm.write(f\"  - Mean Obj: {archive.stats.obj_mean}\")  # Mean objective value of elites in the archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZIzzxxWFKhp"
   },
   "source": [
    "## Visualizing the Archive\n",
    "\n",
    "Using [`grid_archive_heatmap`](https://docs.pyribs.org/en/latest/api/ribs.visualize.grid_archive_heatmap.html) from the [`ribs.visualize`](https://docs.pyribs.org/en/latest/api/ribs.visualize.html) module, we can view a heatmap of the archive. The heatmap shows the measures for which CMA-ME found a solution. The color of each cell shows the objective value of the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "ufCTDxiAFKhp",
    "outputId": "5b3b2a69-6ffc-4540-8a4b-a64dc0455587"
   },
   "outputs": [],
   "source": [
    "from ribs.visualize import grid_archive_heatmap\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "grid_archive_heatmap(archive, vmin=-300, vmax=300)\n",
    "plt.gca().invert_yaxis()  # Makes more sense if larger velocities are on top.\n",
    "plt.ylabel(\"Impact y-velocity\")\n",
    "plt.xlabel(\"Impact x-position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sHKT6nJFKhq"
   },
   "source": [
    "From this heatmap, we can make a few observations:\n",
    "\n",
    "- CMA-ME found solutions for all cells in the archive (empty cells would show up as white).\n",
    "- Most of the high-performing solutions have lower impact $y$-velocities (see the bright area at the bottom of the map). This is reasonable, as a lander that crashes into the ground would not do well.\n",
    "- The high-performing solutions are spread across a wide range of impact $x$-positions. The highest solutions seem to be at $x \\approx 0$ (the bright spot in the middle). This makes sense since an impact $x$-position of 0 corresponds to the direct vertical approach. Nevertheless, there are many high-performing solutions that had other $x$-positions, and we will visualize them in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG6GDOpyFKhr"
   },
   "source": [
    "## Visualizing Individual Trajectories\n",
    "\n",
    "To view the trajectories for different models, we can use the [RecordVideo](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordVideo) wrapper, and we can use IPython to display the video in the notebook. The `display_video` function below shows how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vT-xb4myFKhr"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def display_video(model):\n",
    "    \"\"\"Displays a video of the model in the environment.\"\"\"\n",
    "\n",
    "    video_env = gym.wrappers.RecordVideo(\n",
    "        gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\"),\n",
    "        video_folder=\"videos\",\n",
    "        # This will ensure all episodes are recorded as videos.\n",
    "        episode_trigger=lambda idx: True,\n",
    "        # Disables moviepy's logger to reduce clutter in the output.\n",
    "        disable_logger=True,\n",
    "    )\n",
    "    simulate(video_env, model, seed)\n",
    "    video_env.close()  # Save video.\n",
    "\n",
    "    # Display the video with HTML. Though we use glob, there is only 1 video.\n",
    "    for video_file in glob.glob(\"videos/*.mp4\"):\n",
    "        video = io.open(video_file, 'rb').read()\n",
    "        encoded = base64.b64encode(video).decode(\"ascii\")\n",
    "        display(\n",
    "            HTML(f'''\n",
    "            <video width=\"360\" height=\"auto\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\" />\n",
    "            </video>'''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t2QPnuqgFKhr"
   },
   "source": [
    "\n",
    "We can retrieve policies with measures that are close to a query with the [`retrieve_single`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html#ribs.archives.GridArchive.retrieve_single) method. This method will look up the cell corresponding to the queried measures. Then, the method will check if there is an elite in that cell, and return the elite if it exists (the method does not check neighboring cells for elites). The returned elite may not have the exact measures requested because the elite only has to be in the same cell as the queried measures.\n",
    "\n",
    "Below, we first retrieve a policy that impacted the ground on the left (approximately -0.4) with low velocity (approximately -0.10) by querying for `[-0.4, 0.10]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "FxTO1P3tFKhs",
    "outputId": "b5a556a4-77fa-45b5-ac38-7ae634c7e6f6"
   },
   "outputs": [],
   "source": [
    "elite = archive.retrieve_single([-0.4, -0.10])\n",
    "if elite.solution is not None:\n",
    "    print(f\"Objective: {elite.objective}\")\n",
    "    print(f\"Measures: (x-pos: {elite.measures[0]}, y-vel: {elite.measures[1]})\")\n",
    "    display_video(elite.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA3KJCMLFKhs"
   },
   "source": [
    "We can also find a policy that impacted the ground on the right (0.8) with low velocity.\n",
    "\n",
    "**Note: Batch and Single Methods**\n",
    "\n",
    "> `retrieve_single` returns an [Elite](https://docs.pyribs.org/en/latest/api/ribs.archives.Elite.html) object given a single `measures` array. Meanwhile, the [`retrieve`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html#ribs.archives.GridArchive.retrieve) method takes in a _batch_ of measures (named `measures_batch`) and returns an [EliteBatch](https://docs.pyribs.org/en/latest/api/ribs.archives.EliteBatch.html) object. Several archive methods in pyribs follow a similar pattern of having a batch and single version, e.g., [`add`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html#ribs.archives.GridArchive.add) and [`add_single`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html#ribs.archives.GridArchive.add_single)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "0lgiOQ2sFKht",
    "outputId": "567a4f20-28cc-4385-b280-fbcd3dd2a769"
   },
   "outputs": [],
   "source": [
    "elite = archive.retrieve_single([0.8, -0.10])\n",
    "if elite.solution is not None:\n",
    "    print(f\"Objective: {elite.objective}\")\n",
    "    print(f\"Measures: (x-pos: {elite.measures[0]}, y-vel: {elite.measures[1]})\")\n",
    "    display_video(elite.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OuP_5VVDFKht"
   },
   "source": [
    "And we can find a policy that executes a regular vertical landing, which happens when the impact $x$-position is around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "v4nAwfmXFKht",
    "outputId": "cb99d56d-401c-44e1-9d57-8bc35c04160d"
   },
   "outputs": [],
   "source": [
    "elite = archive.retrieve_single([0.0, -0.10])\n",
    "if elite.solution is not None:\n",
    "    print(f\"Objective: {elite.objective}\")\n",
    "    print(f\"Measures: (x-pos: {elite.measures[0]}, y-vel: {elite.measures[1]})\")\n",
    "    display_video(elite.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZLDoutPFKhu"
   },
   "source": [
    "As the archive has ~2500 solutions, we cannot view them all, but we can filter for high-performing solutions. We first retrieve the archive's elites with the [`as_pandas`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html#ribs.archives.GridArchive.as_pandas) method. Then, we choose solutions that scored above 200 because 200 is the [threshold for the problem to be considered solved](https://gymnasium.farama.org/environments/box2d/lunar_lander/). Note that many high-performing solutions do not land on the landing pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0rHgh4fgFKhu"
   },
   "outputs": [],
   "source": [
    "df = archive.as_pandas()\n",
    "high_perf_sols = df.query(\"objective > 200\").sort_values(\"objective\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_RYE1rTFKhu"
   },
   "source": [
    "Below we visualize several of these high-performing solutions. The `iterelites` method is available because `as_pandas` returns an [`ArchiveDataFrame`](https://docs.pyribs.org/en/latest/api/ribs.archives.ArchiveDataFrame.html), a subclass of the Pandas DataFrame specialized for pyribs. `iterelites` iterates over the entries in the DataFrame and returns them as [`Elite`](https://docs.pyribs.org/en/latest/api/ribs.archives.Elite.html) objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 749
    },
    "id": "brTepWvkFKhv",
    "outputId": "d4f907fa-8767-4300-ff6d-c83ea7cc7326"
   },
   "outputs": [],
   "source": [
    "if len(high_perf_sols) > 0:\n",
    "    for elite in high_perf_sols.iloc[[0, len(high_perf_sols) // 2, -1]].iterelites():\n",
    "        print(f\"Objective: {elite.objective}\")\n",
    "        print(f\"Measures: (x-pos: {elite.measures[0]}, y-vel: {elite.measures[1]})\")\n",
    "        display_video(elite.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3cQJ2ctOBG5"
   },
   "source": [
    "And finally, the [`best_elite`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html#ribs.archives.GridArchive.best_elite) property is the [`Elite`](https://docs.pyribs.org/en/latest/api/ribs.archives.Elite.html) which has the highest performance in the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "4OTjcg1XN_xz",
    "outputId": "d75b31be-64a0-4f9c-adaa-c62d7e4257fa"
   },
   "outputs": [],
   "source": [
    "print(f\"Objective: {archive.best_elite.objective}\")\n",
    "print(f\"Measures: (x-pos: {archive.best_elite.measures[0]}, y-vel: {archive.best_elite.measures[1]})\")\n",
    "display_video(archive.best_elite.solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooGfhHCuFKhv"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "As the saying goes, \"there is more than one way to land an airplane\" 😉. However, it is often difficult to shape the reward function in reinforcement learning to discover these unique and \"creative\" solutions. In such cases, a QD algorithm can help search for solutions that vary in \"interestingness.\"\n",
    "\n",
    "In this tutorial, we showed that this is the case for the lunar lander environment. Using CMA-ME, we searched for lunar lander trajectories with differing impact measures. Though these trajectories all take a different approach, many perform well.\n",
    "\n",
    "For extending this tutorial, we suggest the following:\n",
    "\n",
    "- Replace the impact measures with your own measures. What other properties might be interesting for the lunar lander?\n",
    "- Try different terrains by changing the seed. For instance, if the environment has valleys, can the lander learn to go into this valley and glide back up?\n",
    "- Use other gym environments. What measures could you use in an environment like `BipedalWalker-v2`?\n",
    "\n",
    "Finally, to learn about an algorithm which performs even better on QD problems, check out the [CMA-MAE tutorial](https://docs.pyribs.org/en/latest/tutorials/cma_mae.html).\n",
    "\n",
    "And for a version of this tutorial that uses [Dask](https://dask.org) to parallelize evaluations and thus speed things up, refer to the [Lunar Lander example](https://docs.pyribs.org/en/latest/examples/lunar_lander.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H8B12jOFKhw"
   },
   "source": [
    "## Citation\n",
    "\n",
    "If you find this tutorial useful, please cite it as:\n",
    "\n",
    "```text\n",
    "@article{pyribs_lunar_lander,\n",
    "  title   = {Using CMA-ME to Land a Lunar Lander Like a Space Shuttle},\n",
    "  author  = {Bryon Tjanaka and Sam Sommerer and Nikitas Klapsis and Matthew C. Fontaine and Stefanos Nikolaidis},\n",
    "  journal = {pyribs.org},\n",
    "  year    = {2021},\n",
    "  url     = {https://docs.pyribs.org/en/stable/tutorials/lunar_lander.html}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kAdvjZiFKhw"
   },
   "source": [
    "## Credits\n",
    "\n",
    "This tutorial is based on a [poster](https://web.archive.org/web/20220817214422/https://1l7puj10vwe3zflo2jsktkit-wpengine.netdna-ssl.com/wp-content/uploads/2020/08/S20-Klapsis-Poster.pdf) created by Nikitas Klapsis as part of USC's 2020 SHINE program."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
