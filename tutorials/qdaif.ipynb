{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback\n",
    "\n",
    "_This tutorial is part of the series of pyribs tutorials! See [here](https://docs.pyribs.org/en/latest/tutorials.html) for the list of all tutorials and the order in which they should be read._\n",
    "\n",
    "Given a general creative writing task like \"write a story about a spy and a politician,\" there are many possible outcomes. For instance, we could write a story that ends with the spy getting away with classified information. Alternatively, we could write a story where the spy and politician put aside their differences and team up to overthrow a government, or even one where the spy and politician fall in love. In short, a wide range of stories exist, each with their own interesting plots and details.\n",
    "\n",
    "<figure style=\"width: 50%; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "![](_static/spy-and-politician.png)\n",
    "\n",
    "<figcaption style=\"text-align: center; font-style: italic\">\"An image of a suspicious spy and a rich politician.\" Generated with ChatGPT.</figcaption>\n",
    "</figure>\n",
    "\n",
    "To explore the range of such possibilities available in creative writing, [Quality Diversity through AI Feedback (QDAIF; Bradley 2024)](https://qdaif.github.io/) proposes to orchestrate LLMs in two ways. First, QDAIF uses LLMs to _generate_ new stories. Given a story, QDAIF prompts the LLM to mutate the story into a new one. Second, and equally as important, QDAIF leverages LLMs to _evaluate_ each story, providing the quality and diversity metrics (i.e., objective and measure values). Thus, QDAIF can repeatedly generate and evaluate stories, eventually producing an archive of diverse stories.\n",
    "\n",
    "In this tutorial, we will demonstrate how to implement a variation of QDAIF in pyribs on the task of writing a story about a spy and a politician. We will describe how to evaluate the objective and measures for each story, set up the QD algorithm components, run the algorithm, and visualize the results.\n",
    "\n",
    "_Since this tutorial involves running LLMs, we recommend running it on a machine with a GPU, either on Colab or on a local workstation. It should also be possible to run on a standard laptop, although it will be much slower. Alternatively, if you would like to use an API such as OpenAI or Google Gemini, it is also possible to hook up that LLM (more details below)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let us set up the prerequisites for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "\n",
    "In addition to pyribs, this tutorial depends on [LangChain](https://python.langchain.com/docs/introduction/), a framework for developing LLM applications. Below we install these dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE4F13T5vc-t",
    "outputId": "fa943f14-bd85-488f-de82-45f32599afc9"
   },
   "outputs": [],
   "source": [
    "%pip install ribs[visualize] langchain tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating an LLM with LangChain and Ollama\n",
    "\n",
    "To make this tutorial flexible to the choice of LLM, we use [LangChain](https://python.langchain.com/docs/introduction/). Among other things, LangChain provides a common interface for operating with LLMs from providers like OpenAI and Google. In this tutorial, we will use LangChain's integration with [Ollama](https://ollama.com). Ollama is a framework that enables efficiently running LLMs on local machines. In other words, _we will use LangChain to call an LLM hosted locally by Ollama_.\n",
    "\n",
    "If you are running this tutorial on your own machine, please follow the [installation instructions](https://ollama.com/download) for Ollama and skip this cell. If you are running on Google Colab, we can install Ollama by following the instructions shown below, which were adapted from this [notebook](https://colab.research.google.com/github/5aharsh/collama/blob/main/Ollama_Setup.ipynb) by Saharsh Anand.\n",
    "\n",
    "**Note:** If you would like to use an LLM from an API like OpenAI or Google Gemini, LangChain also provides integrations for many APIs; more details (such as how to use `init_chat_model`) are available [here](https://python.langchain.com/docs/tutorials/llm_chain/). In that case, feel free to skip this section and instantiate a `model` variable on your own. Note that we assume the model is a _chat model_, i.e., an instance of [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RoROPhPuoXA",
    "outputId": "5b9bb1c5-28ca-4b83-856a-8ce7ad0e0f35"
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72slzM1SwGWH"
   },
   "source": [
    "After installing Ollama, we start the Ollama server in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViZYk__Juyv_"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def run_ollama_serve():\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)  # Wait for the server to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflVXn-ZwaBa"
   },
   "source": [
    "We can now pull the LLM model from Ollama's library and instantiate it in LangChain. We have chosen [Llama 3.1](https://ollama.com/library/llama3.1:8b-instruct-q4_K_M), specifically the 8B parameter model that has been finetuned for instruction following. We choose the `q4_K_M` [quantization](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md) as it is a recommended size that balances between speed/memory usage and accuracy. For alternative models, visit the library [here](https://ollama.com/library). Example alternatives include `llama3.1:70b-instruct-q4_K_M` (70B version of Llama 3.1) and `gpt-oss:20b` (gpt-oss-20b from OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBMVl2zzu9Ji",
    "outputId": "168fb3e6-2986-4ce6-b2cc-051ef5c3fb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Model: model='llama3.1:8b-instruct-q4_K_M'\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_name = \"llama3.1:8b-instruct-q4_K_M\"  # @param {\"type\":\"string\"}\n",
    "\n",
    "# Pull the model from the Ollama library.\n",
    "!ollama pull {model_name}\n",
    "\n",
    "# Instantiate the model in LangChain.\n",
    "model = ChatOllama(model=model_name)\n",
    "print(\"Model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Feedback with an Evaluator\n",
    "\n",
    "The first ingredient for QDAIF is an evaluator that calls the LLM to provide feedback on the quality and diversity of each story by evaluating the objective and measures. Before creating the evaluator, let us first define the objective and measures for this creative writing problem. This problem is adapted from the Stories domain in the original QDAIF paper (Bradley 2023), where the measures were slightly different.\n",
    "\n",
    "- **Objective:** Is the story about a suspicious spy and a rich politician?\n",
    "- **Measure 0:** Is the story a romance story?\n",
    "- **Measure 1:** Does the story have a happy ending?\n",
    "\n",
    "There are many ways to obtain the LLM's score for the objective and for each measure. For example, the original QDAIF paper predominantly extracts scores by asking the LLM to output an answer like yes or no and analyzing the logits of the tokens associated with that answer. Notably, this approach requires access to the logits output by the LLM, which are not always available (but it can be really helpful if they are). Thus, in this tutorial, we instead ask the LLM to output a rating on a scale of 1 to 10 when evaluating the objective and each measure. This approach is general in that it works with any LLM, but a key drawback is that the rating is stochastic and requires multiple evaluations.\n",
    "\n",
    "Below we show the exact prompts for the objective and for each measure. The prompt starts with defining the characteristic of the story and ends with asking the LLM to output a rating from 1 to 10. Feel free to customize the prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_prompt = \"Determine if the input text contains a high-quality short story containing two characters, a suspicious spy, and a rich politician. For example, a high-quality short story would have good flow, interesting plot, and not repeat similar sentences or undesired items such as titles and URLs. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "measure_0_prompt = \"Determine if the input text is a romance story. For example, a romance story talks about two characters who fall in love with each other. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "measure_1_prompt = \"Determine if the input text is a story with a happy ending. For example, a story where the two characters make peach with each other has a happy ending. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the prompts, we can now define an `Evaluator` class that calls the LLMs to evaluate the objective and measures. Notably, this class takes an `n_evals` param that determines how many times to evaluate the objective and each measure. As mentioned above, since the LLM is stochastic, its ratings will vary on subsequent calls. As such, during each evaluation, we call the LLM `n_evals` times and average the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xkwuRVopvIfm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Manages an LLM to compute the objective and measures.\n",
    "\n",
    "    Args:\n",
    "        model: Chat model for computing evaluations.\n",
    "        objective_prompt: Prompt for the objective.\n",
    "        measure_0_prompt: Prompt for the first measure (measure 0).\n",
    "        measure_1_prompt: Prompt for the second measure (measure 1).\n",
    "        n_evals: Number of times to evaluate the objective and each measure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        model: BaseChatModel,\n",
    "        objective_prompt: str,\n",
    "        measure_0_prompt: str,\n",
    "        measure_1_prompt: str,\n",
    "        n_evals: int,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.n_evals = n_evals\n",
    "        self.min_score = 1\n",
    "        self.max_score = 10\n",
    "\n",
    "        # To receive the output from the LLM in a consistent format, we use structured\n",
    "        # output (https://python.langchain.com/docs/how_to/structured_output/). This\n",
    "        # Pydantic model defines the schema for receiving ratings from the LLM. Note\n",
    "        # that the text in the schema class (including class name, field name, field\n",
    "        # description, docstrings) all have some influence on the LLM output.\n",
    "        class Rating(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        # Objective. We first define a chat template, where the `objective_prompt`\n",
    "        # passed in is the system prompt, and the `text` of the story is the user's\n",
    "        # message. Then, we form a chain that connects this template to the model. We\n",
    "        # do the same for measure 0 and measure 1 below. For more background on\n",
    "        # LangChain, refer to the documentation, such as:\n",
    "        # - https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "        # - https://python.langchain.com/docs/concepts/lcel/\n",
    "        self.objective_template = ChatPromptTemplate(\n",
    "            [(\"system\", objective_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.objective_chain = (\n",
    "            self.objective_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 0.\n",
    "        self.measure_0_template = ChatPromptTemplate(\n",
    "            [(\"system\", measure_0_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.measure_0_chain = (\n",
    "            self.measure_0_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 1.\n",
    "        self.measure_1_template = ChatPromptTemplate(\n",
    "            [(\"system\", measure_1_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.measure_1_chain = (\n",
    "            self.measure_1_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "    def _compute_score(self, chain: Runnable, texts: list[str]):\n",
    "        \"\"\"Uses the given chain to compute scores for the given batch of input texts.\n",
    "\n",
    "        Each text input is evaluated `n_evals` times.\n",
    "\n",
    "        Two values are returned:\n",
    "        - The first value is `all_scores`, which is a list where each entry contains the\n",
    "          `n_evals` scores for each text.\n",
    "        - The second is `mean_scores`, which is the mean score for each piece of text.\n",
    "        \"\"\"\n",
    "        inputs = [{\"text\": text} for text in texts for _ in range(self.n_evals)]\n",
    "        outputs = chain.batch(inputs)\n",
    "\n",
    "        all_scores = []\n",
    "        mean_scores = []\n",
    "\n",
    "        for i in range(0, len(outputs), self.n_evals):\n",
    "            results = outputs[i : i + self.n_evals]\n",
    "            scores = []\n",
    "            for r in results:\n",
    "                # Note: this assumes the schema for each result has a `rating` field,\n",
    "                # which may not be the case if you modify the schema above.\n",
    "                score = np.clip(r.rating, self.min_score, self.max_score)\n",
    "                scores.append(score)\n",
    "\n",
    "            scores = np.asarray(scores)\n",
    "            all_scores.append(scores)\n",
    "            mean_scores.append(scores.mean())\n",
    "\n",
    "        return all_scores, np.asarray(mean_scores)\n",
    "\n",
    "    def evaluate(self, texts: list[str]):\n",
    "        objectives = self._compute_score(self.objective_chain, texts)[1]\n",
    "        measure_0 = self._compute_score(self.measure_0_chain, texts)[1]\n",
    "        measure_1 = self._compute_score(self.measure_1_chain, texts)[1]\n",
    "        measures = np.stack((measure_0, measure_1), axis=1)\n",
    "        return objectives, measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the evaluator, here is an example of calling it on two example stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MurzWysRvkTz",
    "outputId": "7fed94c4-da89-490b-eae4-ef3ef36df793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 0 | Objective: 7.0, Measure 0: 2.0, Measure 1: 2.2\n",
      "Story 1 | Objective: 7.0, Measure 0: 7.6, Measure 1: 8.4\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(\n",
    "    model=model,\n",
    "    objective_prompt=objective_prompt,\n",
    "    measure_0_prompt=measure_0_prompt,\n",
    "    measure_1_prompt=measure_1_prompt,\n",
    "    n_evals=5,\n",
    ")\n",
    "\n",
    "objectives, measures = evaluator.evaluate(\n",
    "    [\n",
    "        \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "        \"Jack was a politician in the city when one day he met Sarah. Sarah had been working for the government as a secret spy. Jack decided he really liked Sarah, and they fell in love. They both quite their jobs and decided to live in the countryside together.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, (obj, meas) in enumerate(zip(objectives, measures)):\n",
    "    print(f\"Story {i} | Objective: {obj}, Measure 0: {meas[0]}, Measure 1: {meas[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDAIF Components in pyribs\n",
    "\n",
    "Like other QD algorithms in pyribs, QDAIF is composed of an archive, emitters, and a scheduler. Below we define each component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridArchive for Storing Stories\n",
    "\n",
    "The archive for QDAIF is a [`GridArchive`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html), which divides the measure space into a grid and stores a story in each grid cell. Below, we specify the dimensions (`dims`) of the grid to be $20 \\times 20$, and the `ranges` to be 1 to 10 for each dimension.\n",
    "\n",
    "For those familiar with the `GridArchive` from previous tutorials, the settings for this `GridArchive` are slightly different since it must store text-based solutions, whereas previous tutorials involved solutions that were continuous vectors. The differences are as follows. First, we set `solution_dim` to be `()`, indicating a scalar value. Second, we set `dtype` such that the `solution` is an `object` (while the `objective` and `measures` remain as floating-point values). This way, the archive can store pieces of text, which are single objects of type `str` (i.e., \"scalar\" objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.archives import GridArchive\n",
    "\n",
    "archive = GridArchive(\n",
    "    solution_dim=(),\n",
    "    dims=[20, 20],\n",
    "    ranges=[(1, 10), (1, 10)],\n",
    "    dtype={\"solution\": object, \"objective\": np.float32, \"measures\": np.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Emitter for Generating Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.archives import ArchiveBase\n",
    "from ribs.emitters import EmitterBase\n",
    "\n",
    "\n",
    "class LLMDirectionalEmitter(EmitterBase):\n",
    "    \"\"\"Uses LLMs to modify pieces of text in random archive directions.\n",
    "\n",
    "    Args:\n",
    "        archive: Archive of solutions, e.g., :class:`ribs.archives.GridArchive`. The\n",
    "            archive must contain solutions of type :class:`str`.\n",
    "        model: LLM for mutating pieces of text.\n",
    "        batch_size: Number of solutions to return in :meth:`ask`.\n",
    "        initial_solutions: Initial pieces of text for the LLM.\n",
    "        seed: Value to seed the random number generator. Set to None to avoid a fixed\n",
    "            seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        archive: ArchiveBase,\n",
    "        *,\n",
    "        model: BaseChatModel,\n",
    "        batch_size: int,\n",
    "        initial_solutions: list[str],\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        EmitterBase.__init__(\n",
    "            self,\n",
    "            archive,\n",
    "            solution_dim=archive.solution_dim,\n",
    "            bounds=None,\n",
    "        )\n",
    "\n",
    "        self._model = model\n",
    "        self._batch_size = batch_size\n",
    "        self._initial_solutions = initial_solutions\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        self._mutation_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"The following is a story about two characters, a suspicious spy, and a rich politician. Modify the story in the following ways: {measure_0_direction}, and {measure_1_direction}. Output only the new story.\",\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self._mutation_dirs = {\n",
    "            # Each measure has two possible directions: one decreases the measure while\n",
    "            # the other increases the measure.\n",
    "            \"measure_0\": [\n",
    "                \"make the story sound less like a romance story\",\n",
    "                \"make the story sound more like a romance story\",\n",
    "            ],\n",
    "            \"measure_1\": [\n",
    "                \"make the ending of the story less happy\",\n",
    "                \"make the ending of the story more happy\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        class Story(BaseModel):\n",
    "            story: str = Field(description=\"The modified story.\")\n",
    "\n",
    "        self._mutation_chain = (\n",
    "            self._mutation_template | self._model.with_structured_output(Story)\n",
    "        )\n",
    "\n",
    "    def ask(self):\n",
    "        if self.archive.empty:\n",
    "            return self._initial_solutions\n",
    "\n",
    "        prompts = []\n",
    "        for _ in range(self._batch_size):\n",
    "            # For both measure_0 and measure_1 (hence size=2), choose between the two\n",
    "            # possible directions.\n",
    "            dirs = self._rng.choice(2, size=2)\n",
    "\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"text\": self._archive.sample_elites(1)[\"solution\"][0],\n",
    "                    \"measure_0_direction\": self._mutation_dirs[\"measure_0\"][dirs[0]],\n",
    "                    \"measure_1_direction\": self._mutation_dirs[\"measure_1\"][dirs[1]],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        stories = self._mutation_chain.batch(prompts)\n",
    "        return [s.story for s in stories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitters = [\n",
    "    LLMDirectionalEmitter(\n",
    "        archive,\n",
    "        model=model,\n",
    "        batch_size=1,\n",
    "        # From QDAIF paper (Appendix A.21).\n",
    "        initial_solutions=[\n",
    "            \"A spy named Joanne wants to infiltrate the premises of Karl Johnson, a highly-influential figure in the city. Karl was a wealthy mayor, and would do anything in his power to suppress any opposing voices. Joanne wanted to figure out what Karl was hiding, but she took a turn for the worse, as she was highly suspicious in her presence outside his home.\",\n",
    "            \"The wealthy entrepreneur and member of parliament, Susan, hosted a party at her mansion. She invited all of the residents, as well as an unusual looking man. The man, Dave, was wearing a tacky shirt, and star-shaped glasses, and was actually a spy. He made the whole room laugh with his jokes, and had a secret agenda - to find what Susan does in her private fun room!\",\n",
    "            \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "        ],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.schedulers import Scheduler\n",
    "\n",
    "scheduler = Scheduler(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running QDAIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm, trange\n\u001b[1;32m      4\u001b[0m total_itrs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m1\u001b[39m, total_itrs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, file\u001b[38;5;241m=\u001b[39m\u001b[43msys\u001b[49m\u001b[38;5;241m.\u001b[39mstdout, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      7\u001b[0m     solutions \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mask()\n\u001b[1;32m      8\u001b[0m     objectives, measures \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(solutions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sys' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "total_itrs = 200\n",
    "\n",
    "for itr in trange(1, total_itrs + 1, file=sys.stdout, desc=\"Iterations\"):\n",
    "    solutions = scheduler.ask()\n",
    "    objectives, measures = evaluator.evaluate(solutions)\n",
    "    scheduler.tell(objectives, measures)\n",
    "\n",
    "    if itr % 5 == 0 or itr == total_itrs:\n",
    "        tqdm.write(\n",
    "            f\"Iteration {itr:5d} | \"\n",
    "            f\"Archive Coverage: {archive.stats.coverage * 100:6.3f}%  \"\n",
    "            f\"QD Score: {archive.stats.qd_score:6.3f}\"\n",
    "        )\n",
    "\n",
    "        plot_archive(archive, itr)\n",
    "\n",
    "        # TODO: Comment on this.\n",
    "        archive.data(return_type=\"pandas\").to_csv(\"qdaif_archive.csv\")\n",
    "        with open(\"qdaif_scheduler.pkl\", \"wb\") as file:\n",
    "            pkl.dump(scheduler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: clarify loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qdaif_scheduler.pkl\", \"rb\") as file:\n",
    "    scheduler = pkl.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you find this tutorial useful, please cite it as:\n",
    "\n",
    "```\n",
    "@article{pyribs_qdaif,\n",
    "  title   = {Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback},\n",
    "  author  = {Bryon Tjanaka},\n",
    "  journal = {pyribs.org},\n",
    "  year    = {2025},\n",
    "  url     = {https://docs.pyribs.org/en/stable/tutorials/qdaif.html}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "Thank you to [Sid Srikanth](https://sidsrikanth.com/), [Saeed Hedayatian](https://conflictednerd.github.io/), and the members of the ICAROS Lab for their invaluable feedback in developing this tutorial."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
