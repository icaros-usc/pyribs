{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback\n",
    "\n",
    "_This tutorial is part of the series of pyribs tutorials! See [here](https://docs.pyribs.org/en/latest/tutorials.html) for the list of all tutorials and the order in which they should be read._\n",
    "\n",
    "Given a creative writing task like \"write a story about a spy and a politician,\" there are many possible interpretations. For instance, we could write a story that ends with the spy getting away with classified information. Or one where the spy and politician put aside their differences and team up to overthrow a government. Or even one where the spy and politician fall in love and leave their lives behind. In short, a wide range of stories exist, each with their own unique plots and details.\n",
    "\n",
    "<figure style=\"width: 50%; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "![](_static/spy-and-politician.png)\n",
    "\n",
    "<figcaption style=\"text-align: center; font-style: italic\">\"An image of a suspicious spy and a rich politician.\" Generated with ChatGPT.</figcaption>\n",
    "</figure>\n",
    "\n",
    "To explore the range of such possibilities, [Quality Diversity through AI Feedback (QDAIF; Bradley 2024)](https://qdaif.github.io/) proposes to orchestrate LLMs in two ways. First, QDAIF uses LLMs to _generate_ new stories. Given a story, QDAIF prompts the LLM to mutate the story into a new one. Second, and equally as important, QDAIF leverages LLMs to _evaluate_ each story, providing the quality and diversity metrics (i.e., objective and measure values). Thus, QDAIF can repeatedly generate and evaluate stories, eventually producing an archive of diverse stories.\n",
    "\n",
    "In this tutorial, we will demonstrate how to implement a variation of QDAIF in pyribs on the task of writing a story about a spy and a politician. We will describe how to evaluate the objective and measures for each story, set up the QD algorithm components, run the algorithm, and visualize the results.\n",
    "\n",
    "_Since this tutorial involves running LLMs, we recommend running it on a machine with a GPU, either on Colab or on a local workstation. It should also be possible to run on a standard laptop, although it will be much slower. Alternatively, if you would like to use an API such as OpenAI or Google Gemini, it may also be possible to set that up (more details below)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the prerequisites for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "\n",
    "In addition to pyribs, this tutorial depends on [LangChain](https://python.langchain.com/docs/introduction/), a framework for developing LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE4F13T5vc-t",
    "outputId": "fa943f14-bd85-488f-de82-45f32599afc9"
   },
   "outputs": [],
   "source": [
    "%pip install ribs[visualize] langchain tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating an LLM with LangChain and Ollama\n",
    "\n",
    "To make this tutorial flexible to the choice of LLM, we use [LangChain](https://python.langchain.com/docs/introduction/). Among other things, LangChain provides a common interface for operating with LLMs from providers like OpenAI and Google. In this tutorial, we will use LangChain's integration with [Ollama](https://ollama.com). Ollama is a framework that enables efficiently running LLMs on local machines. In other words, _we will use LangChain to call an LLM hosted locally by Ollama_.\n",
    "\n",
    "If you are running this tutorial on your own machine, please follow the [installation instructions](https://ollama.com/download) for Ollama and skip only the cell below. If you are running on Google Colab, we can install Ollama by following the instructions shown below, which were adapted from this [notebook](https://colab.research.google.com/github/5aharsh/collama/blob/main/Ollama_Setup.ipynb) by Saharsh Anand.\n",
    "\n",
    "**Note:** If you would like to use an LLM from an API like OpenAI or Google Gemini, LangChain also provides integrations for many APIs; more details (such as how to use `init_chat_model`) are available [here](https://python.langchain.com/docs/tutorials/llm_chain/). In that case, feel free to skip this section and instantiate a `model` variable on your own. Note that we assume the model is a _chat model_, i.e., an instance of [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RoROPhPuoXA",
    "outputId": "5b9bb1c5-28ca-4b83-856a-8ce7ad0e0f35"
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72slzM1SwGWH"
   },
   "source": [
    "After installing Ollama, we start the Ollama server in the background. If Ollama is already running, this cell will output an error that the address is already in use. _This is perfectly fine; simply proceed to the next steps._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViZYk__Juyv_"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def run_ollama_serve():\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)  # Wait for the server to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflVXn-ZwaBa"
   },
   "source": [
    "We can now pull the LLM model from Ollama's library and instantiate it in LangChain. We have chosen [Llama 3.1](https://ollama.com/library/llama3.1:8b-instruct-q4_K_M), specifically the 8B parameter model that has been finetuned for instruction following. We choose the `q4_K_M` [quantization](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md) as it is a recommended size that balances between speed/memory usage and accuracy. For alternative models, visit the Ollama library [here](https://ollama.com/library). Example alternatives include `llama3.1:70b-instruct-q4_K_M` (70B version of Llama 3.1) and `gpt-oss:20b` (gpt-oss-20b from OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBMVl2zzu9Ji",
    "outputId": "168fb3e6-2986-4ce6-b2cc-051ef5c3fb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Model: model='llama3.1:8b-instruct-q4_K_M'\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_name = \"llama3.1:8b-instruct-q4_K_M\"  # @param {\"type\":\"string\"}\n",
    "\n",
    "# Pull the model from the Ollama library.\n",
    "!ollama pull {model_name}\n",
    "\n",
    "# Instantiate the model in LangChain.\n",
    "model = ChatOllama(model=model_name)\n",
    "print(\"Model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Feedback with an Evaluator\n",
    "\n",
    "The first ingredient for QDAIF is an evaluator that calls the LLM to provide feedback on the quality and diversity of each story by evaluating the objective and measures. Before creating the evaluator, let us first define the objective and measures for this creative writing domain:\n",
    "\n",
    "- **Objective:** Is the story a high-quality short story about a suspicious spy and a rich politician?\n",
    "- **Measure 0:** Is the story a romance story?\n",
    "- **Measure 1:** Does the story have a happy ending?\n",
    "\n",
    "This domain is adapted from the Stories domain in the original QDAIF paper (Bradley 2023), where the measures were slightly different. Later on, we will present the exact LLM prompts for each of these characteristics.\n",
    "\n",
    "There are many ways to obtain the LLM's score for the objective and for each measure. For example, the original QDAIF paper predominantly extracts scores by asking the LLM to output an answer like yes or no and analyzing the logits of the tokens associated with that answer. Notably, this approach requires access to the logits output by the LLM, and logits are not always available. Thus, in this tutorial, we instead ask the LLM to output a rating on a scale of 1 to 10 when evaluating the objective and each measure. This approach is general in that it works with any LLM, but a key drawback is that the rating is stochastic and requires multiple evaluations.\n",
    "\n",
    "Having defined the objective and measures, we now define the `Evaluator` class that calls the LLM to evaluate the objective and measures. Notably, this class takes an `n_evals` parameter that determines how many times to evaluate the objective and each measure. The final score for the objective and each measure is averaged over the ratings from the `n_evals` evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xkwuRVopvIfm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Manages an LLM to compute the objective and measures.\n",
    "\n",
    "    Args:\n",
    "        model: Chat model for computing evaluations.\n",
    "        objective_prompt: Prompt for the objective.\n",
    "        measure_0_prompt: Prompt for the first measure (measure 0).\n",
    "        measure_1_prompt: Prompt for the second measure (measure 1).\n",
    "        n_evals: Number of times to evaluate the objective and each measure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        model: BaseChatModel,\n",
    "        objective_prompt: str,\n",
    "        measure_0_prompt: str,\n",
    "        measure_1_prompt: str,\n",
    "        n_evals: int,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.n_evals = n_evals\n",
    "        self.min_score = 1\n",
    "        self.max_score = 10\n",
    "\n",
    "        # To receive the output from the LLM in a consistent format, we use structured\n",
    "        # output (https://python.langchain.com/docs/how_to/structured_output/). This\n",
    "        # Pydantic model defines the schema for receiving ratings from the LLM. Note\n",
    "        # that the text in the schema class (including class name, field name, field\n",
    "        # description, docstrings) all have some influence on the LLM output.\n",
    "        class Rating(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        # Objective. We first define a chat template, where the `objective_prompt`\n",
    "        # passed in is the system prompt, and the `text` of the story is the user's\n",
    "        # message. Then, we form a chain that connects this template to the model. We\n",
    "        # do the same for measure 0 and measure 1 below. For more background on\n",
    "        # LangChain, refer to the documentation, such as:\n",
    "        # - https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "        # - https://python.langchain.com/docs/concepts/lcel/\n",
    "        self.objective_template = ChatPromptTemplate(\n",
    "            [(\"system\", objective_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.objective_chain = (\n",
    "            self.objective_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 0.\n",
    "        self.measure_0_template = ChatPromptTemplate(\n",
    "            [(\"system\", measure_0_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.measure_0_chain = (\n",
    "            self.measure_0_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 1.\n",
    "        self.measure_1_template = ChatPromptTemplate(\n",
    "            [(\"system\", measure_1_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.measure_1_chain = (\n",
    "            self.measure_1_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "    def compute_score(self, chain: Runnable, texts: list[str]):\n",
    "        \"\"\"Uses the given chain to compute scores for the given batch of input texts.\n",
    "\n",
    "        Each text input is evaluated `n_evals` times.\n",
    "\n",
    "        Two values are returned:\n",
    "        - The first value is `all_scores`, which is a list where each entry contains the\n",
    "          `n_evals` scores for each text.\n",
    "        - The second is `mean_scores`, which is the mean score for each piece of text.\n",
    "        \"\"\"\n",
    "        inputs = [{\"text\": text} for text in texts for _ in range(self.n_evals)]\n",
    "        outputs = chain.batch(inputs)\n",
    "\n",
    "        all_scores = []\n",
    "        mean_scores = []\n",
    "\n",
    "        for i in range(0, len(outputs), self.n_evals):\n",
    "            results = outputs[i : i + self.n_evals]\n",
    "            scores = []\n",
    "            for r in results:\n",
    "                # Note: this assumes the schema for each result has a `rating` field,\n",
    "                # which may not be the case if you modify the schema above.\n",
    "                score = np.clip(r.rating, self.min_score, self.max_score)\n",
    "                scores.append(score)\n",
    "\n",
    "            scores = np.asarray(scores)\n",
    "            all_scores.append(scores)\n",
    "            mean_scores.append(scores.mean())\n",
    "\n",
    "        return all_scores, np.asarray(mean_scores)\n",
    "\n",
    "    def evaluate(self, texts: list[str]):\n",
    "        objectives = self.compute_score(self.objective_chain, texts)[1]\n",
    "        measure_0 = self.compute_score(self.measure_0_chain, texts)[1]\n",
    "        measure_1 = self.compute_score(self.measure_1_chain, texts)[1]\n",
    "        measures = np.stack((measure_0, measure_1), axis=1)\n",
    "        return objectives, measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the `Evaluator`, we now instantiate it with the `model`, the prompts for the objective and measures, and the number of evaluations `n_evals`. Each prompt starts with defining a characteristic of the story and ends with asking the LLM to output a rating from 1 to 10. Feel free to customize the prompts to specify different objectives and measures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MurzWysRvkTz",
    "outputId": "7fed94c4-da89-490b-eae4-ef3ef36df793"
   },
   "outputs": [],
   "source": [
    "objective_prompt = \"Determine if the input text contains a high-quality short story containing two characters, a suspicious spy, and a rich politician. For example, a high-quality short story would have good flow, interesting plot, and not repeat similar sentences or undesired items such as titles and URLs. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "measure_0_prompt = \"Determine if the input text is a romance story. For example, a romance story talks about two characters who fall in love with each other. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "measure_1_prompt = \"Determine if the input text is a story with a happy ending. For example, a story where the two characters make peach with each other has a happy ending. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "n_evals = 5  # @param {\"type\":\"integer\"}\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    model=model,\n",
    "    objective_prompt=objective_prompt,\n",
    "    measure_0_prompt=measure_0_prompt,\n",
    "    measure_1_prompt=measure_1_prompt,\n",
    "    n_evals=n_evals,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's use the `Evaluator` to evaluate two stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MurzWysRvkTz",
    "outputId": "7fed94c4-da89-490b-eae4-ef3ef36df793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 0 | Objective: 7.2, Measure 0: 1.6, Measure 1: 2.0\n",
      "Story 1 | Objective: 6.8, Measure 0: 7.6, Measure 1: 9.2\n"
     ]
    }
   ],
   "source": [
    "objectives, measures = evaluator.evaluate(\n",
    "    [\n",
    "        \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "        \"Jack was a politician in the city when one day he met Sarah. Sarah had been working for the government as a secret spy. Jack decided he really liked Sarah, and they fell in love. They both quite their jobs and decided to live in the countryside together.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, (obj, meas) in enumerate(zip(objectives, measures)):\n",
    "    print(f\"Story {i} | Objective: {obj}, Measure 0: {meas[0]}, Measure 1: {meas[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDAIF Components in pyribs\n",
    "\n",
    "Like other QD algorithms in pyribs, QDAIF is composed of an archive, emitters, and a scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridArchive for Storing Stories\n",
    "\n",
    "The archive for QDAIF is a [`GridArchive`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html), which divides the measure space into a grid and stores a story in each grid cell. Below, we specify the dimensions (`dims`) of the grid to be $20 \\times 20$, and the `ranges` to be 1 to 10 for each dimension.\n",
    "\n",
    "For those familiar with the `GridArchive` from previous tutorials, the settings for this `GridArchive` are slightly different since it must store text-based solutions, whereas previous tutorials involved solutions that were continuous vectors. The differences are as follows. First, we set `solution_dim` to be `()`, indicating a scalar value. Second, we set `dtype` such that the `solution` is an `object` (while the `objective` and `measures` remain as floating-point values). This way, the archive can store pieces of text, which are single objects of type `str` (i.e., \"scalar\" objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.archives import GridArchive\n",
    "\n",
    "archive = GridArchive(\n",
    "    solution_dim=(),\n",
    "    dims=[20, 20],\n",
    "    ranges=[(1, 10), (1, 10)],\n",
    "    dtype={\"solution\": object, \"objective\": np.float32, \"measures\": np.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Emitter for Generating Stories\n",
    "\n",
    "TODO: write\n",
    "TODO: custom prompts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.archives import ArchiveBase\n",
    "from ribs.emitters import EmitterBase\n",
    "\n",
    "\n",
    "class LLMDirectionalEmitter(EmitterBase):\n",
    "    \"\"\"Uses LLMs to modify pieces of text in random archive directions.\n",
    "\n",
    "    Args:\n",
    "        archive: Archive of solutions, e.g., :class:`ribs.archives.GridArchive`. The\n",
    "            archive must contain solutions of type :class:`str`.\n",
    "        model: LLM for mutating pieces of text.\n",
    "        batch_size: Number of solutions to return in :meth:`ask`.\n",
    "        initial_solutions: Initial pieces of text for the LLM.\n",
    "        seed: Value to seed the random number generator. Set to None to avoid a fixed\n",
    "            seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        archive: ArchiveBase,\n",
    "        *,\n",
    "        model: BaseChatModel,\n",
    "        batch_size: int,\n",
    "        initial_solutions: list[str],\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        EmitterBase.__init__(\n",
    "            self,\n",
    "            archive,\n",
    "            solution_dim=archive.solution_dim,\n",
    "            bounds=None,\n",
    "        )\n",
    "\n",
    "        self._model = model\n",
    "        self._batch_size = batch_size\n",
    "        self._initial_solutions = initial_solutions\n",
    "        self._rng = np.random.default_rng(seed)\n",
    "\n",
    "        self._mutation_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"The following is a story about two characters, a suspicious spy, and a rich politician. Modify the story in the following ways: {measure_0_direction}, and {measure_1_direction}. Output only the new story.\",\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self._mutation_dirs = {\n",
    "            # Each measure has two possible directions: one decreases the measure while\n",
    "            # the other increases the measure.\n",
    "            \"measure_0\": [\n",
    "                \"make the story sound less like a romance story\",\n",
    "                \"make the story sound more like a romance story\",\n",
    "            ],\n",
    "            \"measure_1\": [\n",
    "                \"make the ending of the story less happy\",\n",
    "                \"make the ending of the story more happy\",\n",
    "            ],\n",
    "        }\n",
    "\n",
    "        class Story(BaseModel):\n",
    "            story: str = Field(description=\"The modified story.\")\n",
    "\n",
    "        self._mutation_chain = (\n",
    "            self._mutation_template | self._model.with_structured_output(Story)\n",
    "        )\n",
    "\n",
    "    def ask(self):\n",
    "        if self.archive.empty:\n",
    "            return self._initial_solutions\n",
    "\n",
    "        prompts = []\n",
    "        for _ in range(self._batch_size):\n",
    "            # For both measure_0 and measure_1 (hence size=2), choose between the two\n",
    "            # possible directions.\n",
    "            dirs = self._rng.choice(2, size=2)\n",
    "\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"text\": self._archive.sample_elites(1)[\"solution\"][0],\n",
    "                    \"measure_0_direction\": self._mutation_dirs[\"measure_0\"][dirs[0]],\n",
    "                    \"measure_1_direction\": self._mutation_dirs[\"measure_1\"][dirs[1]],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        stories = self._mutation_chain.batch(prompts)\n",
    "        return [s.story for s in stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: instantiation; only uses one emitter with 1 batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emitters = [\n",
    "    LLMDirectionalEmitter(\n",
    "        archive,\n",
    "        model=model,\n",
    "        batch_size=1,\n",
    "        # From QDAIF paper (Appendix A.21).\n",
    "        initial_solutions=[\n",
    "            \"A spy named Joanne wants to infiltrate the premises of Karl Johnson, a highly-influential figure in the city. Karl was a wealthy mayor, and would do anything in his power to suppress any opposing voices. Joanne wanted to figure out what Karl was hiding, but she took a turn for the worse, as she was highly suspicious in her presence outside his home.\",\n",
    "            \"The wealthy entrepreneur and member of parliament, Susan, hosted a party at her mansion. She invited all of the residents, as well as an unusual looking man. The man, Dave, was wearing a tacky shirt, and star-shaped glasses, and was actually a spy. He made the whole room laugh with his jokes, and had a secret agenda - to find what Susan does in her private fun room!\",\n",
    "            \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "        ],\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler for Composing QDAIF\n",
    "\n",
    "Finally, to compose the archive and emitters for QDAIF together, we define a [`Scheduler`](https://docs.pyribs.org/en/latest/api/ribs.schedulers.Scheduler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.schedulers import Scheduler\n",
    "\n",
    "scheduler = Scheduler(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running QDAIF\n",
    "\n",
    "With the pyribs components defined, we can\n",
    "\n",
    "TODO: plotting?\n",
    "TODO: Debug; check stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:   0%|                                  | 0/200 [00:00<?, ?it/s]solutions ['A spy named Joanne wants to infiltrate the premises of Karl Johnson, a highly-influential figure in the city. Karl was a wealthy mayor, and would do anything in his power to suppress any opposing voices. Joanne wanted to figure out what Karl was hiding, but she took a turn for the worse, as she was highly suspicious in her presence outside his home.'\n",
      " 'The wealthy entrepreneur and member of parliament, Susan, hosted a party at her mansion. She invited all of the residents, as well as an unusual looking man. The man, Dave, was wearing a tacky shirt, and star-shaped glasses, and was actually a spy. He made the whole room laugh with his jokes, and had a secret agenda - to find what Susan does in her private fun room!'\n",
      " 'The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.']\n",
      "objectives [5.6 6.8 7. ]\n",
      "measures [[1.6 2. ]\n",
      " [2.4 2.6]\n",
      " [1.6 2. ]]\n",
      "Iterations:   0%|                        | 1/200 [00:20<1:07:51, 20.46s/it]solutions [\"The rich politician, Tom, was a man under siege - he feared all of his close aides after sensing danger in his inner circle. The country was embroiled in a brutal civil war, and he knew that with every passing day, the threat to his life grew. Determined to stay one step ahead of his enemies, Tom had increased security measures around him, but still, a sense of unease lingered. His trusted aide, Johnny, seemed like an exception - honest and loyal. Or so he thought. Meanwhile, a former soldier turned secret agent, Alex, who was part of the security detail, couldn't shake off the feeling that something wasn't right about Johnny. With his training and instincts screaming at him to investigate further, Alex decided to sneak into the classified files room, where Johnny was alone. He observed from a safe distance, trying not to be seen. What he saw shook him - Johnny's files were encrypted, but a small note on his desk hinted at a secret bank account in an offshore location. This could be a game-changer. Alex knew that with this information, he could expose Johnny for the traitor he seemed to be.\"]\n",
      "objectives [8.2]\n",
      "measures [[2.  3.2]]\n",
      "Iterations:   1%|▎                         | 2/200 [00:34<54:51, 16.63s/it]solutions [\"The rich politician, Tom's life had become a delicate balancing act - he walked on eggshells around his close aides, suspecting betrayal at every turn. The ongoing civil war outside only added to his paranoia as he navigated the treacherous waters of politics. That's when Johnny, one of his most trusted advisors, suddenly caught his attention. Tom had been noticing some discrepancies in Johnny's financial reports and couldn't shake off the feeling that something was amiss. Little did he know, a mole within the security team, codename 'Nova', had been secretly watching him. Nova had infiltrated the highest echelons of power, feeding classified intel to the rebel forces fighting against Tom's regime. But as he surveilled Johnny, Nova began to realize that this politician was not what he seemed. Despite his questionable methods, Tom had a vision for the country - one that aligned with Nova's own values. A spark of curiosity ignited within Nova as he continued to observe Johnny, now more intrigued than ever about the man's true intentions.\"]\n",
      "objectives [8.2]\n",
      "measures [[2.4 3.8]]\n",
      "Iterations:   2%|▍                         | 3/200 [00:48<51:00, 15.54s/it]solutions ['The Whispers in the Shadows']\n",
      "objectives [2.4]\n",
      "measures [[5.8 3. ]]\n",
      "Iterations:   2%|▌                         | 4/200 [01:05<53:09, 16.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m itr \u001b[38;5;129;01min\u001b[39;00m trange(\u001b[38;5;241m1\u001b[39m, total_itrs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39mstdout, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIterations\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      9\u001b[0m     solutions \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mask()\n\u001b[0;32m---> 10\u001b[0m     objectives, measures \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msolutions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mtell(objectives, measures)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolutions\u001b[39m\u001b[38;5;124m\"\u001b[39m, solutions)\n",
      "Cell \u001b[0;32mIn[3], line 103\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]):\n\u001b[0;32m--> 103\u001b[0m     objectives \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobjective_chain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    104\u001b[0m     measure_0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_score(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeasure_0_chain, texts)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    105\u001b[0m     measure_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_score(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeasure_1_chain, texts)[\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 82\u001b[0m, in \u001b[0;36mEvaluator.compute_score\u001b[0;34m(self, chain, texts)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Uses the given chain to compute scores for the given batch of input texts.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03mEach text input is evaluated `n_evals` times.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;124;03m- The second is `mean_scores`, which is the mean score for each piece of text.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     81\u001b[0m inputs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: text} \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_evals)]\n\u001b[0;32m---> 82\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m all_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     85\u001b[0m mean_scores \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/pyribs/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:3195\u001b[0m, in \u001b[0;36mRunnableSequence.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   3193\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3194\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 3195\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m                \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m   3198\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m# each step a child run of the corresponding root run\u001b[39;49;00m\n\u001b[1;32m   3199\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3200\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;250;43m \u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3201\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3202\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3203\u001b[0m \u001b[43m                \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3204\u001b[0m \u001b[43m                \u001b[49m\u001b[43mreturn_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3205\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3206\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;66;03m# finish the root runs\u001b[39;00m\n\u001b[1;32m   3209\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/pyribs/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:5469\u001b[0m, in \u001b[0;36mRunnableBindingBase.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m   5467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5468\u001b[0m     configs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))]\n\u001b[0;32m-> 5469\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5470\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5471\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5472\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_exceptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/pyribs/env/lib/python3.10/site-packages/langchain_core/runnables/base.py:797\u001b[0m, in \u001b[0;36mRunnable.batch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[Output]\u001b[39m\u001b[38;5;124m\"\u001b[39m, [invoke(inputs[\u001b[38;5;241m0\u001b[39m], configs[\u001b[38;5;241m0\u001b[39m])])\n\u001b[1;32m    796\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(configs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[0;32m--> 797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlist[Output]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/pyribs/env/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m~/pyribs/env/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m~/pyribs/env/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/pyribs/env/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "total_itrs = 200\n",
    "\n",
    "for itr in trange(1, total_itrs + 1, file=sys.stdout, desc=\"Iterations\"):\n",
    "    solutions = scheduler.ask()\n",
    "    objectives, measures = evaluator.evaluate(solutions)\n",
    "    scheduler.tell(objectives, measures)\n",
    "\n",
    "    if itr % 5 == 0 or itr == total_itrs:\n",
    "        tqdm.write(\n",
    "            f\"Iteration {itr:5d} | \"\n",
    "            f\"Archive Coverage: {archive.stats.coverage * 100:6.3f}%  \"\n",
    "            f\"QD Score: {archive.stats.qd_score:6.3f}\"\n",
    "        )\n",
    "\n",
    "        # Save the archive's data as a CSV, and save the archive itself as a pickle.\n",
    "        # The CSV makes it easy to read the text, while the pickle is easier to reload.\n",
    "        archive.data(return_type=\"pandas\").to_csv(\"qdaif_archive.csv\")\n",
    "        with open(\"qdaif_archive.pkl\", \"wb\") as file:\n",
    "            pkl.dump(archive, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: clarify loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"qdaif_scheduler.pkl\", \"rb\") as file:\n",
    "    scheduler = pkl.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you find this tutorial useful, please cite it as:\n",
    "\n",
    "```\n",
    "@article{pyribs_qdaif,\n",
    "  title   = {Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback},\n",
    "  author  = {Bryon Tjanaka},\n",
    "  journal = {pyribs.org},\n",
    "  year    = {2025},\n",
    "  url     = {https://docs.pyribs.org/en/stable/tutorials/qdaif.html}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "Thank you to [Sid Srikanth](https://sidsrikanth.com/), [Saeed Hedayatian](https://conflictednerd.github.io/), and the members of the ICAROS Lab for their invaluable feedback in developing this tutorial."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
