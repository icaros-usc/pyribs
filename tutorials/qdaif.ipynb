{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback\n",
    "\n",
    "_This tutorial is part of the series of pyribs tutorials! See [here](https://docs.pyribs.org/en/latest/tutorials.html) for the list of all tutorials and the order in which they should be read._\n",
    "\n",
    "Given a general creative writing task like \"write a story about a spy and a politician,\" there are many possible outcomes. For instance, we could write a story that ends with the spy getting away with classified information. Alternatively, we could write a story where the spy and politician put aside their differences and team up to overthrow a government, or even one where the spy and politician fall in love. In short, a wide range of stories exist, each with their own interesting plots and details.\n",
    "\n",
    "<figure style=\"width: 50%; margin-left: auto; margin-right: auto;\">\n",
    "    <img alt=\"An image of a suspicious spy and a rich politician. Generated with ChatGPT.\" src=\"_static/spy-and-politician.png\" />\n",
    "    <figcaption style=\"text-align: center; font-style: italic\">\"An image of a suspicious spy and a rich politician.\" Generated with ChatGPT.</figcaption>\n",
    "</figure>\n",
    "\n",
    "To explore the range of such possibilities available in creative writing, [Quality Diversity through AI Feedback (QDAIF; Bradley 2024)](https://qdaif.github.io/) proposes to orchestrate LLMs in two ways. First, QDAIF uses LLMs to _generate_ new stories. Given a story, QDAIF prompts the LLM to mutate the story into a new one. Second, and equally as important, QDAIF leverages LLMs to _evaluate_ each story, providing the quality and diversity metrics (i.e., objective and measure values). Thus, QDAIF can repeatedly generate and evaluate stories, eventually producing an archive of diverse creative texts.\n",
    "\n",
    "In this tutorial, we will demonstrate how to implement a variation of QDAIF in pyribs on the task of writing a story about a spy and a politician. We will describe how to evaluate the objective and measures for each story, set up the QD algorithm components, run the algorithm, and visualize the results.\n",
    "\n",
    "_Since this tutorial involves running LLMs, we recommend running it on a machine with a GPU, either on Colab or on a local workstation. It should also be possible to run on a standard laptop, although it will be much slower. Alternatively, if you would like to use an API such as OpenAI or Google Gemini, it is also possible to hook up that LLM (more details below)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let us set up the prerequisites for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "\n",
    "In addition to pyribs, this tutorial depends on [LangChain](https://python.langchain.com/docs/introduction/), a framework for developing LLM applications. Below we install these dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE4F13T5vc-t",
    "outputId": "fa943f14-bd85-488f-de82-45f32599afc9"
   },
   "outputs": [],
   "source": [
    "%pip install ribs[visualize] langchain tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating an LLM with LangChain and Ollama\n",
    "\n",
    "To make this tutorial flexible to the choice of LLM, we use [LangChain](https://python.langchain.com/docs/introduction/). Among other things, LangChain provides a common interface for operating with LLMs from providers like OpenAI and Google. In this tutorial, we will use LangChain's integration with [Ollama](https://ollama.com). Ollama is a framework that enables efficiently running LLMs on local machines. In other words, _we will use LangChain to call an LLM hosted locally by Ollama_.\n",
    "\n",
    "If you are running this tutorial on your own machine, please follow the [installation instructions](https://ollama.com/download) for Ollama and skip this cell. If you are running on Google Colab, we can install Ollama by following the instructions shown below, which were adapted from this [notebook](https://colab.research.google.com/github/5aharsh/collama/blob/main/Ollama_Setup.ipynb) by Saharsh Anand.\n",
    "\n",
    "**Note:** If you would like to use an LLM from an API like OpenAI or Google Gemini, LangChain also provides integrations for many APIs; more details (such as how to use `init_chat_model`) are available [here](https://python.langchain.com/docs/tutorials/llm_chain/). In that case, feel free to skip this section and instantiate a `model` variable on your own. Note that we assume the model is a _chat model_, i.e., an instance of [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RoROPhPuoXA",
    "outputId": "5b9bb1c5-28ca-4b83-856a-8ce7ad0e0f35"
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72slzM1SwGWH"
   },
   "source": [
    "After installing Ollama, we start the Ollama server in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViZYk__Juyv_"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def run_ollama_serve():\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)  # Wait for the server to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflVXn-ZwaBa"
   },
   "source": [
    "We can now pull the LLM model from Ollama's library and instantiate it in LangChain. We have chosen [Llama 3.1](https://ollama.com/library/llama3.1:8b-instruct-q4_K_M), specifically the 8B parameter model that has been finetuned for instruction following. We choose the `q4_K_M` [quantization](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md) as it is a recommended size that balances between speed/memory usage and accuracy. For alternative models, visit the library [here](https://ollama.com/library). Example alternatives include `llama3.1:70b-instruct-q4_K_M` (70B version of Llama 3.1) and `gpt-oss:20b` (gpt-oss-20b from OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBMVl2zzu9Ji",
    "outputId": "168fb3e6-2986-4ce6-b2cc-051ef5c3fb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "Model: model='llama3.1:8b-instruct-q4_K_M'\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_name = \"llama3.1:8b-instruct-q4_K_M\"  # @param {\"type\":\"string\"}\n",
    "\n",
    "# Pull the model from the Ollama library.\n",
    "!ollama pull {model_name}\n",
    "\n",
    "# Instantiate the model in LangChain.\n",
    "model = ChatOllama(model=model_name)\n",
    "print(\"Model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Feedback with an Evaluator\n",
    "\n",
    "The first ingredient for QDAIF is an evaluator that calls the LLM to provide feedback. In this creative writing domain, we consider the following objective and measures:\n",
    "\n",
    "- Objective: Write a story about a\n",
    "\n",
    "TODO: define objective and measure prompts\n",
    "TODO: talk about rating 1-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_prompt = \"Determine if the input text contains a high-quality short story containing two characters, a suspicious spy, and a rich politician. For example, a high-quality short story would have good flow, interesting plot, and not repeat similar sentences or undesired items such as titles and URLs. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xkwuRVopvIfm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Manages an LLM to compute objectives and measures.\n",
    "\n",
    "    Args:\n",
    "        model: Chat model for computing evaluations.\n",
    "        n_evals: Number of times to evaluate the objective and each measure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: BaseChatModel,\n",
    "        objective_prompt: str,\n",
    "        measure_0_prompt: str,\n",
    "        measure_1_prompt: str,\n",
    "        n_evals: int,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.n_evals = n_evals\n",
    "        self.min_score = 1\n",
    "        self.max_score = 10\n",
    "\n",
    "        # Objective: Short story about spy and politician. Note that the text in the\n",
    "        # schema class (including class name, field name, field description, docstrings)\n",
    "        # all have some influence on the LLM output -- see here:\n",
    "        # https://python.langchain.com/docs/how_to/structured_output/\n",
    "        class Rating(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        self.objective_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    objective_prompt,\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.objective_chain = (\n",
    "            self.objective_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 0: Romance genre.\n",
    "        class Romance(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        self.measure_0_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Determine if the input text is a romance story. For example, a romance story talks about two characters who fall in love with each other. Output your rating on a scale of 1 to 10. Output only a single integer.\",\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.measure_0_chain = (\n",
    "            self.measure_0_template | self.model.with_structured_output(Romance)\n",
    "        )\n",
    "\n",
    "        # Measure 1: Happy ending.\n",
    "        class HappyEnding(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        self.measure_1_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Determine if the input text is a story with a happy ending. For example, a story where the two characters make peach with each other has a happy ending. Output your rating on a scale of 1 to 10. Output only a single integer.\",\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.measure_1_chain = (\n",
    "            self.measure_1_template | self.model.with_structured_output(HappyEnding)\n",
    "        )\n",
    "\n",
    "    def _compute_score(self, chain: Runnable, texts: list[str]):\n",
    "        \"\"\"Computes the objectives and measures for the given batch of inputs.\n",
    "\n",
    "        Each text input is evaluated `n_evals` times.\n",
    "\n",
    "        Two values are returned:\n",
    "        - The first value is `all_scores`, which is a list where each entry contains the\n",
    "          `n_evals` scores for each text. This is a list because some evals may fail,\n",
    "          meaning that not all texts will have `n_evals` scores.\n",
    "        - The second is `mean_scores`, which is the mean score for each piece of text.\n",
    "        \"\"\"\n",
    "        inputs = [{\"text\": text} for text in texts for _ in range(self.n_evals)]\n",
    "        outputs = chain.batch(inputs)\n",
    "\n",
    "        all_scores = []\n",
    "        mean_scores = []\n",
    "\n",
    "        for i in range(0, len(outputs), self.n_evals):\n",
    "            results = outputs[i : i + self.n_evals]\n",
    "            scores = []\n",
    "            for r in results:\n",
    "                # Note: This assumes the schema for each result has a `rating` field,\n",
    "                # which may not be the case if you modify the schema above.\n",
    "                score = np.clip(r.rating, self.min_score, self.max_score)\n",
    "                scores.append(score)\n",
    "\n",
    "            scores = np.asarray(scores)\n",
    "            all_scores.append(scores)\n",
    "            mean_scores.append(scores.mean())\n",
    "\n",
    "        return all_scores, np.asarray(mean_scores)\n",
    "\n",
    "    def evaluate_objective(self, texts: list[str]):\n",
    "        return self._compute_score(self.objective_chain, texts)\n",
    "\n",
    "    def evaluate(self, texts: list[str]):\n",
    "        objectives = self._compute_score(self.objective_chain, texts)[1]\n",
    "        measure_0 = self._compute_score(self.measure_0_chain, texts)[1]\n",
    "        measure_1 = self._compute_score(self.measure_1_chain, texts)[1]\n",
    "        measures = np.stack((measure_0, measure_1), axis=1)\n",
    "        return objectives, measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MurzWysRvkTz",
    "outputId": "7fed94c4-da89-490b-eae4-ef3ef36df793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objectives:\n",
      "[6.66666667 6.33333333 7.33333333 6.66666667]\n",
      "Measures:\n",
      "[[1.66666667 2.        ]\n",
      " [2.33333333 3.        ]\n",
      " [1.66666667 2.        ]\n",
      " [8.33333333 9.        ]]\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(\n",
    "    model=\"llama3.1:8b-instruct-q4_K_M\",\n",
    "    n_evals=5,\n",
    ")\n",
    "\n",
    "objectives, measures = evaluator.evaluate(\n",
    "    [\n",
    "        # From QDAIF paper.\n",
    "        \"A spy named Joanne wants to infiltrate the premises of Karl Johnson, a highly-influential figure in the city. Karl was a wealthy mayor, and would do anything in his power to suppress any opposing voices. Joanne wanted to figure out what Karl was hiding, but she took a turn for the worse, as she was highly suspicious in her presence outside his home.\",\n",
    "        \"The wealthy entrepreneur and member of parliament, Susan, hosted a party at her mansion. She invited all of the residents, as well as an unusual looking man. The man, Dave, was wearing a tacky shirt, and star-shaped glasses, and was actually a spy. He made the whole room laugh with his jokes, and had a secret agenda - to find what Susan does in her private fun room!\",\n",
    "        \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Objectives:\")\n",
    "print(objectives)\n",
    "print(\"Measures:\")\n",
    "print(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDAIF Components in pyribs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.schedulers import Scheduler\n",
    "\n",
    "scheduler = Scheduler(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running QDAIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_itrs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you find this tutorial useful, please cite it as:\n",
    "\n",
    "```\n",
    "@article{pyribs_qdaif,\n",
    "  title   = {Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback},\n",
    "  author  = {Bryon Tjanaka},\n",
    "  journal = {pyribs.org},\n",
    "  year    = {2025},\n",
    "  url     = {https://docs.pyribs.org/en/stable/tutorials/qdaif.html}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "Thank you to [Sid Srikanth](https://sidsrikanth.com/), [Saeed Hedayatian](https://conflictednerd.github.io/), and the members of the ICAROS Lab for their invaluable feedback in developing this tutorial."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
