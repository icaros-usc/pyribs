{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback\n",
    "\n",
    "_This tutorial is part of the series of pyribs tutorials! See [here](https://docs.pyribs.org/en/latest/tutorials.html) for the list of all tutorials and the order in which they should be read._\n",
    "\n",
    "Given a creative writing task like \"write a story about a spy and a politician,\" there are many possible interpretations. For instance, we could write a story that ends with the spy getting away with classified information. Or one where the spy and politician put aside their differences and team up to overthrow a government. Or even one where the spy and politician fall in love and leave their lives behind. In short, a wide range of stories exist, each with their own unique plots and details.\n",
    "\n",
    "<figure style=\"width: 50%; margin-left: auto; margin-right: auto;\">\n",
    "\n",
    "![](_static/spy-and-politician.png)\n",
    "\n",
    "<figcaption style=\"text-align: center; font-style: italic\">\"An image of a suspicious spy and a rich politician.\" Generated with ChatGPT.</figcaption>\n",
    "</figure>\n",
    "\n",
    "To explore the range of such possibilities, [Quality Diversity through AI Feedback (QDAIF; Bradley 2024)](https://qdaif.github.io/) proposes to orchestrate LLMs in two ways. First, QDAIF uses LLMs to _generate_ new stories. Given a story, QDAIF prompts the LLM to mutate the story into a new one. Second, and equally as important, QDAIF leverages LLMs to _evaluate_ each story, providing the quality and diversity metrics (i.e., objective and measure values). Thus, QDAIF can repeatedly generate and evaluate stories, eventually producing an archive of diverse stories.\n",
    "\n",
    "In this tutorial, we will demonstrate how to implement a variation of QDAIF in pyribs on the task of writing a story about a spy and a politician. We will describe how to evaluate the objective and measures for each story, set up the QD algorithm components, run the algorithm, and visualize the results.\n",
    "\n",
    "_Since this tutorial involves running LLMs, we recommend running it on a machine with a GPU, either on Colab or on a local workstation. It should also be possible to run on a standard laptop, although it will be much slower. Alternatively, if you would like to use an API such as OpenAI or Google Gemini, it may also be possible to set that up (more details below)._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up the prerequisites for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "\n",
    "In addition to pyribs, this tutorial depends on [LangChain](https://python.langchain.com/docs/introduction/), a framework for developing LLM applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE4F13T5vc-t",
    "outputId": "fa943f14-bd85-488f-de82-45f32599afc9"
   },
   "outputs": [],
   "source": [
    "%pip install ribs[visualize] langchain tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating an LLM with LangChain and Ollama\n",
    "\n",
    "To make this tutorial flexible to the choice of LLM, we use [LangChain](https://python.langchain.com/docs/introduction/). Among other things, LangChain provides a common interface for operating with LLMs from providers like OpenAI and Google. In this tutorial, we will use LangChain's integration with [Ollama](https://ollama.com). Ollama is a framework that enables efficiently running LLMs on local machines. In other words, _we will use LangChain to call an LLM hosted locally by Ollama_.\n",
    "\n",
    "If you are running this tutorial on your own machine, please follow the [installation instructions](https://ollama.com/download) for Ollama and skip only the cell below. If you are running on Google Colab, we can install Ollama by following the instructions shown below, which were adapted from this [notebook](https://colab.research.google.com/github/5aharsh/collama/blob/main/Ollama_Setup.ipynb) by Saharsh Anand.\n",
    "\n",
    "**Note:** If you would like to use an LLM from an API like OpenAI or Google Gemini, LangChain also provides integrations for many APIs; more details (such as how to use `init_chat_model`) are available [here](https://python.langchain.com/docs/tutorials/llm_chain/). In that case, feel free to skip this section and instantiate a `model` variable on your own. Note that we assume the model is a _chat model_, i.e., an instance of [BaseChatModel](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RoROPhPuoXA",
    "outputId": "5b9bb1c5-28ca-4b83-856a-8ce7ad0e0f35"
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72slzM1SwGWH"
   },
   "source": [
    "After installing Ollama, we start the Ollama server in the background. If Ollama is already running, this cell will output an error that the address is already in use. _This is perfectly fine; simply proceed to the next steps._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViZYk__Juyv_"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def run_ollama_serve():\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)  # Wait for the server to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflVXn-ZwaBa"
   },
   "source": [
    "We can now pull the LLM model from Ollama's library and instantiate it in LangChain. We have chosen [Llama 3.1](https://ollama.com/library/llama3.1:8b-instruct-q4_K_M), specifically the 8B parameter model that has been finetuned for instruction following. We choose the `q4_K_M` [quantization](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md) as it is a recommended size that balances between speed/memory usage and accuracy. For alternative models, visit the Ollama library [here](https://ollama.com/library). Example alternatives include `llama3.1:70b-instruct-q4_K_M` (70B version of Llama 3.1) and `gpt-oss:20b` (gpt-oss-20b from OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBMVl2zzu9Ji",
    "outputId": "168fb3e6-2986-4ce6-b2cc-051ef5c3fb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n",
      "---\n",
      "Model: model='llama3.1:8b-instruct-q4_K_M'\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_name = \"llama3.1:8b-instruct-q4_K_M\"  # @param {\"type\":\"string\"}\n",
    "\n",
    "# Pull the model from the Ollama library.\n",
    "!ollama pull {model_name}\n",
    "print(\"---\")\n",
    "\n",
    "# Instantiate the model in LangChain.\n",
    "model = ChatOllama(model=model_name)\n",
    "print(\"Model:\", model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Feedback with an Evaluator\n",
    "\n",
    "The first ingredient for QDAIF is an evaluator that calls the LLM to provide feedback on the quality and diversity of each story by evaluating the objective and measures. Before creating the evaluator, let us first define the objective and measures for this creative writing domain:\n",
    "\n",
    "- **Objective:** Is the story about a suspicious spy and a rich politician?\n",
    "- **Measure 0:** Is the story a romance story?\n",
    "- **Measure 1:** Does the story have a happy ending?\n",
    "\n",
    "This domain is adapted from the Stories domain in the original QDAIF paper (Bradley 2023), where the measures were slightly different. Later on, we will present the exact LLM prompts for each of these characteristics.\n",
    "\n",
    "There are many ways to obtain the LLM's score for the objective and for each measure. For example, the original QDAIF paper predominantly extracts scores by asking the LLM to output an answer like yes or no and analyzing the logits of the tokens associated with that answer. Notably, this approach requires access to the logits output by the LLM, and logits are not always available. Thus, in this tutorial, we instead ask the LLM to output a rating on a scale of 1 to 10 when evaluating the objective and each measure. This approach is general in that it works with any LLM, but a key drawback is that the rating is stochastic and requires multiple evaluations.\n",
    "\n",
    "Having defined the objective and measures, we now define the `Evaluator` class that calls the LLM to evaluate the objective and measures. Notably, this class takes an `n_evals` parameter that determines how many times to evaluate the objective and each measure. The final score for the objective and each measure is averaged over the ratings from the `n_evals` evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xkwuRVopvIfm"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import Runnable\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Manages an LLM to compute the objective and measures.\n",
    "\n",
    "    Args:\n",
    "        model: Chat model for computing evaluations.\n",
    "        objective_prompt: Prompt for the objective.\n",
    "        measure_0_prompt: Prompt for the first measure (measure 0).\n",
    "        measure_1_prompt: Prompt for the second measure (measure 1).\n",
    "        n_evals: Number of times to evaluate the objective and each measure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        model: BaseChatModel,\n",
    "        objective_prompt: str,\n",
    "        measure_0_prompt: str,\n",
    "        measure_1_prompt: str,\n",
    "        n_evals: int,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.n_evals = n_evals\n",
    "        self.min_score = 1\n",
    "        self.max_score = 10\n",
    "\n",
    "        # To receive the output from the LLM in a consistent format, we use structured\n",
    "        # output (https://python.langchain.com/docs/how_to/structured_output/). This\n",
    "        # Pydantic model defines the schema for receiving ratings from the LLM. Note\n",
    "        # that the text in the schema class (including class name, field name, field\n",
    "        # description, docstrings) all have some influence on the LLM output.\n",
    "        class Rating(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        # Objective. We first define a chat template, where the `objective_prompt`\n",
    "        # passed in is the system prompt, and the `text` of the story is the user's\n",
    "        # message. Then, we form a chain that connects this template to the model. We\n",
    "        # do the same for measure 0 and measure 1 below. For more background on\n",
    "        # LangChain, refer to the documentation, such as:\n",
    "        # - https://python.langchain.com/docs/tutorials/llm_chain/\n",
    "        # - https://python.langchain.com/docs/concepts/lcel/\n",
    "        self.objective_template = ChatPromptTemplate(\n",
    "            [(\"system\", objective_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.objective_chain = (\n",
    "            self.objective_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 0.\n",
    "        self.measure_0_template = ChatPromptTemplate(\n",
    "            [(\"system\", measure_0_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.measure_0_chain = (\n",
    "            self.measure_0_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 1.\n",
    "        self.measure_1_template = ChatPromptTemplate(\n",
    "            [(\"system\", measure_1_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "        self.measure_1_chain = (\n",
    "            self.measure_1_template | self.model.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "    def compute_score(self, chain: Runnable, texts: list[str]):\n",
    "        \"\"\"Uses the given chain to compute scores for the given batch of input texts.\n",
    "\n",
    "        Each text input is evaluated `n_evals` times.\n",
    "\n",
    "        Two values are returned:\n",
    "        - The first value is `all_scores`, which is a list where each entry contains the\n",
    "          `n_evals` scores for each text.\n",
    "        - The second is `mean_scores`, which is the mean score for each piece of text.\n",
    "        \"\"\"\n",
    "        inputs = [{\"text\": text} for text in texts for _ in range(self.n_evals)]\n",
    "        outputs = chain.batch(inputs)\n",
    "\n",
    "        all_scores = []\n",
    "        mean_scores = []\n",
    "\n",
    "        for i in range(0, len(outputs), self.n_evals):\n",
    "            results = outputs[i : i + self.n_evals]\n",
    "            scores = []\n",
    "            for r in results:\n",
    "                # Note: this assumes the schema for each result has a `rating` field,\n",
    "                # which may not be the case if you modify the schema above.\n",
    "                score = np.clip(r.rating, self.min_score, self.max_score)\n",
    "                scores.append(score)\n",
    "\n",
    "            scores = np.asarray(scores)\n",
    "            all_scores.append(scores)\n",
    "            mean_scores.append(scores.mean())\n",
    "\n",
    "        return all_scores, np.asarray(mean_scores)\n",
    "\n",
    "    def evaluate(self, texts: list[str]):\n",
    "        objectives = self.compute_score(self.objective_chain, texts)[1]\n",
    "        measure_0 = self.compute_score(self.measure_0_chain, texts)[1]\n",
    "        measure_1 = self.compute_score(self.measure_1_chain, texts)[1]\n",
    "        measures = np.stack((measure_0, measure_1), axis=1)\n",
    "        return objectives, measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the `Evaluator`, we now instantiate it with the `model`, the prompts for the objective and measures, and the number of evaluations `n_evals`. Each prompt starts with defining a characteristic of the story and ends with asking the LLM to output a rating from 1 to 10. Feel free to customize the prompts to specify different objectives and measures!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MurzWysRvkTz",
    "outputId": "7fed94c4-da89-490b-eae4-ef3ef36df793"
   },
   "outputs": [],
   "source": [
    "objective_prompt = \"Determine if the input text contains a high-quality short story containing two characters, a suspicious spy, and a rich politician. For example, a high-quality short story would have good flow, interesting plot, and not repeat similar sentences or undesired items such as titles and URLs. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "measure_0_prompt = \"Determine if the input text is a romance story. For example, a romance story talks about two characters who fall in love with each other. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "measure_1_prompt = \"Determine if the input text is a story with a happy ending. For example, a story where the two characters make peach with each other has a happy ending. Output your rating on a scale of 1 to 10. Output only a single integer.\"  # @param {\"type\":\"string\"}\n",
    "n_evals = 5  # @param {\"type\":\"integer\"}\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    model=model,\n",
    "    objective_prompt=objective_prompt,\n",
    "    measure_0_prompt=measure_0_prompt,\n",
    "    measure_1_prompt=measure_1_prompt,\n",
    "    n_evals=n_evals,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's use the `Evaluator` to evaluate two stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MurzWysRvkTz",
    "outputId": "7fed94c4-da89-490b-eae4-ef3ef36df793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story 0 | Objective: 6.4, Measure 0: 2.0, Measure 1: 2.0\n",
      "Story 1 | Objective: 7.0, Measure 0: 7.8, Measure 1: 8.6\n"
     ]
    }
   ],
   "source": [
    "objectives, measures = evaluator.evaluate(\n",
    "    [\n",
    "        \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "        \"Jack was a politician in the city when one day he met Sarah. Sarah had been working for the government as a secret spy. Jack decided he really liked Sarah, and they fell in love. They both quite their jobs and decided to live in the countryside together.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "for i, (obj, meas) in enumerate(zip(objectives, measures)):\n",
    "    print(f\"Story {i} | Objective: {obj}, Measure 0: {meas[0]}, Measure 1: {meas[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDAIF Components in pyribs\n",
    "\n",
    "Like other QD algorithms in pyribs, QDAIF is composed of an archive, emitters, and a scheduler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridArchive for Storing Stories\n",
    "\n",
    "The archive for QDAIF is a [`GridArchive`](https://docs.pyribs.org/en/latest/api/ribs.archives.GridArchive.html), which divides the measure space into a grid and stores a story in each grid cell. Below, we specify the dimensions (`dims`) of the grid to be $20 \\times 20$, and the `ranges` to be 1 to 10 for each dimension.\n",
    "\n",
    "For those familiar with the `GridArchive` from previous tutorials, the settings for this `GridArchive` are slightly different since it must store text-based solutions, whereas previous tutorials involved solutions that were continuous vectors. The differences are as follows. First, we set `solution_dim` to be `()`, indicating a scalar value. Second, we set `dtype` such that the `solution` is an `object` (while the `objective` and `measures` remain as floating-point values). This way, the archive can store pieces of text, which are single objects of type `str` (i.e., \"scalar\" objects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.archives import GridArchive\n",
    "\n",
    "archive = GridArchive(\n",
    "    solution_dim=(),\n",
    "    dims=[20, 20],\n",
    "    ranges=[(1, 10), (1, 10)],\n",
    "    dtype={\"solution\": object, \"objective\": np.float32, \"measures\": np.float32},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Emitter for Generating New Stories\n",
    "\n",
    "In pyribs, emitters generate new candidate solutions for evaluation. In the case of QDAIF, the emitter generates new stories by mutating existing ones in the archive. The original QDAIF performed such mutations with Language Model Crossover from [Meyerson 2023](https://arxiv.org/abs/2302.12170). In this tutorial, we differ from the original QDAIF by implementing \"directional\" variations inspired by PLAN-QD in [Srikanth 2025](https://arxiv.org/abs/2504.03991).\n",
    "\n",
    "Specifically, the emitter does the following. First, it selects a random story from the archive. Next, for each measure, it selects a random direction, either up or down. For instance, for measure 0 in this tutorial, the story can be made to sound more like a romance story or less like a romance story, and for measure 1, the ending of the story can be made more happy or less happy. Having selected a story and these two random directions (one for each measure), the emitter calls the LLM to mutate the story in the given directions, e.g., \"given this story, make the story sound more like a romance story, and make the ending more happy.\" The result is a new story that is then sent for evaluation.\n",
    "\n",
    "Finally, note that the emitter below also takes in an `initial_solutions` parameter. These define the initial texts that will be output by the emitter on the first iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.archives import ArchiveBase\n",
    "from ribs.emitters import EmitterBase\n",
    "\n",
    "\n",
    "class LLMDirectionalEmitter(EmitterBase):\n",
    "    \"\"\"Uses LLMs to modify pieces of text in random archive directions.\n",
    "\n",
    "    Args:\n",
    "        archive: Archive of solutions, e.g., :class:`ribs.archives.GridArchive`. The\n",
    "            archive must contain solutions of type :class:`str`.\n",
    "        model: LLM for mutating pieces of text.\n",
    "        mutation_prompt: Prompt for mutating the text. This prompt must contain fields\n",
    "            for ``measure_0_direction`` and ``measure_1_direction``.\n",
    "        measure_*_{down,up}: Prompts indicating directions for mutating the story. Note\n",
    "            that these directions are inserted into the ``mutation_prompt``.\n",
    "        batch_size: Number of solutions to return in :meth:`ask`.\n",
    "        initial_solutions: Initial pieces of text for the LLM.\n",
    "        seed: Value to seed the random number generator. Set to None to avoid a fixed\n",
    "            seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        archive: ArchiveBase,\n",
    "        *,\n",
    "        model: BaseChatModel,\n",
    "        mutation_prompt: str,\n",
    "        measure_0_down: str,\n",
    "        measure_0_up: str,\n",
    "        measure_1_down: str,\n",
    "        measure_1_up: str,\n",
    "        batch_size: int,\n",
    "        initial_solutions: list[str],\n",
    "        seed: int | None = None,\n",
    "    ):\n",
    "        EmitterBase.__init__(\n",
    "            self,\n",
    "            archive,\n",
    "            solution_dim=archive.solution_dim,\n",
    "            bounds=None,\n",
    "        )\n",
    "\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.initial_solutions = initial_solutions\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "        # This template provides the mutation prompt as the LLM's system prompt, and\n",
    "        # then provides the story as the `text` field.\n",
    "        self.mutation_template = ChatPromptTemplate(\n",
    "            [(\"system\", mutation_prompt), (\"user\", \"{text}\")]\n",
    "        )\n",
    "\n",
    "        # Each measure has two possible directions: up or down.\n",
    "        self.mutation_dirs = {\n",
    "            \"measure_0\": [measure_0_down, measure_0_up],\n",
    "            \"measure_1\": [measure_1_down, measure_1_up],\n",
    "        }\n",
    "\n",
    "        # Schema for retrieving stories from the model.\n",
    "        class Story(BaseModel):\n",
    "            story: str = Field(description=\"The modified story.\")\n",
    "\n",
    "        self.mutation_chain = (\n",
    "            self.mutation_template | self.model.with_structured_output(Story)\n",
    "        )\n",
    "\n",
    "    def ask(self):\n",
    "        # When no solutions have been created yet, just return the initial seed texts.\n",
    "        if self.archive.empty:\n",
    "            return self.initial_solutions\n",
    "\n",
    "        # During normal operation, sample new texts and directions to pass to the LLM.\n",
    "        prompts = []\n",
    "        for _ in range(self.batch_size):\n",
    "            # For both measure_0 and measure_1 (hence size=2), choose between the two\n",
    "            # possible directions.\n",
    "            dirs = self.rng.choice(2, size=2)\n",
    "\n",
    "            prompts.append(\n",
    "                {\n",
    "                    \"text\": self._archive.sample_elites(1)[\"solution\"][0],\n",
    "                    \"measure_0_direction\": self.mutation_dirs[\"measure_0\"][dirs[0]],\n",
    "                    \"measure_1_direction\": self.mutation_dirs[\"measure_1\"][dirs[1]],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        stories = self.mutation_chain.batch(prompts)\n",
    "        return [s.story for s in stories]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we instantiate the emitter with prompts to enable making the story more/less romantic and the ending more/less happy. We also provide three initial stories (`initial_solutions`) drawn from the QDAIF paper. Feel free to modify any of the prompts or the initial stories.\n",
    "\n",
    "To save on computation, we have only defined one emitter with a batch size of 1, so we only generate 1 solution per iteration. We also use the same `model` that is used by the `Evaluator`, so that we do not have to load another model. It is certainly possible to increase the batch size of each emitter, create multiple emitters that each perform mutations with different prompts, or use different models than that used in the `Evaluator`. If you have the time, we certainly encourage trying out these ideas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "mutation_prompt = \"The following is a story about two characters, a suspicious spy, and a rich politician. Modify the story in the following ways: {measure_0_direction}, and {measure_1_direction}. Output only the new story.\"  # @param {\"type\":\"string\"}\n",
    "measure_0_down = \"make the story sound less like a romance story\"  # @param {\"type\":\"string\"}\n",
    "measure_0_up = \"make the story sound more like a romance story\"  # @param {\"type\":\"string\"}\n",
    "measure_1_down = \"make the ending of the story less happy\"  # @param {\"type\":\"string\"}\n",
    "measure_1_up = \"make the ending of the story more happy\"  # @param {\"type\":\"string\"}\n",
    "batch_size = 1  # @param {\"type\":\"integer\"}\n",
    "# fmt: on\n",
    "\n",
    "emitters = [\n",
    "    LLMDirectionalEmitter(\n",
    "        archive,\n",
    "        model=model,\n",
    "        mutation_prompt=mutation_prompt,\n",
    "        measure_0_down=measure_0_down,\n",
    "        measure_0_up=measure_0_up,\n",
    "        measure_1_down=measure_1_down,\n",
    "        measure_1_up=measure_1_up,\n",
    "        batch_size=batch_size,\n",
    "        initial_solutions=[\n",
    "            # From QDAIF paper (Appendix A.21).\n",
    "            \"A spy named Joanne wants to infiltrate the premises of Karl Johnson, a highly-influential figure in the city. Karl was a wealthy mayor, and would do anything in his power to suppress any opposing voices. Joanne wanted to figure out what Karl was hiding, but she took a turn for the worse, as she was highly suspicious in her presence outside his home.\",\n",
    "            \"The wealthy entrepreneur and member of parliament, Susan, hosted a party at her mansion. She invited all of the residents, as well as an unusual looking man. The man, Dave, was wearing a tacky shirt, and star-shaped glasses, and was actually a spy. He made the whole room laugh with his jokes, and had a secret agenda - to find what Susan does in her private fun room!\",\n",
    "            \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "        ],\n",
    "    ),\n",
    "    # Additional emitters would go here (`emitters` is a list of emitters).\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler for Composing QDAIF\n",
    "\n",
    "Finally, to compose the archive and emitters for QDAIF together, we define a [`Scheduler`](https://docs.pyribs.org/en/latest/api/ribs.schedulers.Scheduler.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.schedulers import Scheduler\n",
    "\n",
    "scheduler = Scheduler(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running QDAIF\n",
    "\n",
    "With the QDAIF components defined, we can execute QDAIF in the loop below. During this loop, we first call `ask` on the `scheduler`, which generates new stories (`solutions`) by calling the `ask` method defined on `LLMDirectionalEmitter` above. After evaluating the `objectives` and `measures` of the stories, we pass them to the `scheduler` with `tell`, which inserts the stories into the `archive`. Along the way, we regularly log metrics and save the archive data as a CSV. On a workstation with an A6000 GPU, this loop took ~1 hour to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration     5 | Archive Coverage:  1.500%  QD Score: 44.800              \n",
      "Iteration    10 | Archive Coverage:  2.500%  QD Score: 78.200              \n",
      "Iteration    15 | Archive Coverage:  3.750%  QD Score: 118.600             \n",
      "Iteration    20 | Archive Coverage:  4.500%  QD Score: 145.200             \n",
      "Iteration    25 | Archive Coverage:  5.500%  QD Score: 180.400             \n",
      "Iteration    30 | Archive Coverage:  6.750%  QD Score: 209.600             \n",
      "Iteration    35 | Archive Coverage:  7.500%  QD Score: 238.600             \n",
      "Iteration    40 | Archive Coverage:  8.250%  QD Score: 264.200             \n",
      "Iterations:  22%|█████▍                   | 43/200 [11:06<41:28, 15.85s/it]"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "total_itrs = 200\n",
    "\n",
    "for itr in trange(1, total_itrs + 1, file=sys.stdout, desc=\"Iterations\"):\n",
    "    solutions = scheduler.ask()\n",
    "    objectives, measures = evaluator.evaluate(solutions)\n",
    "    scheduler.tell(objectives, measures)\n",
    "\n",
    "    if itr % 5 == 0 or itr == total_itrs:\n",
    "        tqdm.write(\n",
    "            f\"Iteration {itr:5d} | \"\n",
    "            f\"Archive Coverage: {archive.stats.coverage * 100:6.3f}%  \"\n",
    "            f\"QD Score: {archive.stats.qd_score:6.3f}\"\n",
    "        )\n",
    "\n",
    "        # Save the archive's data as a CSV, which makes it easy to read the stories.\n",
    "        archive.data(return_type=\"pandas\").to_csv(\"qdaif_archive.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "grid_archive_heatmap(archive, vmin=1, vmax=10)\n",
    "plt.title(f\"Iteration {itr}\")\n",
    "plt.title(\"A story about a suspicious spy and a rich politician.\")\n",
    "plt.xlabel(\"Romance Genre\")\n",
    "plt.ylabel(\"Happy Ending\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Stories to Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you find this tutorial useful, please cite it as:\n",
    "\n",
    "```\n",
    "@article{pyribs_qdaif,\n",
    "  title   = {Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback},\n",
    "  author  = {Bryon Tjanaka},\n",
    "  journal = {pyribs.org},\n",
    "  year    = {2025},\n",
    "  url     = {https://docs.pyribs.org/en/stable/tutorials/qdaif.html}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "Thank you to [Sid Srikanth](https://sidsrikanth.com/), [Saeed Hedayatian](https://conflictednerd.github.io/), and the members of the ICAROS Lab for their invaluable feedback in developing this tutorial."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
