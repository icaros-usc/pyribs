{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback\n",
    "\n",
    "_This tutorial is part of the series of pyribs tutorials! See [here](https://docs.pyribs.org/en/latest/tutorials.html) for the list of all tutorials and the order in which they should be read._\n",
    "\n",
    "Given a creative writing task, such as \"write a story about a spy and a politician,\" there are many possible outcomes. For instance, we could have a story where\n",
    "\n",
    "(TODO)\n",
    "\n",
    "or a story where\n",
    "\n",
    "(TODO -- refer to outputs).\n",
    "\n",
    "Alternative, we could even...\n",
    "\n",
    "TODO\n",
    "\n",
    "In short, a wide range of stories exist, each with their own interesting capabilities.\n",
    "\n",
    "To explore the diverse possibilities available in creative writing, [Quality Diversity through AI Feedback (QDAIF; Bradley 2024)](https://qdaif.github.io/) proposes to leverage recent advances in large language models (LLMs). Specifically, LLMs in QDAIF serve two purposes.\n",
    "\n",
    "First, they can generate new stories...\n",
    "\n",
    "Second, they can evaluate...\n",
    "\n",
    "In this tutorial, we will show how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama\n",
    "\n",
    "To run the LLMs for this tutorial, we will use [Ollama](https://ollama.com). If you are running this tutorial locally, please follow the [installation instructions](https://ollama.com/download) for Ollama and skip this cell. On Google Colab, we can install Ollama following the instructions in this [notebook](https://colab.research.google.com/github/5aharsh/collama/blob/main/Ollama_Setup.ipynb) by Saharsh Anand. First, we download and run the Ollama installation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6RoROPhPuoXA",
    "outputId": "5b9bb1c5-28ca-4b83-856a-8ce7ad0e0f35"
   },
   "outputs": [],
   "source": [
    "!sudo apt update\n",
    "!sudo apt install -y pciutils\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72slzM1SwGWH"
   },
   "source": [
    "Next, we start the Ollama server in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViZYk__Juyv_"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "\n",
    "def run_ollama_serve():\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "\n",
    "thread = threading.Thread(target=run_ollama_serve)\n",
    "thread.start()\n",
    "time.sleep(5)  # Wait for the server to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nflVXn-ZwaBa"
   },
   "source": [
    "We can now pull the Llama 3.1 model from [Ollama's library](https://ollama.com/library/llama3.1:8b-instruct-q4_K_M). We select the 8B parameter model that has been finetuned for instruction following. We choose the `q4_K_M` [quantization](https://github.com/ggml-org/llama.cpp/blob/master/tools/quantize/README.md) as it is a recommended size that balances between speed/memory usage and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NBMVl2zzu9Ji",
    "outputId": "168fb3e6-2986-4ce6-b2cc-051ef5c3fb0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama3.1:8b-instruct-q4_K_M\n",
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling 667b0c1932bc: 100% ▕██████████████████▏ 4.9 GB                         \u001b[K\n",
      "pulling 948af2743fc7: 100% ▕██████████████████▏ 1.5 KB                         \u001b[K\n",
      "pulling 0ba8f0e314b4: 100% ▕██████████████████▏  12 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5: 100% ▕██████████████████▏   96 B                         \u001b[K\n",
      "pulling 455f34728c9b: 100% ▕██████████████████▏  487 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "model_name = \"llama3.1:8b-instruct-q4_K_M\"  # @param {\"type\":\"string\"}\n",
    "!ollama pull {model_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Dependencies\n",
    "\n",
    "In addition to pyribs, this tutorial depends on [LangChain](https://python.langchain.com/docs/introduction/), a framework for developing LLM applications. Below we install pyribs, LangChain, and the package that integrates LangChain with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sE4F13T5vc-t",
    "outputId": "fa943f14-bd85-488f-de82-45f32599afc9"
   },
   "outputs": [],
   "source": [
    "%pip install ribs[visualize] langchain langchain-ollama tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import Runnable\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xkwuRVopvIfm"
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    \"\"\"Manages an LLM to compute objective and measures.\n",
    "\n",
    "    Args:\n",
    "        llm: Chat model for computing evaluations.\n",
    "        n_evals: Number of times to evaluate each objective and measure.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, llm: BaseChatModel, n_evals: int):\n",
    "        self.llm = llm\n",
    "        self.n_evals = n_evals\n",
    "        self.min_score = 1\n",
    "        self.max_score = 10\n",
    "\n",
    "        # Objective: Short story about spy and politician. Note that the text in the\n",
    "        # schema class (including class name, field name, field description, docstrings)\n",
    "        # all have some influence on the LLM output -- see here:\n",
    "        # https://python.langchain.com/docs/how_to/structured_output/\n",
    "        class Rating(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        self.objective_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Determine if the input text contains a high-quality short story containing two characters, a suspicious spy, and a rich politician. For example, a high-quality short story would have good flow, interesting plot, and not repeat similar sentences or undesired items such as titles and URLs. Output your rating on a scale of 1 to 10. Output only a single integer.\",\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.objective_chain = (\n",
    "            self.objective_template | self.llm.with_structured_output(Rating)\n",
    "        )\n",
    "\n",
    "        # Measure 0: Romance genre.\n",
    "        class Romance(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        self.measure_0_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Determine if the input text is a romance story. For example, a romance story talks about two characters who fall in love with each other. Output your rating on a scale of 1 to 10. Output only a single integer.\",\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.measure_0_chain = (\n",
    "            self.measure_0_template | self.llm.with_structured_output(Romance)\n",
    "        )\n",
    "\n",
    "        # Measure 1: Happy ending.\n",
    "        class HappyEnding(BaseModel):\n",
    "            rating: int = Field(description=\"The rating on a scale of 1 to 10.\")\n",
    "\n",
    "        self.measure_1_template = ChatPromptTemplate(\n",
    "            [\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"Determine if the input text is a story with a happy ending. For example, a story where the two characters make peach with each other has a happy ending. Output your rating on a scale of 1 to 10. Output only a single integer.\",\n",
    "                ),\n",
    "                (\"user\", \"{text}\"),\n",
    "            ]\n",
    "        )\n",
    "        self.measure_1_chain = (\n",
    "            self.measure_1_template | self.llm.with_structured_output(HappyEnding)\n",
    "        )\n",
    "\n",
    "    def _compute_score(self, chain: Runnable, texts: list[str]):\n",
    "        \"\"\"Computes the objectives and measures for the given batch of inputs.\n",
    "\n",
    "        Each text input is evaluated `n_evals` times.\n",
    "\n",
    "        Two values are returned:\n",
    "        - The first value is `all_scores`, which is a list where each entry contains the\n",
    "          `n_evals` scores for each text. This is a list because some evals may fail,\n",
    "          meaning that not all texts will have `n_evals` scores.\n",
    "        - The second is `mean_scores`, which is the mean score for each piece of text.\n",
    "        \"\"\"\n",
    "        inputs = [{\"text\": text} for text in texts for _ in range(self.n_evals)]\n",
    "        outputs = chain.batch(inputs)\n",
    "\n",
    "        all_scores = []\n",
    "        mean_scores = []\n",
    "\n",
    "        for i in range(0, len(outputs), self.n_evals):\n",
    "            results = outputs[i : i + self.n_evals]\n",
    "            scores = []\n",
    "            for r in results:\n",
    "                # Note: This assumes the schema for each result has a `rating` field,\n",
    "                # which may not be the case if you modify the schema above.\n",
    "                score = np.clip(r.rating, self.min_score, self.max_score)\n",
    "                scores.append(score)\n",
    "\n",
    "            scores = np.asarray(scores)\n",
    "            all_scores.append(scores)\n",
    "            mean_scores.append(scores.mean())\n",
    "\n",
    "        return all_scores, np.asarray(mean_scores)\n",
    "\n",
    "    def evaluate_objective(self, texts: list[str]):\n",
    "        return self._compute_score(self.objective_chain, texts)\n",
    "\n",
    "    def evaluate(self, texts: list[str]):\n",
    "        objectives = self._compute_score(self.objective_chain, texts)[1]\n",
    "        measure_0 = self._compute_score(self.measure_0_chain, texts)[1]\n",
    "        measure_1 = self._compute_score(self.measure_1_chain, texts)[1]\n",
    "        measures = np.stack((measure_0, measure_1), axis=1)\n",
    "        return objectives, measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MurzWysRvkTz",
    "outputId": "7fed94c4-da89-490b-eae4-ef3ef36df793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objectives:\n",
      "[6.66666667 6.33333333 7.33333333 6.66666667]\n",
      "Measures:\n",
      "[[1.66666667 2.        ]\n",
      " [2.33333333 3.        ]\n",
      " [1.66666667 2.        ]\n",
      " [8.33333333 9.        ]]\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(\n",
    "    model=\"llama3.1:8b-instruct-q4_K_M\",\n",
    "    n_evals=5,\n",
    ")\n",
    "\n",
    "objectives, measures = evaluator.evaluate(\n",
    "    [\n",
    "        # From QDAIF paper.\n",
    "        \"A spy named Joanne wants to infiltrate the premises of Karl Johnson, a highly-influential figure in the city. Karl was a wealthy mayor, and would do anything in his power to suppress any opposing voices. Joanne wanted to figure out what Karl was hiding, but she took a turn for the worse, as she was highly suspicious in her presence outside his home.\",\n",
    "        \"The wealthy entrepreneur and member of parliament, Susan, hosted a party at her mansion. She invited all of the residents, as well as an unusual looking man. The man, Dave, was wearing a tacky shirt, and star-shaped glasses, and was actually a spy. He made the whole room laugh with his jokes, and had a secret agenda - to find what Susan does in her private fun room!\",\n",
    "        \"The rich politician, Tom’s life took a turn for the worst - he feared all of his close aides all of a sudden after sensing danger in his clique. There was a civil war going on, and he feared for his life. One day, one of his security guards, turned secret agent, decided to sneak into the classified files room, and spied on Johnny, who was in the room. He wanted to find Johnny’s weakness, and strike at the right time.\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Objectives:\")\n",
    "print(objectives)\n",
    "print(\"Measures:\")\n",
    "print(measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDAIF Components in pyribs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ribs.schedulers import Scheduler\n",
    "\n",
    "scheduler = Scheduler(archive, emitters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running QDAIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_itrs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "If you find this tutorial useful, please cite it as:\n",
    "\n",
    "```\n",
    "@article{pyribs_qdaif,\n",
    "  title   = {Orchestrating LLMs to Write Diverse Stories with Quality Diversity through AI Feedback},\n",
    "  author  = {Bryon Tjanaka},\n",
    "  journal = {pyribs.org},\n",
    "  year    = {2025},\n",
    "  url     = {https://docs.pyribs.org/en/stable/tutorials/qdaif.html}\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "\n",
    "Thank you to [Sid Srikanth](https://sidsrikanth.com/), [Saeed Hedayatian](https://conflictednerd.github.io/), and the members of the ICAROS Lab for their invaluable feedback in developing this tutorial."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
